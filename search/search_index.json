{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Amundsen is a data discovery and metadata engine for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen, the first person to discover the South Pole.</p> <p></p> <p>Amundsen is hosted by the LF AI &amp; Data Foundation. It includes three microservices, one data ingestion library and one common library.</p> <ul> <li>amundsenfrontendlibrary: Frontend service which is a Flask application with a React frontend. </li> <li>amundsensearchlibrary: Search service, which leverages Elasticsearch for search capabilities, is used to power frontend metadata searching. </li> <li>amundsenmetadatalibrary: Metadata service, which leverages Neo4j or Apache Atlas as the persistent layer, to provide various metadata. </li> <li>amundsendatabuilder: Data ingestion library for building metadata graph and search index.   Users could either load the data with a python script with the library   or with an Airflow DAG importing the library. </li> <li>amundsencommon: Amundsen Common library holds common codes among microservices in Amundsen. </li> <li>amundsengremlin: Amundsen Gremlin library holds code used for converting model objects into vertices and edges in gremlin. It\u2019s used for loading data into an AWS Neptune backend. </li> <li>amundsenrds: Amundsenrds contains ORM models to support relational database as metadata backend store in Amundsen. The schema in ORM models follows the logic of databuilder models. Amundsenrds will be used in databuilder and metadatalibrary for metadata storage and retrieval with relational databases. </li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Homepage</li> <li>Documentation</li> </ul>"},{"location":"#community-roadmap","title":"Community Roadmap","text":"<p>We want your input about what is important, for that, add your votes using the \ud83d\udc4d reaction: * Top Feature Requests * Documentation Requests * Top Bugs * Top Questions</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>Node v12</li> </ul>"},{"location":"#user-interface","title":"User Interface","text":"<p>Please note that the mock images only served as demonstration purpose.</p> <ul> <li>Landing Page: The landing page for Amundsen including 1. search bars; 2. popular used tables;</li> </ul> <p></p> <ul> <li>Search Preview: See inline search results as you type</li> </ul> <p></p> <ul> <li>Table Detail Page: Visualization of a Hive / Redshift table</li> </ul> <p></p> <ul> <li>Column detail: Visualization of columns of a Hive / Redshift table which includes an optional stats display</li> </ul> <p></p> <ul> <li>Data Preview Page: Visualization of table data preview which could integrate with Apache Superset or other Data Visualization Tools.</li> </ul> <p></p>"},{"location":"#getting-started-and-installation","title":"Getting Started and Installation","text":"<p>Please visit the Amundsen installation documentation for a quick start to bootstrap a default version of Amundsen with dummy data.</p>"},{"location":"#supported-entities","title":"Supported Entities","text":"<ul> <li>Tables (from Databases)</li> <li>Dashboards</li> <li>ML Features</li> <li>People (from HR systems)</li> </ul>"},{"location":"#supported-integrations","title":"Supported Integrations","text":""},{"location":"#table-connectors","title":"Table Connectors","text":"<ul> <li>Amazon Athena</li> <li>Amazon EventBridge</li> <li>Amazon Glue and anything built over it</li> <li>Amazon Redshift</li> <li>Apache Cassandra</li> <li>Apache Druid</li> <li>Apache Hive</li> <li>CSV</li> <li>dbt</li> <li>Delta Lake</li> <li>Elasticsearch</li> <li>Google BigQuery</li> <li>IBM DB2</li> <li>Kafka Schema Registry</li> <li>Microsoft SQL Server</li> <li>MySQL</li> <li>Oracle (through dbapi or sql_alchemy)</li> <li>PostgreSQL</li> <li>PrestoDB</li> <li>Trino (formerly Presto SQL)</li> <li>Vertica</li> <li>Snowflake</li> </ul> <p>Amundsen can also connect to any database that provides <code>dbapi</code> or <code>sql_alchemy</code> interface (which most DBs provide).</p>"},{"location":"#table-column-statistics","title":"Table Column Statistics","text":"<ul> <li>Pandas Profiling</li> </ul>"},{"location":"#dashboard-connectors","title":"Dashboard Connectors","text":"<ul> <li>Apache Superset</li> <li>Mode Analytics</li> <li>Redash</li> <li>Tableau</li> <li>Databricks SQL</li> </ul>"},{"location":"#etl-orchestration","title":"ETL Orchestration","text":"<ul> <li>Apache Airflow</li> </ul>"},{"location":"#get-involved-in-the-community","title":"Get Involved in the Community","text":"<p>Want help or want to help? Use the button in our header to join our slack channel.</p> <p>Contributions are also more than welcome! As explained in CONTRIBUTING.md there are many ways to contribute, it does not all have to be code with new features and bug fixes, also documentation, like FAQ entries, bug reports, blog posts sharing experiences etc. all help move Amundsen forward. If you find a security vulnerability, please follow this guide.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>Please visit Architecture for Amundsen architecture overview.</p>"},{"location":"#resources","title":"Resources","text":""},{"location":"#blog-posts-and-interviews","title":"Blog Posts and Interviews","text":"<ul> <li>Amundsen - Lyft\u2019s data discovery &amp; metadata engine (April 2019)</li> <li>Software Engineering Daily podcast on Amundsen (April 2019)</li> <li>How Lyft Drives Data Discovery (July 2019)</li> <li>Data Engineering podcast on Solving Data Discovery At Lyft (Aug 2019)</li> <li>Open Sourcing Amundsen: A Data Discovery And Metadata Platform (Oct 2019)</li> <li>Adding Data Quality into Amundsen with Programmatic Descriptions by Sam Shuster from Edmunds.com (May 2020)</li> <li>Facilitating Data discovery with Apache Atlas and Amundsen by Mariusz G\u00f3rski from ING (June 2020)</li> <li>Using Amundsen to Support User Privacy via Metadata Collection at Square by Alyssa Ransbury from Square (July 14, 2020)</li> <li>Amundsen Joins LF AI as New Incubation Project (Aug 11, 2020)</li> <li>Amundsen: one year later (Oct 6, 2020)</li> </ul>"},{"location":"#talks","title":"Talks","text":"<ul> <li>Disrupting Data Discovery {slides, recording} (Strata SF, March 2019)</li> <li>Amundsen: A Data Discovery Platform from Lyft {slides} (Data Council SF, April 2019)</li> <li>Disrupting Data Discovery {slides} (Strata London, May 2019)</li> <li>ING Data Analytics Platform (Amundsen is mentioned) {slides, recording } (Kubecon Barcelona, May 2019)</li> <li>Disrupting Data Discovery {slides, recording} (Making Big Data Easy SF, May 2019)</li> <li>Disrupting Data Discovery {slides, recording} (Neo4j Graph Tour Santa Monica, September 2019)</li> <li>Disrupting Data Discovery {slides} (IDEAS SoCal AI &amp; Data Science Conference, Oct 2019)</li> <li>Data Discovery with Amundsen by Gerard Toonstra from Coolblue {slides} and {talk} (BigData Vilnius 2019)</li> <li>Towards Enterprise Grade Data Discovery and Data Lineage with Apache Atlas and Amundsen by Verdan Mahmood and Marek Wiewiorka from ING {slides, talk} (Big Data Technology Warsaw Summit 2020)</li> <li>Airflow @ Lyft (which covers how we integrate Airflow and Amundsen) by Tao Feng {slides and website} (Airflow Summit 2020)</li> <li>Data DAGs with lineage for fun and for profit by Bolke de Bruin {website} (Airflow Summit 2020)</li> <li>Solving Data Discovery Challenges at Lyft with Amundsen, an Open-source Metadata Platform by Tao Feng (Data+AI summit Europe 2020)</li> <li>Data Discovery at Databricks with Amundsen by Tao Feng and Tianru Zhou (Data+AI summit NA 2021)</li> </ul>"},{"location":"#related-articles","title":"Related Articles","text":"<ul> <li>How LinkedIn, Uber, Lyft, Airbnb and Netflix are Solving Data Management and Discovery for Machine Learning Solutions</li> <li>Data Discovery in 2020</li> <li>4 Data Trends to Watch in 2020</li> <li>Work-Bench Snapshot: The Evolution of Data Discovery &amp; Catalog</li> <li>Future of Data Engineering</li> <li>Governance and Discovery</li> <li>A Data Engineer\u2019s Perspective On Data Democratization</li> <li>Graph Technology Landscape 2020</li> <li>In-house Data Discovery platforms</li> <li>Linux Foundation AI Foundation Landscape</li> <li>Lyft\u2019s Amundsen: Data-Discovery with Built-In Trust</li> <li>How to find and organize your data from the command-line</li> <li>Cataloging Tools for Data Teams</li> <li>An Overview of Data Discovery Platforms and Open Source Solutions</li> <li>Hacking Data Discovery in AWS with Amundsen at SEEK</li> <li>A step-by-step guide deploying Amundsen on Google Cloud Platform</li> <li>Machine Learning Features discovery with Feast and Amundsen</li> <li>Data discovery at REA group</li> <li>Integrating Slack with Amundsen for Ease of Data Discovery</li> <li>Building a data discovery solution with Amundsen and Amazon Neptune</li> <li>Amundsen \u2014 Installing in an Istio-enabled environment</li> <li>Amundsen \u2014 Integrate with Okta SingleSignOn</li> <li>Data Engineering Skills</li> </ul>"},{"location":"#community-meetings","title":"Community meetings","text":"<p>Community meetings are held on the first Thursday of every month at 9 AM Pacific, Noon Eastern, 6 PM Central European Time. Link to join</p>"},{"location":"#upcoming-meetings-notes","title":"Upcoming meetings &amp; notes","text":"<p>You can the exact date for the next meeting and the agenda a few weeks before the meeting in this doc.</p> <p>Notes from all past meetings are available here.</p>"},{"location":"#who-uses-amundsen","title":"Who uses Amundsen?","text":"<p>Here is the list of organizations that are officially using Amundsen today. If your organization uses Amundsen, please file a PR and update this list.</p> <ul> <li>Asana</li> <li>Bang &amp; Olufsen</li> <li>Brex</li> <li>Cameo</li> <li>Chan Zuckerberg Initiative</li> <li>Cimpress Technology</li> <li>Coles Group</li> <li>Convoy</li> <li>Data Sprints</li> <li>Dcard</li> <li>Delivery Hero</li> </ul> <ul> <li>Devoted Health</li> <li>DHI Group</li> <li>Edmunds</li> <li>Everfi</li> <li>Gusto</li> <li>Hurb</li> <li>ING</li> <li>Instacart</li> <li>iRobot</li> <li>Lett</li> <li>LMC</li> <li>Loft</li> <li>Lyft</li> </ul> <ul> <li>Merlin</li> <li>PicPay</li> <li>Plarium Krasnodar</li> <li>PUBG</li> <li>Rapido</li> <li>REA Group</li> <li>Remitly</li> <li>Snap</li> <li>Square</li> <li>Tile</li> <li>WePay</li> <li>WeTransfer</li> <li>Workday</li> </ul>"},{"location":"#contributors","title":"Contributors \u2728","text":"<p>Thanks goes to these incredible people:</p> <p> </p>"},{"location":"CONTRIBUTING/","title":"Contributing Guide","text":""},{"location":"CONTRIBUTING/#first-time-contributors","title":"First-time Contributors","text":"<p>If this is your first contribution to open source, you can follow this tutorial or check this video series to learn about the contribution workflow with GitHub.</p> <p>We always have tickets labeled \u2018good first issue\u2019 and \u2018help wanted\u2019. These are a great starting point if you want to contribute. Don\u2019t hesitate to ask questions in the Amundsen Slack channel about the issue if you are not sure about the strategy to follow.</p>"},{"location":"CONTRIBUTING/#reporting-an-issue","title":"Reporting an Issue","text":"<p>The easiest way you can contribute to Amundsen is by creating issues. First, search the issues section of the Amundsen repository in case a similar bug or feature request already exists. If you don\u2019t find it, submit your bug, question, proposal or feature request. They will remain closed until sufficient interest, e.g. \ud83d\udc4d reactions, has been shown by the community.</p>"},{"location":"CONTRIBUTING/#creating-pull-requests","title":"Creating Pull Requests","text":"<p>Before sending a pull request with significant changes, please use the issue tracker to discuss the potential improvements you want to make. This can help us send you to a solution, a workaround, or an RFC (request for comments item in our RFC repo).</p>"},{"location":"CONTRIBUTING/#requesting-a-feature","title":"Requesting a Feature","text":"<p>We have created a community roadmap where you can vote on plans for next releases. However, we are open to hearing your ideas for new features!</p> <p>For that, you can create an issue and select the \u201cFeature Proposal\u201d template. Fill in as much information as possible, and if you can, add responses to the following questions:</p> <ul> <li>Will we need to add a new model or change any existing model?</li> <li>What would the migration plan look like? Will it be backwards-compatible?</li> <li>Which alternatives did you consider?</li> </ul>"},{"location":"CONTRIBUTING/#setup","title":"Setup","text":"<p>To start contributing to Amundsen, you need to set up your machine to develop with the project. For that, we have prepared a developer guide that will guide you to set up your environment to develop locally with Amundsen.</p>"},{"location":"CONTRIBUTING/#next-steps","title":"Next Steps","text":"<p>Once you have your environment set and ready to go, you can check our documentation and the project\u2019s community roadmap to see what\u2019s coming.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>The following diagram shows the overall architecture for Amundsen. </p>"},{"location":"architecture/#frontend","title":"Frontend","text":"<p>The frontend service serves as web UI portal for users interaction. It is Flask-based web app which representation layer is built with React with Redux, Bootstrap, Webpack, and Babel.</p>"},{"location":"architecture/#search","title":"Search","text":"<p>The search service proxy leverages Elasticsearch\u2019s search functionality (or Apache Atlas\u2019s search API, if that\u2019s the backend you picked) and provides a RESTful API to serve search requests from the frontend service. This API is documented and live explorable through OpenAPI aka \u201cSwagger\u201d. Currently only table resources are indexed and searchable. The search index is built with the databuilder elasticsearch publisher.</p>"},{"location":"architecture/#metadata","title":"Metadata","text":"<p>The metadata service currently uses a Neo4j proxy to interact with Neo4j graph db and serves frontend service\u2019s metadata. The metadata is represented as a graph model:  The above diagram shows how metadata is modeled in Amundsen.</p>"},{"location":"architecture/#databuilder","title":"Databuilder","text":"<p>Amundsen provides a data ingestion library for building the metadata. At Lyft, we build the metadata once a day using an Airflow DAG (examples).</p> <p>In addition to \u201creal use\u201d the databuilder is also employed as a handy tool to ingest some \u201cpre-cooked\u201d demo data used in the Quickstart guide. This allows you to have a supersmall sample of data to explore so many of the features in Amundsen are lit up without you even having to setup any connections to databases etc. to ingest real data.</p>"},{"location":"deployment-best-practices/","title":"Deployment best practices","text":"<p>Amundsen allows for many modifications, and many require code-level modifications. Until we put together a \u201cpaved path\u201d suggestion on how to manage such a set-up, for now we will document what community members are doing independenly. If you have a production Amundsen deployment, please edit this doc to describe your setup.</p>"},{"location":"deployment-best-practices/#notes-from-community-meeting-2020-12-03","title":"Notes from community meeting 2020-12-03","text":"<p>These notes are from 2 companies a community round-table: https://www.youtube.com/watch?v=gVf7S98bnyg</p>"},{"location":"deployment-best-practices/#brex","title":"Brex","text":"<ul> <li>What modifications have you made?<ul> <li>We\u2019ve added backups</li> <li>We wanted table descriptions to come solely from code.</li> </ul> </li> <li>How do you deploy/secure Amundsen?<ul> <li>Our hosting is behind VPN, use OIDC</li> </ul> </li> <li>What do you use Amundsen for?<ul> <li>Our primary use case: if I change this table, what dashboards will it break.</li> <li>We also do PII tagging</li> <li>ETL pipeline puts docs in Snowflake</li> </ul> </li> </ul>"},{"location":"deployment-best-practices/#rea-group","title":"REA group","text":"<ul> <li>Why did you choose Amundsen?<ul> <li>We don\u2019t have data stewards or formal roles who work on documentation. We liked that Amundsen didn\u2019t rely on curated/manual documentation.</li> <li>Google Data Catalog doesn\u2019t allow you to search for data that you don\u2019t have access to.</li> <li>Things that we considered in other vendors - business metric glossary, column level lineage.</li> </ul> </li> <li>How do you deploy Amundsen?<ul> <li>Deployment on ECS. Built docker images on our own.</li> <li>Deployment is done so that metadata is not lost. Looked into backing metadata in AWS, but decided not to. Instead use block storage so even if the instance goes down, the metadata is still there.</li> <li>We only index prod data sets.</li> <li>We don\u2019t consider Amundsen as a source of truth. Thus, we don\u2019t let people to enable update descriptions.</li> <li>ETL indexer gets descriptions from BQ and puts it into Amundsen.</li> <li>Postgres/source tables need some work to get descriptions from Go into Amundsen.</li> </ul> </li> <li>Some changes we\u2019d like to make:<ul> <li>Authentication and Authorization<ul> <li>Workflow for requesting access to data you can\u2019t already access: right now we don\u2019t have a workflow for requesting access that\u2019s connected to Amundsen. Seems like an area of investment.</li> </ul> </li> <li>Data Lineage</li> <li>Business metrics glossary</li> </ul> </li> <li>Q&amp;A<ul> <li>Why build your own images?<ul> <li>Want to make sure system image and code running on the image should be tightly controlled. Patch over the specific files on top of Amundsen upstream code. Don\u2019t fork right now. We chose to patch and not fork.</li> </ul> </li> </ul> </li> <li>What was the process of getting alpha users onboard and getting feedback?<ul> <li>Chose ~8 people who had different roles and different tenure. Then did UX interviews.</li> </ul> </li> </ul>"},{"location":"developer_guide/","title":"Developer Guide","text":"<p>This repository uses <code>git submodules</code> to link the code for all of Amundsen\u2019s libraries into a central location. This document offers guidance on how to develop locally with this setup.</p> <p>This workflow leverages <code>docker</code> and <code>docker-compose</code> in a very similar manner to our installation documentation, to spin up instances of all 3 of Amundsen\u2019s services connected with an instances of Neo4j and ElasticSearch which ingest dummy data.</p>"},{"location":"developer_guide/#cloning-the-repository","title":"Cloning the Repository","text":"<p>If cloning the repository for the first time, run the following command to clone the repository and pull the submodules:</p> <pre><code>$ git clone --recursive git@github.com:amundsen-io/amundsen.git\n</code></pre> <p>If  you have already cloned the repository but your submodules are empty, from your cloned <code>amundsen</code> directory run:</p> <pre><code>$ git submodule init\n$ git submodule update\n</code></pre> <p>After cloning the repository you can change directories into any of the upstream folders and work in those directories as you normally would. You will have full access to all of the git features, and working in the upstream directories will function the same as if you were working in a cloned version of that repository.</p>"},{"location":"developer_guide/#local-development","title":"Local Development","text":""},{"location":"developer_guide/#ensure-you-have-the-latest-code","title":"Ensure you have the latest code","text":"<p>Beyond running <code>git pull origin master</code> in your local <code>amundsen</code> directory, the submodules for our libraries also have to be manually updated to point to the latest versions of each libraries\u2019 code. When creating a new branch on <code>amundsen</code> to begin local work, ensure your local submodules are pointing to the latest code for each library by running:</p> <pre><code>$ git submodule update --remote\n</code></pre>"},{"location":"developer_guide/#building-local-changes","title":"Building local changes","text":"<ol> <li> <p>First, be sure that you have first followed the installation documentation and can spin up a default version of Amundsen without any issues. If you have already completed this step, be sure to have stopped and removed those containers by running:     <pre><code>$ docker-compose -f docker-amundsen.yml down\n</code></pre></p> </li> <li> <p>Launch the containers needed for local development (the <code>-d</code> option launches in background) :     <pre><code>$ docker-compose -f docker-amundsen-local.yml up -d\n</code></pre></p> </li> <li> <p>After making local changes rebuild and relaunch modified containers:     <pre><code>$ docker-compose -f docker-amundsen-local.yml build \\\n  &amp;&amp; docker-compose -f docker-amundsen-local.yml up -d\n</code></pre></p> </li> <li> <p>Optionally, to still tail logs, in a different terminal you can:     <pre><code>$ docker-compose -f docker-amundsen-local.yml logs --tail=3 -f\n## - or just tail single container(s):\n$ docker logs amundsenmetadata --tail 10 -f\n</code></pre></p> </li> </ol>"},{"location":"developer_guide/#local-data","title":"Local data","text":"<p>Local data is persisted under .local/ (at the root of the project), clean up the following directories to reset the databases:</p> <pre><code>#  reset elasticsearch\nrm -rf .local/elasticsearch\n\n#  reset neo4j\nrm -rf .local/neo4j\n</code></pre>"},{"location":"developer_guide/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If you have made a change in <code>amundsen/amundsenfrontendlibrary</code> and do not see your changes, this could be due to your browser\u2019s caching behaviors. Either execute a hard refresh (recommended) or clear your browser cache (last resort).</li> </ol>"},{"location":"developer_guide/#testing-amundsen-frontend-locally","title":"Testing Amundsen frontend locally","text":"<p>Amundsen has an instruction regarding local frontend launch here</p> <p>Here are some additional changes you might need for windows (OS Win 10):</p> <ul> <li>amundsen_application/config.py, set LOCAL_HOST = \u2018127.0.0.1\u2019</li> <li>amundsen_application/wsgi.py, set host=\u2018127.0.0.1\u2019  (for other microservices also need to change <code>port</code> here because the default is 5000)</li> </ul> <p>(using that approach you can run locally another microservices as well if needed)  </p> <p>Once you have a running frontend microservice, the rest of Amundsen components can be launched with docker-compose from the root Amundsen project (don\u2019t forget to remove frontend microservice section from docker-amundsen.yml): <code>docker-compose -f docker-amundsen.yml up</code> https://github.com/amundsen-io/amundsen/blob/main/docs/installation.md</p>"},{"location":"developer_guide/#developing-dockerbuild-file","title":"Developing Dockerbuild file","text":"<p>When making edits to Dockerbuild file (docker-amundsen-local.yml) it is good to see what you are getting wrong locally. To do that you build it <code>docker build .</code></p> <p>And then the output should include a line like so at the step right before it failed:</p> <pre><code>Step 3/20 : RUN git clone --recursive git://github.com/amundsen-io/amundsenfrontendlibrary.git  &amp;&amp; cd amundsenfrontendlibrary  &amp;&amp; git submodule foreach git pull origin master\n ---&gt; Using cache\n ---&gt; ec052612747e\n</code></pre> <p>You can then launch a container from this image like so</p> <pre><code>docker container run -it --name=debug ec052612747e /bin/sh\n</code></pre>"},{"location":"developer_guide/#building-and-testing-amundsen-frontend-docker-image-or-any-other-service","title":"Building and Testing Amundsen Frontend Docker Image (or any other service)","text":"<ol> <li>Build your image <code>docker build --no-cache .</code> it is recommended that you use \u2013no-cache so you aren\u2019t accidentally using an old version of an image.</li> <li>Determine the hash of your images by running <code>docker images</code> and getting the id of your most recent image</li> <li>Go to your locally cloned amundsen repo and edit the docker compose file \u201cdocker-amundsen.yml\u201d to have  the amundsenfrontend image point to the hash of the image that you built</li> </ol> <pre><code>  amundsenfrontend:\n      #image: amundsendev/amundsen-frontend:1.0.9\n      #image: 1234.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:2020-01-21\n      image: 0312d0ac3938\n</code></pre>"},{"location":"developer_guide/#pushing-image-to-ecr-and-using-in-k8s","title":"Pushing image to ECR and using in K8s","text":"<p>Assumptions:</p> <ul> <li>You have an aws account</li> <li> <p>You have aws command line set up and ready to go</p> </li> <li> <p>Choose an ECR repository you\u2019d like to push to (or create a new one) https://us-west-2.console.aws.amazon.com/ecr/repositories</p> </li> <li> <p>Click onto repository name and open \u201cView push commands\u201d cheat sheet 2b. Login</p> <p>it would look something like this:</p> <p><code>aws ecr get-login --no-include-email --region us-west-2</code> Then execute what is returned by above</p> </li> <li> <p>Follow the instructions (you may need to install first AWS CLI, aws-okta and configure your AWS credentials if you haven\u2019t done it before) Given image name is amundsen-frontend, build, tag and push commands will be the following: Here you can see the tag is YYYY-MM-dd but you should choose whatever you like.      <pre><code>docker build -t amundsen-frontend:{YYYY-MM-dd} .\ndocker tag amundsen-frontend:{YYYY-MM-dd} &lt;?&gt;.dkr.ecr.&lt;?&gt;.amazonaws.com/amundsen-frontend:{YYYY-MM-dd}\ndocker push &lt;?&gt;.dkr.ecr.&lt;?&gt;.amazonaws.com/amundsen-frontend:{YYYY-MM-dd}\n</code></pre></p> </li> <li> <p>Go to the <code>helm/{env}/amundsen/values.yaml</code> and modify to the image tag that you want to use.</p> </li> <li> <p>When updating amundsen-frontend, make sure to do a hard refresh of amundsen with emptying the cache, otherwise you will see stale version of webpage.</p> </li> </ul>"},{"location":"developer_guide/#test-search-service-in-local-using-staging-or-production-data","title":"Test search service in local using staging or production data","text":"<p>To test in local, we need to stand up Elasticsearch, publish index data, and stand up Elastic search</p>"},{"location":"developer_guide/#standup-elasticsearch","title":"Standup Elasticsearch","text":"<p>Running Elasticsearch via Docker. To install Docker, go here Example:</p> <pre><code>docker run -p 9200:9200  -p 9300:9300  -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:6.2.4\n</code></pre>"},{"location":"developer_guide/#optional-standup-kibana","title":"(Optional) Standup Kibana","text":"<pre><code>docker run --link ecstatic_edison:elasticsearch -p 5601:5601 docker.elastic.co/kibana/kibana:6.2.4\n</code></pre> <p>*Note that <code>ecstatic_edison</code> is container_id of Elasticsearch container. Update it if it\u2019s different by looking at <code>docker ps</code></p>"},{"location":"developer_guide/#publish-table-index-through-databuilder","title":"Publish Table index through Databuilder","text":""},{"location":"developer_guide/#install-databuilder","title":"Install Databuilder","text":"<pre><code>cd ~/src/\ngit clone git@github.com:amundsen-io/amundsendatabuilder.git\ncd ~/src/amundsendatabuilder\nvirtualenv venv\nsource venv/bin/activate\npython setup.py install\npip install -r requirements.txt\n</code></pre>"},{"location":"developer_guide/#publish-table-index","title":"Publish Table index","text":"<p>First fill this two environment variables: <code>NEO4J_ENDPOINT</code> , <code>CREDENTIALS_NEO4J_PASSWORD</code></p> <pre><code>$ python\n\nimport logging  \nimport os  \nimport uuid\n\nfrom elasticsearch import Elasticsearch  \nfrom pyhocon import ConfigFactory\n\nfrom databuilder.extractor.neo4j_extractor import Neo4jExtractor  \nfrom databuilder.extractor.neo4j_search_data_extractor import Neo4jSearchDataExtractor  \nfrom databuilder.job.job import DefaultJob  \nfrom databuilder.loader.file_system_elasticsearch_json_loader import FSElasticsearchJSONLoader  \nfrom databuilder.publisher.elasticsearch_publisher import ElasticsearchPublisher  \nfrom databuilder.task.task import DefaultTask\n\nlogging.basicConfig(level=logging.INFO)\n\nneo4j_user = 'neo4j'  \nneo4j_password = os.getenv('CREDENTIALS_NEO4J_PASSWORD')  \nneo4j_endpoint = os.getenv('NEO4J_ENDPOINT')\n\nelasticsearch_client = Elasticsearch([  \n    {'host': 'localhost'},  \n])\n\ndata_file_path = '/var/tmp/amundsen/elasticsearch_upload/es_data.json'\n\nelasticsearch_new_index = 'table_search_index_{hex_str}'.format(hex_str=uuid.uuid4().hex)\nlogging.info(\"Elasticsearch new index: \" + elasticsearch_new_index)\n\nelasticsearch_doc_type = 'table'  \nelasticsearch_index_alias = 'table_search_index'\n\njob_config = ConfigFactory.from_dict({  \n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY):  \n        neo4j_endpoint,  \n  'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY):  \n        'databuilder.models.table_elasticsearch_document.TableESDocument',  \n  'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER):  \n        neo4j_user,  \n  'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW):  \n        neo4j_password,  \n  'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY):  \n        data_file_path,  \n  'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY):  \n        'w',  \n  'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY):  \n        data_file_path,  \n  'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY):  \n        'r',  \n  'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY):  \n        elasticsearch_client,  \n  'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY):  \n        elasticsearch_new_index,  \n  'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY):  \n        elasticsearch_doc_type,  \n  'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY):  \n        elasticsearch_index_alias,  \n})\n\njob = DefaultJob(conf=job_config,  \n  task=DefaultTask(extractor=Neo4jSearchDataExtractor(),  \n  loader=FSElasticsearchJSONLoader()),  \n  publisher=ElasticsearchPublisher())  \nif neo4j_password:  \n    job.launch()  \nelse:  \n    raise ValueError('Add environment variable CREDENTIALS_NEO4J_PASSWORD')\n</code></pre>"},{"location":"developer_guide/#standup-search-service","title":"Standup Search service","text":"<p>Follow this instruction</p> <p>Test the search API with this command:</p> <pre><code>curl -vv \"http://localhost:5001/search?query_term=test&amp;page_index=0\"\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#how-to-select-between-neo4j-and-atlas-as-backend-for-amundsen","title":"How to select between Neo4j and Atlas as backend for Amundsen?","text":""},{"location":"faq/#why-neo4j","title":"Why Neo4j?","text":"<ol> <li>Amundsen has direct influence over the data model if you use neo4j. This, at least initially, will benefit the speed by which new features in amundsen can arrive.</li> <li>Neo4j for it is the market leader in Graph database and also was proven by Airbnb\u2019s Data portal on their Data discovery tool.</li> </ol>"},{"location":"faq/#why-atlas","title":"Why Atlas?","text":"<ol> <li>Atlas has lineage support already available. It\u2019s been tried and tested.</li> <li>Tag/Badge propagation is supported.</li> <li>It has a robust authentication and authorization system.</li> <li>Atlas does data governance adding Amundsen for discovery makes it best of both worlds.</li> <li>Apache Atlas is the only proxy in Amundsen supporting both push and pull approaches for collecting metadata:<ul> <li><code>Push</code> method by leveraging Apache Atlas Hive Hook. It\u2019s an event listener running alongside Hive Metastore, translating Hive Metastore events into Apache Atlas entities and <code>pushing</code> them to Kafka topic, from which Apache Atlas ingests the data by internal processes.</li> <li><code>Pull</code> method by leveraging Amundsen Databuilder integration with Apache Atlas. It means that extractors available in Databuilder can be used to collect metadata about external systems (like PostgresMetadataExtractor) and sending them to Apache Atlas in a shape consumable by Amundsen. Amundsen &lt;&gt; Atlas integration is prepared in such way that you can use both push and pull models at the same time.</li> </ul> </li> <li>The free version of Neo4j does not have authorization support (Enterprise version does). Your question should actually be why use \u201cneo4j over janusgraph\u201d cause that is the right level of comparison. Atlas adds a whole bunch on top of the graph database.</li> </ol>"},{"location":"faq/#why-not-atlas","title":"Why not Atlas?","text":"<ol> <li>Atlas seems to have a slow development cycle and it\u2019s community is not very responsive although some small improvements have been made.</li> <li>Atlas integration has less community support meaning new features might land slightly later for Atlas in comparison to Neo4j</li> </ol>"},{"location":"faq/#what-are-the-prerequisites-to-use-apache-atlas-as-backend-for-amundsen","title":"What are the prerequisites to use Apache Atlas as backend for Amundsen?","text":"<p>To run Amundsen with Atlas, latest versions of following components should be used: 1. Apache Atlas - built from <code>master</code> branch. Ref <code>103e867cc126ddb84e64bf262791a01a55bee6e5</code> (or higher). 2. amundsenatlastypes - library for installing Atlas entity definitions specific to Amundsen integration. Version <code>1.3.0</code> (or higher).</p>"},{"location":"faq/#how-to-migrate-from-amundsen-1x-2x","title":"How to migrate from Amundsen 1.x -&gt; 2.x?","text":"<p>v2.0 renames a handful of fields in the services to be more consistent. Unfortunately one side effect is that the 2.0 versions of the services will need to be deployed simultaneously, as they are not interoperable with the 1.x versions.</p> <p>Additionally, some indexed field names in the elasticsearch document change as well, so if you\u2019re using elasticsearch, you\u2019ll need to republish Elasticsearch index via Databuilder job.</p> <p>The data in the metadata store, however, can be preserved when migrating from 1.x to 2.0.</p> <p>v2.0 deployments consists of deployment of all three services along with republishing Elasticsearch document on Table with v2.0 Databuilder.</p> <p>Keep in mind there is likely to be some downtime as v2.0.0, between deploying 3 services and re-seeding the elasticsearch indexes, so it might be ideal to stage a rollout by datacenter/environment if uptime is key</p>"},{"location":"faq/#how-to-avoid-certain-metadatas-in-amundsen-got-erased-by-databuilder-ingestion","title":"How to avoid certain metadatas in Amundsen got erased by databuilder ingestion?","text":"<p>By default, databuilder always upserts the metadata. If you want to prevent that happens on certain type of metadata, you could add the following config to your databuilder job\u2019s config</p> <pre><code>'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_CREATE_ONLY_NODES): [DESCRIPTION_NODE_LABEL],\n</code></pre> <p>This config means that databuilder will only update the table / column description if it doesn\u2019t exist before which could be the table is newly created. This is useful when we treat Amundsen graph as the source of truth for certain types of metadata (e.g description).</p>"},{"location":"faq/#how-to-capture-all-google-analytics","title":"How to capture all Google Analytics?","text":"<p>Users are likely to have some sort of adblocker installed, making your Google Analytics less accurate.</p> <p>To put a proxy in place to bypass any adblockers and capture all analytics, follow these steps:</p> <ol> <li>Follow https://github.com/ZitRos/save-analytics-from-content-blockers#setup to set up your own proxy server.</li> <li>In the same repository, run <code>npm run mask www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX</code> and save the output.</li> <li>In your custom frontend, override https://github.com/amundsen-io/amundsenfrontendlibrary/blob/master/amundsen_application/static/templates/fragments/google-analytics-loader.html#L6 to </li> <li>Now, note that network requests to www.googletagmanager.com will be sent from behind your masked proxy endpoint, saving your analytics from content blockers!</li> </ol>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#bootstrap-a-default-version-of-amundsen-using-docker","title":"Bootstrap a default version of Amundsen using Docker","text":"<p>The following instructions are for setting up a version of Amundsen using Docker.</p> <ol> <li>Make sure you have at least 3GB of disk space available to Docker. Install <code>docker</code> and  <code>docker-compose</code>.</li> <li>Clone this repo and its submodules by running:    <pre><code>$ git clone --recursive https://github.com/amundsen-io/amundsen.git\n</code></pre></li> <li>Enter the cloned directory and run the command below:     <pre><code># For Neo4j Backend\n$ docker-compose -f docker-amundsen.yml up\n\n# For Atlas\n$ docker-compose -f docker-amundsen-atlas.yml up\n</code></pre>     If it\u2019s your first time, you may want to proactively go through troubleshooting steps, especially the first one related to heap memory for ElasticSearch and Docker engine memory allocation (leading to Docker error 137).</li> <li> <p>Ingest provided sample data into Neo4j by doing the following: (Please skip if you are using Atlas backend)</p> </li> <li> <p>In a separate terminal window, change directory to databuilder.</p> </li> <li><code>sample_data_loader</code> python script included in <code>examples/</code> directory uses elasticsearch client, pyhocon and other libraries. Install the dependencies in a virtual env and run the script by following the commands below. See Windows Troubleshooting if you encounter an error on <code>python3 setup.py install</code> regarding <code>extas_require</code> on windows.    <pre><code> $ python3 -m venv venv\n $ source venv/bin/activate\n $ pip3 install --upgrade pip\n $ pip3 install -r requirements.txt\n $ python3 setup.py install\n $ python3 example/scripts/sample_data_loader.py\n</code></pre></li> <li> <p>View UI at <code>http://localhost:5000</code> and try to search <code>test</code>, it should return some result. </p> </li> <li> <p>We could also perform an exact-match search for the table entity. For example: search <code>test_table1</code> in table field and it\u2019ll return the records that matched. </p> </li> </ol> <p>Atlas Note: Atlas takes some time to boot properly. So you may not be able to see the results immediately  after you run the <code>docker-compose up</code> command.  Atlas would be ready once you\u2019ll have the following output in the docker output <code>Amundsen Entity Definitions Created...</code> </p>"},{"location":"installation/#verify-your-setup","title":"Verify your setup","text":"<ol> <li>You can verify the dummy data has been ingested into Neo4j by by visiting <code>http://localhost:7474/browser/</code> and run <code>MATCH (n:Table) RETURN n LIMIT 25</code> in the query box. You should see few tables. </li> <li>You can verify the data has been loaded into the metadataservice by visiting:</li> <li><code>http://localhost:5000/table_detail/gold/hive/test_schema/test_table1</code></li> <li><code>http://localhost:5000/table_detail/gold/dynamo/test_schema/test_table2</code></li> </ol>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>If the docker container doesn\u2019t have enough heap memory for Elastic Search, <code>es_amundsen</code> will fail during <code>docker-compose</code>.</li> <li>docker-compose error: <code>es_amundsen | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</code></li> <li> <p>Increase the heap memory detailed instructions here</p> <ol> <li>Edit <code>/etc/sysctl.conf</code></li> <li>Make entry <code>vm.max_map_count=262144</code>. Save and exit.</li> <li>Reload settings <code>$ sysctl -p</code></li> <li>Restart <code>docker-compose</code></li> </ol> </li> <li> <p>If <code>docker-amundsen-local.yml</code> stops because of <code>org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: Failed to create node environment</code>, then <code>es_amundsen</code> cannot write to <code>.local/elasticsearch</code>. </p> </li> <li><code>chown -R 1000:1000 .local/elasticsearch</code></li> <li>Restart <code>docker-compose</code> </li> <li>If when running the sample data loader you recieve a connection error related to ElasticSearch or like this for Neo4j: <pre><code>    Traceback (most recent call last):\n      File \"/home/ubuntu/amundsen/amundsendatabuilder/venv/lib/python3.6/site-packages/neobolt/direct.py\", line 831, in _connect\n        s.connect(resolved_address)\n    ConnectionRefusedError: [Errno 111] Connection refused\n</code></pre></li> <li> <p>If <code>elastic search</code> container stops with an error <code>max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]</code>, then add the below code to the file <code>docker-amundsen-local.yml</code> in the <code>elasticsearch</code> definition. <code>ulimits:    nofile:      soft: 65535      hard: 65535</code>    Then check if all 5 Amundsen related containers are running with <code>docker ps</code>? Can you connect to the Neo4j UI at http://localhost:7474/browser/ and similarly the raw ES API at http://localhost:9200? Does Docker logs reveal any serious issues?</p> </li> <li> <p>If ES container crashed with Docker error 137 on the first call from the website (http://localhost:5000/), this is because you are using the default Docker engine memory allocation of 2GB. The minimum needed for all the containers to run with the loaded sample data is 3GB. To do this go to your <code>Docker -&gt; Preferences -&gt; Resources -&gt; Advanced</code> and increase the <code>Memory</code>, then restart the Docker engine. </p> </li> <li> <p>Windows Troubleshooting</p> </li> </ol>"},{"location":"issue_labeling/","title":"Issue and Feature Labeling","text":"<p>On Amundsen, we aim to be methodical on using issue labels, offering our community a way to understand what are the issues about and their status within or development process.</p> <p>We use a bunch of GitHub labels. They are a mix of custom labels and the default Github labels for open-source projects. We base these labels on four main types: status labels, issue type labels, area labels, and the \u201cother\u201d category. Read on to learn more about them.</p>"},{"location":"issue_labeling/#status-labels","title":"Status Labels","text":"<ul> <li>They show at a glance the status and progress of each issue</li> <li>Prefixed with \u201cStatus:\u201d, followed by the label</li> <li>Only one status label will be applied to any particular issue</li> </ul>"},{"location":"issue_labeling/#labels","title":"Labels","text":"<ul> <li>status:needs_triage \u2013 For all issues that need to be processed</li> <li>status:needs_reproducing \u2013 For bugs that need to be reproduced in order to get fixed</li> <li>status:needs_votes \u2013 Issue or bug fix that needs support from the community to be considered</li> <li>status:in_progress \u2013 Issue that is being worked on right now.</li> <li>status:completed \u2013 Issue is completed and on main</li> <li>status:abandoned \u2013 Issue we won\u2019t go ahead and implement, or that needs a \u201cchampion\u201d to take it through</li> <li>status:blocked \u2013 Issue blocked by any reason (dependencies, previous work, lack of resources, etc.)</li> </ul> <p>Here is a diagram representing these states within the lifecycles: </p>"},{"location":"issue_labeling/#type-labels","title":"Type Labels","text":"<ul> <li>They show the type of the issue</li> <li>Prefixed with \u201cType:\u201d, followed by the label</li> </ul>"},{"location":"issue_labeling/#labels_1","title":"Labels","text":"<ul> <li>type:bug \u2013 An unexpected problem or unintended behavior</li> <li>type:feature \u2013 A new feature request</li> <li>type:maintenance \u2013 A regular maintenance chore or task, including refactors, build system, CI, performance improvements</li> <li>type:documentation \u2013 A documentation improvement task</li> <li>type:question \u2013 An issue or PR that needs more information or a user question</li> </ul>"},{"location":"issue_labeling/#area-labels","title":"Area Labels","text":"<ul> <li>They indicate which area of the project the issue refers to</li> <li>Prefixed with \u201cArea:\u201d, followed by the name of the project</li> </ul>"},{"location":"issue_labeling/#labels_2","title":"Labels","text":"<ul> <li>area:common \u2013 From common</li> <li>area:databuilder \u2013 From databuilder</li> <li>area:frontend \u2013 From frontend</li> <li>area:metadata \u2013 From metadata library</li> <li>area:search \u2013 From search library</li> <li>area:k8s \u2013 Related to the Kubernetes helm chart</li> <li>area:all \u2013 Related to all the projects above</li> </ul>"},{"location":"issue_labeling/#other-labels","title":"Other Labels","text":"<ul> <li>Some of these are part of the standard GitHub labels and intended for OSS contributors</li> <li>Some are related to the tools we use to maintain the library</li> <li>They are not prefixed</li> </ul>"},{"location":"issue_labeling/#labels_3","title":"Labels","text":"<ul> <li>help wanted \u2013 Indicates we are looking for contributors on this issue.</li> <li>good first issue \u2013 Indicates the issue is a great one to tackle by newcomers to the project or OSS in general.</li> <li>rfc - Indicates that there is an RFC associated with this issue.</li> </ul>"},{"location":"k8s_install/","title":"Amundsen K8s Helm Charts","text":"<p>Source code can be found here</p>"},{"location":"k8s_install/#what-is-this","title":"What is this?","text":"<p>This is setup templates for deploying amundsen on k8s (kubernetes), using helm.</p>"},{"location":"k8s_install/#how-do-i-get-started","title":"How do I get started?","text":"<ol> <li>Make sure you have the following command line clients setup:<ul> <li>k8s (kubectl)</li> <li>helm</li> </ul> </li> <li>Build out a cloud based k8s cluster, such as Amazon EKS</li> <li>Ensure you can connect to your cluster with cli tools in step 1.</li> </ol>"},{"location":"k8s_install/#prerequisites","title":"Prerequisites","text":"<ol> <li>Helm 2.14+</li> <li>Kubernetes 1.14+</li> </ol>"},{"location":"k8s_install/#chart-requirements","title":"Chart Requirements","text":"<p>Note we updated from elasticsearch 6 to elasticsearch 7 </p> Repository Name Version https://helm.elastic.co elasticsearch 7.13.4"},{"location":"k8s_install/#values","title":"Values","text":"Key Type Default Description LONG_RANDOM_STRING int <code>1234</code> A long random string. You should probably provide your own. This is needed for OIDC. affinity object <code>{}</code> amundsen application wide configuration of affinity. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref clusterDomain string <code>\"cluster.local\"</code> dnsZone string <code>\"teamname.company.com\"</code> DEPRECATED - its not standard to pre construct urls this way. The dns zone (e.g. group-qa.myaccount.company.com) the app is running in. Used to construct dns hostnames (on aws only). dockerhubImagePath string <code>\"amundsendev\"</code> DEPRECATED - this is not useful, it would be better to just allow the whole image to be swapped instead. The image path for dockerhub. elasticsearch.enabled bool <code>true</code> set this to false, if you want to provide your own ES instance. elasticsearch.esJavaOpts string <code>\"-Xmx8g -Xms8g\"</code> set init memory size (Xms) and maximum memory size (Xmx) for the es jvm. elasticsearch.fullnameOverride string <code>\"amundsen-elasticsearch-master\"</code> this is the service name of the amundsen elasticsearch master. Change it if you want to give a new name for the elasticsearch service elasticsearch.image string <code>\"elasticsearch\"</code> elasticsearch docker image name elasticsearch.resources object <code>{\"limits\":{\"memory\":\"15Gi\"},\"requests\":{\"memory\":\"10Gi\"}}</code> set the pod resources elasticsearch.sysctlInitContainer object <code>{\"enabled\":false}</code> If this set to true, the es pod will require some admin privilege, which is not allowed in most case. So set it to false environment string <code>\"dev\"</code> DEPRECATED - its not standard to pre construct urls this way. The environment the app is running in. Used to construct dns hostnames (on aws only) and ports. flaskApp.class string <code>\"\"</code> The class name within the flaskApp.module flaskApp.module string <code>\"\"</code> Any custom flask module you may need to implement as a wrapper frontEnd.ALL_UNEDITABLE_SCHEMAS string <code>nil</code> Environment variable for allowing/disallowing editing schemas via the UI. All schemas are allowed to be edited by default. Set to \u2018true\u2019 to disallow. See https://www.amundsen.io/amundsen/frontend/docs/flask_config/#uneditable-table-descriptions for more frontEnd.affinity object <code>{}</code> Frontend pod specific affinity. frontEnd.annotations object <code>{}</code> Frontend service specific tolerations. frontEnd.baseUrl string <code>\"http://localhost\"</code> used by notifications util to provide links to amundsen pages in emails. frontEnd.config.class string <code>nil</code> Optional Config class. frontEnd.envVars object <code>{}</code> frontEnd.image string <code>\"amundsendev/amundsen-frontend\"</code> The image of the frontend container. frontEnd.imagePullSecrets list <code>[]</code> Optional pod imagePullSecrets ref frontEnd.imageTag string <code>\"latest\"</code> The image tag of the frontend container. frontEnd.nodeSelector object <code>{}</code> Frontend pod specific nodeSelector. frontEnd.podAnnotations object <code>{}</code> Frontend pod specific annotations. frontEnd.replicas int <code>1</code> How many replicas of the frontend service to run. frontEnd.resources object <code>{}</code> See pod resourcing ref frontEnd.serviceName string <code>\"frontend\"</code> The frontend service name. frontEnd.servicePort int <code>80</code> The port the frontend service will be exposed on via the loadbalancer. frontEnd.serviceType string <code>\"ClusterIP\"</code> The frontend service type. See service types ref frontEnd.tolerations list <code>[]</code> Frontend pod specific tolerations. ingress.annotations object <code>{}</code> ingress.enabled bool <code>true</code> set this to true, if you want a ingress that expose HTTP and HTTPS routes from outside the cluster to your amundsen services. Don\u2019t use this if you are in a public cloud such as AWS, GCP ingress.hosts[0].host string <code>\"amundsen-test.your-domain.com\"</code> ingress.hosts[0].paths[0] string <code>\"/\"</code> ingress.tls[0].hosts[0] string <code>\"amundsen-test.your-domain.com\"</code> metadata.affinity object <code>{}</code> Metadata pod specific affinity. metadata.annotations object <code>{}</code> Metadata service specific tolerations. metadata.envVars object <code>{}</code> metadata.image string <code>\"amundsendev/amundsen-metadata\"</code> The image of the metadata container. metadata.imagePullSecrets list <code>[]</code> Optional pod imagePullSecrets ref metadata.imageTag string <code>\"latest\"</code> The image tag of the metadata container. metadata.nodeSelector object <code>{}</code> Metadata pod specific nodeSelector. metadata.podAnnotations object <code>{}</code> Metadata pod specific annotations. metadata.proxy.host string <code>nil</code> host name / URI of your proxy metadata.proxy.password string <code>nil</code> Credentials - Password of the proxy metadata.proxy.port string <code>nil</code> Port on which the proxy is running metadata.proxy.user string <code>nil</code> Credentials - Username of the proxy metadata.replicas int <code>1</code> How many replicas of the metadata service to run. metadata.resources object <code>{}</code> See pod resourcing ref metadata.serviceName string <code>\"metadata\"</code> The metadata service name. metadata.serviceType string <code>\"ClusterIP\"</code> The metadata service type. See service types ref metadata.tolerations list <code>[]</code> Metadata pod specific tolerations. neo4j.affinity object <code>{}</code> neo4j specific affinity. neo4j.annotations object <code>{}</code> neo4j service specific tolerations. neo4j.backup object <code>{\"enabled\":false,\"podAnnotations\":{},\"s3Path\":\"s3://dev/null\",\"schedule\":\"0 * * * *\"}</code> If enabled is set to true, make sure and set the s3 path as well. neo4j.backup.s3Path string <code>\"s3://dev/null\"</code> The s3path to write to for backups. neo4j.backup.schedule string <code>\"0 * * * *\"</code> The schedule to run backups on. Defaults to hourly. neo4j.config object <code>{\"dbms\":{\"heap_initial_size\":\"1G\",\"heap_max_size\":\"2G\",\"pagecache_size\":\"2G\"}}</code> Neo4j application specific configuration. This type of configuration is why the charts/stable version is not used. See ref neo4j.config.dbms object <code>{\"heap_initial_size\":\"1G\",\"heap_max_size\":\"2G\",\"pagecache_size\":\"2G\"}</code> dbms config for neo4j neo4j.config.dbms.heap_initial_size string <code>\"1G\"</code> the initial java heap for neo4j neo4j.config.dbms.heap_max_size string <code>\"2G\"</code> the max java heap for neo4j neo4j.config.dbms.pagecache_size string <code>\"2G\"</code> the page cache size for neo4j neo4j.enabled bool <code>true</code> If neo4j is enabled as part of this chart, or not. Set this to false if you want to provide your own version. neo4j.image string <code>\"neo4j\"</code> The image of the neo4j container. neo4j.imageTag string <code>\"3.3.0\"</code> The image tag of the neo4j container. neo4j.initPluginsContainer.image string <code>\"appropriate/curl\"</code> The image of the init neo4j plugins container. neo4j.initPluginsContainer.imageTag string <code>\"latest\"</code> The image tag of the init neo4j plugins container. neo4j.initPluginsContainer.command list See values.yaml The command to execute in the init neo4j plugins container. neo4j.nodeSelector object <code>{}</code> neo4j specific nodeSelector. neo4j.persistence object <code>{}</code> Neo4j persistence. Turn this on to keep your data between pod crashes, etc. This is also needed for backups. neo4j.podAnnotations object <code>{}</code> neo4j pod specific annotations. neo4j.resources object <code>{}</code> See pod resourcing ref neo4j.serviceType string <code>\"ClusterIP\"</code> The neo4j service type. See service types ref neo4j.tolerations list <code>[]</code> neo4j specific tolerations. neo4j.version string <code>\"3.3.0\"</code> DEPRECATED - Now using the neo4j.imageTag The neo4j application version used by amundsen. nodeSelector object <code>{}</code> amundsen application wide configuration of nodeSelector. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref oidc.configs.FLASK_OIDC_CONFIG_URL string <code>\"https://accounts.google.com/.well-known/openid-configuration\"</code> oidc.configs.FLASK_OIDC_PROVIDER_NAME string <code>\"google\"</code> oidc.configs.FLASK_OIDC_REDIRECT_URI string <code>\"/auth\"</code> oidc.configs.FLASK_OIDC_SCOPES string <code>\"openid email profile\"</code> oidc.configs.FLASK_OIDC_USER_ID_FIELD string <code>\"email\"</code> oidc.configs.FLASK_OIDC_WHITELISTED_ENDPOINTS string <code>\"status,healthcheck,health\"</code> oidc.enabled bool <code>false</code> flag to enable/disable the OIDC. Once enabled,   - everything under oidc.configs will be parsed   - flaskApp.module will be set as \u2018flaskoidc\u2019   - flaskApp.class will be set as \u2018FlaskOIDC\u2019 oidc.frontend.client_id string <code>\"\"</code> oidc.frontend.client_secret string <code>\"\"</code> oidc.metadata.client_id string <code>\"\"</code> oidc.metadata.client_secret string <code>\"\"</code> oidc.search.client_id string <code>\"\"</code> oidc.search.client_secret string <code>\"\"</code> podAnnotations object <code>{}</code> amundsen application wide configuration of podAnnotations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref provider string <code>\"aws\"</code> The cloud provider the app is running in. Used to construct dns hostnames (on aws only). search.affinity object <code>{}</code> Search pod specific affinity. search.annotations object <code>{}</code> Search service specific tolerations. search.envVars object <code>{}</code> search.image string <code>\"amundsendev/amundsen-search\"</code> The image of the search container. search.imagePullSecrets list <code>[]</code> Optional pod imagePullSecrets ref search.imageTag string <code>\"latest\"</code> The image tag of the search container. search.nodeSelector object <code>{}</code> Search pod specific nodeSelector. search.podAnnotations object <code>{}</code> Search pod specific annotations. search.proxy.endpoint string <code>nil</code> Endpoint of the search proxy (i.e., ES endpoint etc.) You should only need to change this, if you don\u2019t use the version in this chart. elasticsearch-master.user-pengfei.svc.cluster.local search.proxy.password string <code>nil</code> search.proxy.user string <code>nil</code> search.replicas int <code>1</code> How many replicas of the search service to run. search.resources object <code>{}</code> See pod resourcing ref search.serviceName string <code>\"search\"</code> The search service name. search.serviceType string <code>\"ClusterIP\"</code> The search service type. See service types ref search.tolerations list <code>[]</code> Search pod specific tolerations. tolerations list <code>[]</code> amundsen application wide configuration of tolerations. This applies to search, metadata, frontend and neo4j. Elasticsearch has it\u2019s own configuation properties for this. ref <p>Autogenerated from chart metadata using helm-docs v1.5.0</p>"},{"location":"k8s_install/#ingress-support","title":"Ingress support","text":"<p>If you want to deploy Amundsen on a K8s cluster on premise. You can activate the ingress module. Do not active ingress if you are using a public cloud such as AWS, GCP, etc.</p> <pre><code>ingress:\n  enabled: true\n  annotations: {}\n  hosts:\n    - host: amundsen-test.your-domain.com\n      paths: [/]\n  tls: \n    - hosts:\n        - amundsen-test.your-domain.com\n</code></pre>"},{"location":"k8s_install/#neo4j-dbms-config","title":"Neo4j DBMS Config?","text":"<p>You may want to override the default memory usage for Neo4J. In particular, if you\u2019re just test-driving a deployment and your node exits with status 137, you should set the usage to smaller values:</p> <pre><code>config:\n  dbms:\n    heap_initial_size: 1G\n    heap_max_size: 2G\n    pagecache_size: 2G\n</code></pre> <p>With this values file, you can then install Amundsen using Helm 2 with:</p> <pre><code>helm install ./templates/helm --values impl/helm/dev/values.yaml\n</code></pre> <p>For Helm 3 it\u2019s now mandatory to specify a chart reference name e.g. <code>my-amundsen</code>:</p> <pre><code>helm install my-amundsen ./templates/helm --values impl/helm/dev/values.yaml\n</code></pre>"},{"location":"k8s_install/#other-notes","title":"Other Notes","text":"<ul> <li>For aws setup, you will also need to setup the external-dns plugin</li> <li>There is an existing helm chart for neo4j, but, it is missing some features necessary to for use such as:</li> <li>[stable/neo4j] make neo4j service definition more extensible; without this, it is not possible to setup external load balancers, external-dns, etc</li> <li>[stable/neo4j] allow custom configuration of neo4j; without this, custom configuration is not possible which includes setting configmap based settings, which also includes turning on apoc.</li> </ul>"},{"location":"roadmap/","title":"Amundsen Roadmap","text":"<p>The following roadmap gives an overview of what we are currently working on and what we want to tackle next. This helps potential contributors understand the current status of your project and where it\u2019s going next, as well as giving a chance to be part of the planning.</p>"},{"location":"roadmap/#amundsen-mission","title":"Amundsen Mission","text":"<p>To organize all information about data and make it universally actionable</p>"},{"location":"roadmap/#vision-for-2021","title":"Vision for 2021","text":"<p>Centralize a comprehensive and actionable map of all our data resources that can be leveraged to solve a growing number of use cases and workflows</p>"},{"location":"roadmap/#short-term-our-current-focus","title":"Short Term - Our Current focus","text":""},{"location":"roadmap/#native-lineage-integration","title":"Native lineage integration","text":"<p>What: We want to create a native lineage integration in Amundsen, to better surface how data assets interact with each other.</p> <p>Status: designs complete</p>"},{"location":"roadmap/#integrate-with-data-quality-system","title":"Integrate with Data Quality system","text":"<p>What: Integrate with different data quality systems to provide quality score.</p> <p>Status: in progress</p>"},{"location":"roadmap/#improve-search-ranking","title":"Improve search ranking","text":"<p>What: Overhaul search ranking to improve results.</p> <p>Status: planning</p>"},{"location":"roadmap/#show-distinct-column-values","title":"Show distinct column values","text":"<p>What: When a column has a limited set of possible values, we want to make then easily discoverable.</p> <p>Status: implementation started</p>"},{"location":"roadmap/#neptune-databuilder-support","title":"Neptune Databuilder support","text":"<p>What: Supports Databuilder ingestion for Neptune (<code>FsNeo4jCSVLoader</code>, <code>FsNeputuneCSVLoader</code> and various Neptune components). Detail could be found in RFC-13.</p> <p>Status: implementation started</p>"},{"location":"roadmap/#rds-proxy-support","title":"RDS Proxy Support","text":"<p>What: Support RDS as another type of proxy for both metadata and databuilder. Detail could be found in RFC-10</p> <p>Status: Planning</p>"},{"location":"roadmap/#mid-term-our-next-steps","title":"Mid Term - Our Next steps","text":""},{"location":"roadmap/#curated-navigation-experience","title":"Curated navigation experience","text":"<p>What: Currently Amundsen\u2019s experience is very focussed on search. However, especially for new users, an experience where they are able to navigate through the data hierarchy is very important. This item proposes to revamp the navigational experience in Amundsen (currently, barebones - based on tags) to do justice to the user need to browse through data sets when they don\u2019t know what to even search for.</p> <p>Status: planning</p>"},{"location":"roadmap/#notifications-when-a-table-evolves","title":"Notifications when a table evolves","text":"<p>What: Notify users in Amundsen (akin to Facebook notifications or similar) when a table evolves. Owners of data and consumers of data will likely need to be notified of different things.</p> <p>Status: planning has not started</p>"},{"location":"roadmap/#commonly-joined-tables-browsing-the-data-model","title":"Commonly joined tables / browsing the data model","text":"<p>What: As a data user, I would like to see commonly joined tables and how to join them. One option would be to show commonly joined tables and showing example join queries. Another option would be to provide a navigational experience for data model, showing foreign keys and which tables they come from.</p> <p>Status: planning has not started</p>"},{"location":"roadmap/#push-ingest-api","title":"Push ingest API","text":"<p>What: Possible through Kafka extractor, though Kafka topic schema is not well defined. And it requires client side SDK to support message pushing.</p> <p>Status: 50%</p>"},{"location":"roadmap/#granular-access-control","title":"Granular Access Control","text":"<p>What: we want to have a more granular control of the access. For example, only certain types of people would be able to see certain types of metadata/functionality</p> <p>Status: not planned</p>"},{"location":"roadmap/#versioning-system","title":"Versioning system","text":"<p>What: We want to create a versioning system for our indexed resources, to be able to index different versions of the same resource. This is especially required for machine learning purposes.</p> <p>Status: not planned</p>"},{"location":"roadmap/#index-processes","title":"Index Processes","text":"<p>What: we want to index ETLs and pipelines from our Machine Learning Engine</p> <p>Status: not planned</p>"},{"location":"roadmap/#index-teams","title":"Index Teams","text":"<p>What: We want to add teams pages to enable users to see what are the important tables and dashboard a team uses</p> <p>Status: not planned</p>"},{"location":"roadmap/#index-services","title":"Index Services","text":"<p>What: With our microservices architecture, we want to index services and show how these services interact with data artifacts</p> <p>Status: not planned</p>"},{"location":"roadmap/#index-s3-buckets","title":"Index S3 buckets","text":"<p>What: add these new resource types to our data map and create resource pages for them</p> <p>Status: not planned</p>"},{"location":"roadmap/#index-pubsub-systems","title":"Index Pub/Sub systems","text":"<p>What: We want to make our pub/sub systems discoverable</p> <p>Status: not planned</p>"},{"location":"roadmap/#how-to-get-involved","title":"How to Get Involved","text":"<p>Let us know in the Slack channel if you are interested in taking a stab at leading the development of one of these features.</p> <p>You can also jump right in by tackling one of our issues labeled as \u2018help wanted\u2019 or, if you are new to Amundsen, try one of our \u2018good first issue\u2019 tickets.</p>"},{"location":"windows_troubleshooting/","title":"Windows Troubleshooting","text":"<ol> <li>Neo4j UI doesn\u2019t start</li> </ol> <p>If Neo4j UI doesn\u2019t come up, check <code>neo4j</code> container logs. If you see output like below, then change the <code>End of Line Sequence</code> setting for <code>.\\example\\docker\\neo4j\\conf\\neo4j.conf</code> file, using <code>VS Code</code>.</p> <p><pre><code>neo4j_amundsen      | Changed password for user 'neo4j'.\nneo4j_amundsen      | Active database: amundsen.db\nneo4j_amundsen      | Directories in use:\nneo4j_amundsen      |   home:         /var/lib/neo4j\nneo4j_amundsen      |   config:       /var/lib/neo4j/conf\nneo4j_amundsen      |   logs:         /var/log/neo4j\nneo4j_amundsen      |   plugins:      /plugins\nneo4j_amundsen      |   import:       /var/lib/neo4j/import\nneo4j_amundsen      |   data:         /neo4j/data\nneo4j_amundsen      |   certificates: /var/lib/neo4j/certificates\nneo4j_amundsen      |   run:          /var/lib/neo4j/run\nneo4j_amundsen      | Starting Neo4j.\n'eo4j_amundsen      | Unrecognized VM option 'UseG1GC\nneo4j_amundsen      | Did you mean '(+/-)UseG1GC'?\nneo4j_amundsen      | Error: Could not create the Java Virtual Machine.\nneo4j_amundsen      | Error: A fatal exception has occurred. Program will exit.\n</code></pre> VS Code Note: Use ctrlshiftp to open <code>Command Palette</code>. Choose <code>LF</code>. Press <code>Enter</code>.</p> <ol> <li>Installing databuilder fails due to <code>extras_require</code> error:</li> </ol> <pre><code>error in amundsen-databuilder setup command: 'extras_require' must be a dictionary whose values are\nstrings or lists of strings containing valid project/version requirement specifiers.\n</code></pre> <p>This error results from <code>databuilder/requirements_dev.txt</code> containing a symlink to <code>requirements_dev.txt</code> which is not properly    interpreted by Windows, resulting in the path being included as a requirement (which fails to parse as a requirement causing    the error). To work around this issue on windows, copy the root <code>requirements_dev.txt</code> over the file in <code>databuilder/</code>. This breaks    the link between the two files, meaning more work for windows developers to keep them in sync, but the <code>setup.py</code> script    will now function properly.</p> <p></p>"},{"location":"authentication/oidc/","title":"OIDC Authentication","text":"<p>Setting up end-to-end authentication using OIDC is fairly simple and can be done using a Flask wrapper i.e., flaskoidc.</p> <p><code>flaskoidc</code> leverages the Flask\u2019s <code>before_request</code> functionality to authenticate each request before passing that to the views. It also accepts headers on each request if available in order to validate bearer token from incoming requests.</p>"},{"location":"authentication/oidc/#installation","title":"Installation","text":"<p>(If you are using flaskoidc&lt;1.0.0, please follow the documentation here</p> <p>PREREQUISITE: Please refer to the flaskoidc Documentation for the installation and the configurations.</p> <p>Note: You need to install and configure <code>flaskoidc</code> for each microservice of Amundsen i.e., for frontendlibrary, metadatalibrary and searchlibrary in order to secure each of them.</p>"},{"location":"authentication/oidc/#amundsen-configuration","title":"Amundsen Configuration","text":"<p>Once you have <code>flaskoidc</code> installed and configured for each microservice, please set the following environment variables:</p> <ul> <li> <p>amundsenfrontendlibrary (<code>amundsen/frontend</code>): <pre><code>    FLASK_APP_MODULE_NAME: flaskoidc\n    FLASK_APP_CLASS_NAME: FlaskOIDC\n</code></pre></p> </li> <li> <p>amundsenmetadatalibrary (<code>amundsen/metadata</code>): <pre><code>    FLASK_APP_MODULE_NAME: flaskoidc\n    FLASK_APP_CLASS_NAME: FlaskOIDC\n</code></pre></p> </li> <li> <p>amundsensearchlibrary (<code>amundsen/search</code>):  <pre><code>    FLASK_APP_MODULE_NAME: flaskoidc\n    FLASK_APP_CLASS_NAME: FlaskOIDC\n</code></pre></p> </li> </ul> <p>By default <code>flaskoidc</code> whitelist the healthcheck URLs, to not authenticate them. In case of metadatalibrary and searchlibrary we may want to whitelist the healthcheck APIs explicitly using following environment variable.</p> <pre><code>    FLASK_OIDC_WHITELISTED_ENDPOINTS: 'api.healthcheck'\n</code></pre>"},{"location":"authentication/oidc/#setting-up-request-headers","title":"Setting Up Request Headers","text":"<p>To communicate securely between the microservices, you need to pass the bearer token from frontend in each request to metadatalibrary and searchlibrary. This should be done using <code>REQUEST_HEADERS_METHOD</code> config variable in frontendlibrary.</p> <ul> <li>Define a function to add the bearer token in each request in your config.py:</li> </ul> <p>version: flaskoidc&lt;1.0.0 <pre><code>def get_access_headers(app):\n    try:\n        access_token = app.oidc.get_access_token()\n        return {'Authorization': 'Bearer {}'.format(access_token)}\n    except Exception:\n        return None\n</code></pre> version: flaskoidc&gt;=1.0.0 <pre><code>from flask import Flask\n\ndef get_access_headers(app: Flask) -&gt; Optional[Dict]:\n    try:\n        # noinspection PyUnresolvedReferences\n        access_token = json.dumps(app.auth_client.token)\n        return {'Authorization': 'Bearer {}'.format(access_token)}\n    except Exception:\n        pass\n</code></pre></p> <ul> <li>Set the method as the request header method in your config.py: <pre><code>REQUEST_HEADERS_METHOD = get_access_headers\n</code></pre></li> </ul> <p>This function will be called using the current <code>app</code> instance to add the headers in each request when calling any endpoint of metadatalibrary and searchlibrary here</p>"},{"location":"authentication/oidc/#setting-up-auth-user-method","title":"Setting Up Auth User Method","text":"<p>In order to get the current authenticated user (which is being used in Amundsen for many operations), we need to set <code>AUTH_USER_METHOD</code> config variable in frontendlibrary. This function should return email address, user id and any other required information.</p> <ul> <li>Define a function to fetch the user information in your config.py:</li> </ul> <p>version: flaskoidc&lt;1.0.0 <pre><code>from flask import Flask\nfrom amundsen_application.models.user import load_user, User\n\n\ndef get_auth_user(app: Flask) -&gt; User:\n    from flask import g\n    user_info = load_user(g.oidc_id_token)\n    return user_info\n</code></pre> version: flaskoidc&gt;=1.0.0 <pre><code>from flask import Flask, session\nfrom amundsen_application.models.user import load_user, User\n\n\ndef get_auth_user(app: Flask) -&gt; User:\n    user_info = load_user(session.get(\"user\"))\n    return user_info\n</code></pre></p> <ul> <li>Set the method as the auth user method in your config.py: <pre><code>AUTH_USER_METHOD = get_auth_user\n</code></pre></li> </ul> <p>Once done, you\u2019ll have the end-to-end authentication in Amundsen without any proxy or code changes.</p>"},{"location":"authentication/oidc/#using-okta-with-amundsen-on-k8s","title":"Using Okta with Amundsen on K8s","text":"<p>Valid for flaskoidc&lt;1.0.0</p> <p>Assumptions:</p> <ul> <li>You have access to okta (you can create a developer account for free!)</li> <li> <p>You are using k8s to setup amundsen. See amundsen-kube-helm</p> </li> <li> <p>You need to have a stable DNS entry for amundsen-frontend that can be registered in okta.</p> <ul> <li>for example in AWS you can setup route53 I will assume for the rest of this tutorial that your stable uri is \u201chttp://amundsen-frontend\u201c</li> </ul> </li> <li>You need to register amundsen in okta as an app. More info here. But here are specific instructions for amundsen:<ul> <li>At this time, I have only succesfully tested integration after ALL grants were checked.</li> <li>Set the Login redirect URIs to: <code>http://amundsen-frontend/oidc_callback</code></li> <li>No need to set a logout redirect URI</li> <li>Set the Initiate login URI to: <code>http://amundsen-frontend/</code>     (This is where okta will take you if users click on amundsen via okta landing page)</li> <li>Copy the Client ID and Client secret as you will need this later.</li> </ul> </li> <li>At present, there is no oidc build of the frontend. So you will need to build an oidc build yourself and upload it to, for example ECR, for use by k8s.    You can then specify which image you want to use as a property override for your helm install like so:</li> </ul> <pre><code>frontEndServiceImage: 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test\n</code></pre> <p>Please see further down in this doc for more instructions on how to build frontend. 4. When you start up helm you will need to provide some properties. Here are the properties that need to be overridden for oidc to work:</p> <pre><code>```yaml\noidcEnabled: true\ncreateOidcSecret: true\nOIDC_CLIENT_ID: YOUR_CLIENT_ID\nOIDC_CLIENT_SECRET: YOUR_SECRET_ID\nOIDC_ORG_URL: https://amundsen.okta.com\nOIDC_AUTH_SERVER_ID: default\n# You also will need a custom oidc frontend build too\nfrontEndServiceImage: 123.dkr.ecr.us-west-2.amazonaws.com/edmunds/amundsen-frontend:oidc-test\n```\n</code></pre>"},{"location":"authentication/oidc/#building-frontend-with-oidc","title":"Building frontend with OIDC","text":"<ol> <li>Please look at this guide for instructions on how to build a custom frontend docker image.</li> <li>The only difference to above is that in your docker file you will want to add the following at the end. This will make sure its ready to go for oidc. You can take alook at the public.Dockerfile as a reference.</li> </ol> <pre><code>RUN pip3 install .[oidc]\nENV FRONTEND_SVC_CONFIG_MODULE_CLASS=amundsen_application.oidc_config.OidcConfig\nENV FLASK_APP_MODULE_NAME=flaskoidc\nENV FLASK_APP_CLASS_NAME=FlaskOIDC\nENV FLASK_OIDC_WHITELISTED_ENDPOINTS=status,healthcheck,health\nENV SQLALCHEMY_DATABASE_URI=sqlite:///sessions.db\n</code></pre> <p>Please also take a look at this blog post for more detail.</p>"},{"location":"common/","title":"Amundsen Common","text":"<p>Amundsen Common library holds common codes among micro services in Amundsen. For information about Amundsen and our other services, visit the main repository. Please also see our instructions for a quick start setup  of Amundsen with dummy data, and an overview of the architecture.</p>"},{"location":"common/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> </ul>"},{"location":"common/#doc","title":"Doc","text":"<ul> <li>https://www.amundsen.io/amundsen/</li> </ul>"},{"location":"common/CHANGELOG/","title":"CHANGELOG","text":""},{"location":"common/CHANGELOG/#feature","title":"Feature","text":"<ul> <li>Added lineage item and lineage entities (#90) (<code>f1c6011</code>)</li> <li>Add chart into common ES index map (#77) (<code>4a7eea4</code>)</li> <li>Add chart to dashboard model (#73) (<code>241f627</code>)</li> <li>Added badges field (optional) to column in table model (#68) (<code>7bf5a84</code>)</li> <li>Add marshmallow packages to setup.py (#66) (<code>7ff2fe1</code>)</li> <li>Tweaks for gremlin support (#60) (<code>1a2733b</code>)</li> <li>Table model badges field update (#56) (<code>6a393d0</code>)</li> <li>Added new badge model (#55) (<code>09897d9</code>)</li> <li>Add github action for test and pypi publish (#47) (<code>1a466b1</code>)</li> <li>Added resource_reports into Table model (<code>60b1751</code>)</li> </ul>"},{"location":"common/CHANGELOG/#fix","title":"Fix","text":"<ul> <li>Standardize requirements and fixes for marshmallow3+ (#98) (<code>d185046</code>)</li> <li>Moved version declaration (#88) (<code>19be687</code>)</li> <li>Fix table index map bug (#86) (<code>f250d6a</code>)</li> <li>Make column names searchable by lowercase (#85) (<code>0ead455</code>)</li> <li>Changed marshmallow-annotation version, temp solution (#81) (<code>ff9d2e2</code>)</li> <li>Enable flake8 and mypy in CI (#75) (<code>32e317c</code>)</li> <li>Fix import (#74) (<code>2d1725b</code>)</li> <li>Add dashboard index map copied from amundsendatabuilder (#65) (<code>551834b</code>)</li> <li>Update elasticsearch mapping (#64) (<code>b43a687</code>)</li> </ul>"},{"location":"databuilder/","title":"Amundsen Databuilder","text":"<p>Amundsen Databuilder is a data ingestion library, which is inspired by Apache Gobblin. It could be used in an orchestration framework(e.g. Apache Airflow) to build data from Amundsen. You could use the library either with an adhoc python script(example) or inside an Apache Airflow DAG(example).</p> <p>For information about Amundsen and our other services, visit the main repository <code>README.md</code> . Please also see our instructions for a quick start setup  of Amundsen with dummy data, and an overview of the architecture.</p>"},{"location":"databuilder/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8.x</li> <li>elasticsearch 7.x</li> </ul>"},{"location":"databuilder/#doc","title":"Doc","text":"<ul> <li>https://www.amundsen.io/amundsen/</li> </ul>"},{"location":"databuilder/#concept","title":"Concept","text":"<p>ETL job consists of extraction of records from the source, transform records, if necessary, and load records into the sink. Amundsen Databuilder is a ETL framework for Amundsen and there are corresponding components for ETL called Extractor, Transformer, and Loader that deals with record level operation. A component called task controls all these three components. Job is the highest level component in Databuilder that controls task and publisher and is the one that client use to launch the ETL job.</p> <p>In Databuilder, each components are highly modularized and each components are using namespace based config, HOCON config, which makes it highly reusable and pluggable. (e.g: transformer can be reused within extractor, or extractor can be reused within extractor) (Note that concept on components are highly motivated by Apache Gobblin)</p> <p></p>"},{"location":"databuilder/#extractor","title":"Extractor","text":"<p>An extractor extracts records from the source. This does not necessarily mean that it only supports pull pattern in ETL. For example, extracting records from messaging bus makes it a push pattern in ETL.</p>"},{"location":"databuilder/#transformer","title":"Transformer","text":"<p>A transformer takes a record from either an extractor or from other transformers (via ChainedTransformer) to transform the record.</p>"},{"location":"databuilder/#loader","title":"Loader","text":"<p>A loader takes a record from a transformer or from an extractor directly and loads it to a sink, or a staging area. As the loading operates at a record-level, it\u2019s not capable of supporting atomicity.</p>"},{"location":"databuilder/#task","title":"Task","text":"<p>A task orchestrates an extractor, a transformer, and a loader to perform a record-level operation.</p>"},{"location":"databuilder/#record","title":"Record","text":"<p>A record is represented by one of models.</p>"},{"location":"databuilder/#publisher","title":"Publisher","text":"<p>A publisher is an optional component. Its common usage is to support atomicity in job level and/or to easily support bulk load into the sink.</p>"},{"location":"databuilder/#job","title":"Job","text":"<p>A job is the highest level component in Databuilder, and it orchestrates a task and, if any, a publisher.</p>"},{"location":"databuilder/#model","title":"Model","text":"<p>Models are abstractions representing the domain.</p>"},{"location":"databuilder/#list-of-extractors","title":"List of extractors","text":""},{"location":"databuilder/#dbapiextractor","title":"DBAPIExtractor","text":"<p>An extractor that uses Python Database API interface. DBAPI requires three information, connection object that conforms DBAPI spec, a SELECT SQL statement, and a model class that correspond to the output of each row of SQL statement.</p> <pre><code>job_config = ConfigFactory.from_dict({\n        'extractor.dbapi{}'.format(DBAPIExtractor.CONNECTION_CONFIG_KEY): db_api_conn,\n        'extractor.dbapi.{}'.format(DBAPIExtractor.SQL_CONFIG_KEY ): select_sql_stmt,\n        'extractor.dbapi.model_class': 'package.module_name.class_name'\n        })\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=DBAPIExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#genericextractor","title":"GenericExtractor","text":"<p>An extractor that takes list of dict from user through config.</p>"},{"location":"databuilder/#hivetablelastupdatedextractor","title":"HiveTableLastUpdatedExtractor","text":"<p>An extractor that extracts last updated time from Hive metastore and underlying file system. Although, hive metastore has a parameter called \u201clast_modified_time\u201d, but it cannot be used as it provides DDL timestamp not DML timestamp. For this reason, HiveTableLastUpdatedExtractor is utilizing underlying file of Hive to fetch latest updated date. However, it is not efficient to poke all files in Hive, and it only pokes underlying storage for non-partitioned table. For partitioned table, it will fetch partition created timestamp, and it\u2019s close enough for last updated timestamp.</p> <p>As getting metadata from files could be time consuming there\u2019re several features to increase performance. 1. Support of multithreading to parallelize metadata fetching. Although, cpython\u2019s multithreading is not true multithreading as it\u2019s bounded by single core, getting metadata of file is mostly IO bound operation. Note that number of threads should be less or equal to number of connections. 1. User can pass where clause to only include certain schema and also remove certain tables. For example, by adding something like <code>TBL_NAME NOT REGEXP '(tmp|temp)</code> would eliminate unncecessary computation.</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.hive_table_last_updated.partitioned_table_where_clause_suffix': partitioned_table_where_clause,\n    'extractor.hive_table_last_updated.non_partitioned_table_where_clause_suffix'): non_partitioned_table_where_clause,\n    'extractor.hive_table_last_updated.extractor.sqlalchemy.{}'.format(\n            SQLAlchemyExtractor.CONN_STRING): connection_string,\n    'extractor.hive_table_last_updated.extractor.fs_worker_pool_size': pool_size,\n    'extractor.hive_table_last_updated.filesystem.{}'.format(FileSystem.DASK_FILE_SYSTEM): s3fs.S3FileSystem(\n        anon=False,\n        config_kwargs={'max_pool_connections': pool_size})})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=HiveTableLastUpdatedExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#hivetablemetadataextractor","title":"HiveTableMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Hive metastore database. <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.hive_table_metadata.{}'.format(HiveTableMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.hive_table_metadata.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=HiveTableMetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#cassandraextractor","title":"CassandraExtractor","text":"<p>An extractor that extracts table and column metadata including keyspace, table name, column name and column type from Apache Cassandra databases</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.cassandra.{}'.format(CassandraExtractor.CLUSTER_KEY): cluster_identifier_string,\n    'extractor.cassandra.{}'.format(CassandraExtractor.IPS_KEY): [127.0.0.1],\n    'extractor.cassandra.{}'.format(CassandraExtractor.KWARGS_KEY): {},\n    'extractor.cassandra.{}'.format(CassandraExtractor.FILTER_FUNCTION_KEY): my_filter_function,\n\n})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=CassandraExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre> <p>If using the function filter options here is the function description <pre><code>def filter(keytab, table):\n  # return False if you don't want to add that table and True if you want to add\n  return True\n</code></pre></p> <p>If needed to define more args on the cassandra cluster you can pass through kwargs args <pre><code>config = ConfigFactory.from_dict({\n    'extractor.cassandra.{}'.format(CassandraExtractor.IPS_KEY): [127.0.0.1],\n    'extractor.cassandra.{}'.format(CassandraExtractor.KWARGS_KEY): {'port': 9042}\n})\n# it will call the cluster constructor like this\nCluster([127.0.0.1], **kwargs)\n</code></pre></p>"},{"location":"databuilder/#glueextractor","title":"GlueExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from AWS Glue metastore.</p> <p>Before running make sure you have a working AWS profile configured and have access to search tables on Glue <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.glue.{}'.format(GlueExtractor.CLUSTER_KEY): cluster_identifier_string,\n    'extractor.glue.{}'.format(GlueExtractor.FILTER_KEY): [],\n    'extractor.glue.{}'.format(GlueExtractor.PARTITION_BADGE_LABEL_KEY): label_string,\n})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=GlueExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p> <p>Optionally, you may add a partition badge label to the configuration. This will apply that label to all columns that are identified as partition keys in Glue.</p> <p>If using the filters option here is the input format. For more information on filters visit link <pre><code>[\n  {\n    \"Key\": \"string\",\n    \"Value\": \"string\",\n    \"Comparator\": \"EQUALS\"|\"GREATER_THAN\"|\"LESS_THAN\"|\"GREATER_THAN_EQUALS\"|\"LESS_THAN_EQUALS\"\n  }\n  ...\n]\n</code></pre></p> <p>Example filtering on database and table. Note that Comparator can only apply to time fields.</p> <pre><code>[\n  {\n    \"Key\": \"DatabaseName\",\n    \"Value\": \"my_database\"\n  },\n  {\n    \"Key\": \"Name\",\n    \"Value\": \"my_table\"\n  }\n]\n</code></pre>"},{"location":"databuilder/#delta-lake-metadataextractor","title":"Delta-Lake-MetadataExtractor","text":"<p>An extractor that runs on a spark cluster and obtains delta-lake metadata using spark sql commands. This custom solution is currently necessary because the hive metastore does not contain all metadata information for delta-lake tables. For simplicity, this extractor can also be used for all hive tables as well.</p> <p>Because it must run on a spark cluster, it is required that you have an operator (for example a databricks submit run operator) that calls the configuration code on a spark cluster. <pre><code>spark = SparkSession.builder.appName(\"Amundsen Delta Lake Metadata Extraction\").getOrCreate()\njob_config = create_delta_lake_job_config()\ndExtractor = DeltaLakeMetadataExtractor()\ndExtractor.set_spark(spark)\njob = DefaultJob(conf=job_config,\n                 task=DefaultTask(extractor=dExtractor, loader=FsNeo4jCSVLoader()),\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre> The delta lake extractor supports extraction of complex data types to be indexed and searchable.  <pre><code>struct&lt;a:int,b:string,c:array&lt;struct&lt;d:int,e:string&gt;&gt;,f:map&lt;int,&lt;struct&lt;g:int,h:string&gt;&gt;&gt;\n\nWill be extracted as:\na     int\nb     string\nc     array&lt;struct&lt;d:int,e:string&gt;&gt;\nc.d   int\nc.e   string\nf     map&lt;int,&lt;struct&lt;g:int,h:string&gt;&gt;\nf.g   int\nf.h   string\n</code></pre> This functionality is behind a configuration value. Simply set EXTRACT_NESTED_COLUMNS to True in the job config.</p> <p>You can check out the sample deltalake metadata script for a full example.</p>"},{"location":"databuilder/#dremiometadataextractor","title":"DremioMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from Dremio.</p> <p>Before running make sure that you have the Dremio ODBC driver installed. Default config values assume the default driver name for the MacBook install. <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.dremio.{}'.format(DremioMetadataExtractor.DREMIO_USER_KEY): DREMIO_USER,\n    'extractor.dremio.{}'.format(DremioMetadataExtractor.DREMIO_PASSWORD_KEY): DREMIO_PASSWORD,\n    'extractor.dremio.{}'.format(DremioMetadataExtractor.DREMIO_HOST_KEY): DREMIO_HOST})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=DremioMetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#druidmetadataextractor","title":"DruidMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Druid DB.</p> <p>The <code>where_clause_suffix</code> could be defined, normally you would like to filter out the in <code>INFORMATION_SCHEMA</code>.</p> <p>You could specify the following job config <pre><code>conn_string = \"druid+https://{host}:{port}/druid/v2/sql/\".format(\n        host=druid_broker_host,\n        port=443\n)\njob_config = ConfigFactory.from_dict({\n    'extractor.druid_metadata.{}'.format(PostgresMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n  'extractor.druid_metadata.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): conn_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=DruidMetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#oraclemetadataextractor","title":"OracleMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from the Oracle database.</p> <p>By default, the Oracle database name is \u2018oracle\u2019. To override this, set <code>CLUSTER_KEY</code> to what you wish to use as the cluster name.</p> <p>The <code>where_clause_suffix</code> below should define which schemas you\u2019d like to query. The SQL query driving the extraction is defined here</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.oracle_metadata.{}'.format(OracleMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.oracle_metadata.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=OracleMetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#postgresmetadataextractor","title":"PostgresMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Postgres or Redshift database.</p> <p>By default, the Postgres/Redshift database name is used as the cluster name. To override this, set <code>USE_CATALOG_AS_CLUSTER_NAME</code> to <code>False</code>, and <code>CLUSTER_KEY</code> to what you wish to use as the cluster name.</p> <p>The <code>where_clause_suffix</code> below should define which schemas you\u2019d like to query (see the sample dag for an example).</p> <p>The SQL query driving the extraction is defined here</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.postgres_metadata.{}'.format(PostgresMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.postgres_metadata.{}'.format(PostgresMetadataExtractor.USE_CATALOG_AS_CLUSTER_NAME): True,\n    'extractor.postgres_metadata.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=PostgresMetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#mssqlmetadataextractor","title":"MSSQLMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Microsoft SQL database.</p> <p>By default, the Microsoft SQL Server Database name is used as the cluster name. To override this, set <code>USE_CATALOG_AS_CLUSTER_NAME</code> to <code>False</code>, and <code>CLUSTER_KEY</code> to what you wish to use as the cluster name.</p> <p>The <code>where_clause_suffix</code> below should define which schemas you\u2019d like to query (<code>\"('dbo','sys')\"</code>).</p> <p>The SQL query driving the extraction is defined here</p> <p>This extractor is highly derived from PostgresMetadataExtractor. <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.mssql_metadata.{}'.format(MSSQLMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.mssql_metadata.{}'.format(MSSQLMetadataExtractor.USE_CATALOG_AS_CLUSTER_NAME): True,\n    'extractor.mssql_metadata.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=MSSQLMetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#mysqlmetadataextractor","title":"MysqlMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a MYSQL database.</p> <p>By default, the MYSQL database name is used as the cluster name. To override this, set <code>USE_CATALOG_AS_CLUSTER_NAME</code> to <code>False</code>, and <code>CLUSTER_KEY</code> to what you wish to use as the cluster name.</p> <p>The <code>where_clause_suffix</code> below should define which schemas you\u2019d like to query.</p> <p>The SQL query driving the extraction is defined here</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.mysql_metadata.{}'.format(MysqlMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.mysql_metadata.{}'.format(MysqlMetadataExtractor.USE_CATALOG_AS_CLUSTER_NAME): True,\n    'extractor.mysql_metadata.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(conf=job_config,\n                                 task=DefaultTask(extractor=MysqlMetadataExtractor(), loader=FsNeo4jCSVLoader()),\n                                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#db2metadataextractor","title":"Db2MetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Unix, Windows or Linux Db2 database or BigSQL.</p> <p>The <code>where_clause_suffix</code> below should define which schemas you\u2019d like to query or those that you would not (see the sample data loader for an example).</p> <p>The SQL query driving the extraction is defined here</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.db2_metadata.{}'.format(Db2MetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.db2_metadata.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=Db2MetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#snowflakemetadataextractor","title":"SnowflakeMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Snowflake database.</p> <p>By default, the Snowflake database name is used as the cluster name. To override this, set <code>USE_CATALOG_AS_CLUSTER_NAME</code> to <code>False</code>, and <code>CLUSTER_KEY</code> to what you wish to use as the cluster name.</p> <p>By default, the Snowflake database is set to <code>PROD</code>. To override this, set <code>DATABASE_KEY</code> to <code>WhateverNameOfYourDb</code>.</p> <p>By default, the Snowflake schema is set to <code>INFORMATION_SCHEMA</code>. To override this, set <code>SCHEMA_KEY</code> to <code>WhateverNameOfYourSchema</code>.</p> <p>Note that <code>ACCOUNT_USAGE</code> is a separate schema which allows users to query a wider set of data at the cost of latency. Differences are defined here</p> <p>The <code>where_clause_suffix</code> should define which schemas you\u2019d like to query (see the sample dag for an example).</p> <p>The SQL query driving the extraction is defined here</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.snowflake.{}'.format(SnowflakeMetadataExtractor.SNOWFLAKE_DATABASE_KEY): 'YourDbName',\n    'extractor.snowflake.{}'.format(SnowflakeMetadataExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.snowflake.{}'.format(SnowflakeMetadataExtractor.USE_CATALOG_AS_CLUSTER_NAME): True,\n    'extractor.snowflake.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=SnowflakeMetadataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#genericusageextractor","title":"GenericUsageExtractor","text":"<p>An extractor that extracts table popularity metadata from a custom created Snowflake table (created by a script that may look like this scala script). You can create a DAG using the Databricks Operator and run this script within Databricks or wherever you are able to run Scala.</p> <p>By default, <code>snowflake</code> is used as the database name. <code>ColumnReader</code> has the datasource as its <code>database</code> input, and database as its <code>cluster</code> input.</p> <p>The following inputs are related to where you create your Snowflake popularity table.</p> <p>By default, the Snowflake popularity database is set to <code>PROD</code>. To override this, set <code>POPULARITY_TABLE_DATABASE</code> to <code>WhateverNameOfYourDb</code>.</p> <p>By default, the Snowflake popularity schema is set to <code>SCHEMA</code>. To override this, set <code>POPULARTIY_TABLE_SCHEMA</code> to <code>WhateverNameOfYourSchema</code>.</p> <p>By default, the Snowflake popularity table is set to <code>TABLE</code>. To override this, set <code>POPULARITY_TABLE_NAME</code> to <code>WhateverNameOfYourTable</code>.</p> <p>The <code>where_clause_suffix</code> should define any filtering you\u2019d like to include in your query. For example, this may include <code>user_email</code>s that you don\u2019t want to include in your popularity definition.</p> <pre><code>job_config = ConfigFactory.from_dict({\n    f'extractor.generic_usage.extractor.sqlalchemy.{SQLAlchemyExtractor.CONN_STRING}': connection_string(),\n    f'extractor.generic_usage.{GenericUsageExtractor.WHERE_CLAUSE_SUFFIX_KEY}': where_clause_suffix,\n    f'extractor.generic_usage.{GenericUsageExtractor.POPULARITY_TABLE_DATABASE}': 'WhateverNameOfYourDb',\n    f'extractor.generic_usage.{GenericUsageExtractor.POPULARTIY_TABLE_SCHEMA}': 'WhateverNameOfYourSchema',\n    f'extractor.generic_usage.{GenericUsageExtractor.POPULARITY_TABLE_NAME}': 'WhateverNameOfYourTable',\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=GenericUsageExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#snowflaketablelastupdatedextractor","title":"SnowflakeTableLastUpdatedExtractor","text":"<p>An extractor that extracts table last updated timestamp from a Snowflake database.</p> <p>It uses same configs as the <code>SnowflakeMetadataExtractor</code> described above.</p> <p>The SQL query driving the extraction is defined here</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.snowflake_table_last_updated.{}'.format(SnowflakeTableLastUpdatedExtractor.SNOWFLAKE_DATABASE_KEY): 'YourDbName',\n    'extractor.snowflake_table_last_updated.{}'.format(SnowflakeTableLastUpdatedExtractor.WHERE_CLAUSE_SUFFIX_KEY): where_clause_suffix,\n    'extractor.snowflake_table_last_updated.{}'.format(SnowflakeTableLastUpdatedExtractor.USE_CATALOG_AS_CLUSTER_NAME): True,\n    'extractor.snowflake_table_last_updated.extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string()})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=SnowflakeTableLastUpdatedExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#bigquerymetadataextractor","title":"BigQueryMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, table description, column name and column description from a Bigquery database.</p> <p>The API calls driving the extraction is defined here</p> <p>You will need to create a service account for reading metadata and grant it \u201cBigQuery Metadata Viewer\u201d access to all of your datasets. This can all be done via the bigquery ui.</p> <p>Download the credentials file and store it securely. Set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment varible to the location of your credtials files and your code should have access to everything it needs.</p> <p>You can configure bigquery like this. You can optionally set a label filter if you only want to pull tables with a certain label. <pre><code>    job_config = {\n        'extractor.bigquery_table_metadata.{}'.format(\n            BigQueryMetadataExtractor.PROJECT_ID_KEY\n            ): gcloud_project\n    }\n    if label_filter:\n        job_config[\n            'extractor.bigquery_table_metadata.{}'\n            .format(BigQueryMetadataExtractor.FILTER_KEY)\n            ] = label_filter\n    task = DefaultTask(extractor=BigQueryMetadataExtractor(),\n                       loader=csv_loader,\n                       transformer=NoopTransformer())\n    job = DefaultJob(conf=ConfigFactory.from_dict(job_config),\n                     task=task,\n                     publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#neo4jeslastupdatedextractor","title":"Neo4jEsLastUpdatedExtractor","text":"<p>An extractor that basically get current timestamp and passes it GenericExtractor. This extractor is basically being used to create timestamp for \u201cAmundsen was last indexed on \u2026\u201d in Amundsen web page\u2019s footer.</p>"},{"location":"databuilder/#neo4jextractor","title":"Neo4jExtractor","text":"<p>An extractor that extracts records from Neo4j based on provided Cypher query. One example is to extract data from Neo4j so that it can transform and publish to Elasticsearch. <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.neo4j.{}'.format(Neo4jExtractor.CYPHER_QUERY_CONFIG_KEY): cypher_query,\n    'extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY): neo4j_endpoint,\n    'extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY): 'package.module.class_name',\n    'extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER): neo4j_user,\n    'extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW): neo4j_password},\n    'extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_ENCRYPTED): True})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=Neo4jExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#neo4jsearchdataextractor","title":"Neo4jSearchDataExtractor","text":"<p>An extractor that is extracting Neo4j utilizing Neo4jExtractor where CYPHER query is already embedded in it. <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY): neo4j_endpoint,\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY): 'databuilder.models.neo4j_data.Neo4jDataResult',\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER): neo4j_user,\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW): neo4j_password},\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_ENCRYPTED): False})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=Neo4jSearchDataExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#atlassearchdataextractor","title":"AtlasSearchDataExtractor","text":"<p>An extractor that is extracting Atlas Data to index compatible with Elasticsearch Search Proxy. <pre><code>entity_type = 'Table'\nextracted_search_data_path = f'/tmp/{entity_type.lower()}_search_data.json'\nprocess_pool_size = 5\n\n# atlas config\natlas_url = 'localhost'\natlas_port = 21000\natlas_protocol = 'http'\natlas_verify_ssl = False\natlas_username = 'admin'\natlas_password = 'admin'\natlas_search_chunk_size = 200\natlas_details_chunk_size = 10\n\n# elastic config\nes = Elasticsearch([\n    {'host': 'localhost'},\n])\n\nelasticsearch_client = es\nelasticsearch_new_index_key = f'{entity_type.lower()}-' + str(uuid.uuid4())\nelasticsearch_new_index_key_type = '_doc'\nelasticsearch_index_alias = f'{entity_type.lower()}_search_index'\n\njob_config = ConfigFactory.from_dict({\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_URL_CONFIG_KEY):\n        atlas_url,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_PORT_CONFIG_KEY):\n        atlas_port,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_PROTOCOL_CONFIG_KEY):\n        atlas_protocol,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_VALIDATE_SSL_CONFIG_KEY):\n        atlas_verify_ssl,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_USERNAME_CONFIG_KEY):\n        atlas_username,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_PASSWORD_CONFIG_KEY):\n        atlas_password,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_SEARCH_CHUNK_SIZE_KEY):\n        atlas_search_chunk_size,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ATLAS_DETAILS_CHUNK_SIZE_KEY):\n        atlas_details_chunk_size,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.PROCESS_POOL_SIZE_KEY):\n        process_pool_size,\n    'extractor.atlas_search_data.{}'.format(AtlasSearchDataExtractor.ENTITY_TYPE_KEY):\n        entity_type,\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY):\n        extracted_search_data_path,\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY):\n        'w',\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY):\n        extracted_search_data_path,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY):\n        'r',\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY):\n        elasticsearch_client,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY):\n        elasticsearch_new_index_key,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY):\n        elasticsearch_new_index_key_type,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY):\n        elasticsearch_index_alias\n})\n\nif __name__ == \"__main__\":\n    task = DefaultTask(extractor=AtlasSearchDataExtractor(),\n                       transformer=NoopTransformer(),\n                       loader=FSElasticsearchJSONLoader())\n\n    job = DefaultJob(conf=job_config,\n                     task=task,\n                     publisher=ElasticsearchPublisher())\n\n    job.launch()\n</code></pre></p>"},{"location":"databuilder/#verticametadataextractor","title":"VerticaMetadataExtractor","text":"<p>An extractor that extracts table and column metadata including database, schema, table name, column name and column datatype from a Vertica database.</p> <p>A sample loading script for Vertica is provided here</p> <p>By default, the Vertica database name is used as the cluster name. The <code>where_clause_suffix</code> in the example can be used to define which schemas you would like to query.</p>"},{"location":"databuilder/#sqlalchemyextractor","title":"SQLAlchemyExtractor","text":"<p>An extractor utilizes SQLAlchemy to extract record from any database that support SQL Alchemy. <pre><code>job_config = ConfigFactory.from_dict({\n    'extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.CONN_STRING): connection_string(),\n    'extractor.sqlalchemy.{}'.format(SQLAlchemyExtractor.EXTRACT_SQL): sql,\n    'extractor.sqlalchemy.model_class': 'package.module.class_name'})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=SQLAlchemyExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#dbtextractor","title":"DbtExtractor","text":"<p>This extractor utilizes the dbt output files <code>catalog.json</code> and <code>manifest.json</code> to extract metadata and ingest it into Amundsen. The <code>catalog.json</code> and <code>manifest.json</code> can both be generated by running <code>dbt docs generate</code> in your dbt project. Visit the dbt artifacts page for more information.</p> <p>The <code>DbtExtractor</code> can currently create the following:</p> <ul> <li>Tables and their definitions</li> <li>Columns and their definitions</li> <li>Table level lineage</li> <li>dbt tags (as Amundsen badges or tags)</li> <li>Table Sources (e.g. link to GitHib where the dbt template resides)</li> </ul> <pre><code>job_config = ConfigFactory.from_dict({\n    # Required args\n    f'extractor.dbt.{DbtExtractor.DATABASE_NAME}': 'snowflake',\n    f'extractor.dbt.{DbtExtractor.MANIFEST_JSON}': catalog_file_loc,  # File location\n    f'extractor.dbt.{DbtExtractor.DATABASE_NAME}': json.dumps(manifest_data),  # JSON Dumped object\n    # Optional args\n    f'extractor.dbt.{DbtExtractor.SOURCE_URL}': 'https://github.com/your-company/your-repo/tree/main',\n    f'extractor.dbt.{DbtExtractor.EXTRACT_TABLES}': True,\n    f'extractor.dbt.{DbtExtractor.EXTRACT_DESCRIPTIONS}': True,\n    f'extractor.dbt.{DbtExtractor.EXTRACT_TAGS}': True,\n    f'extractor.dbt.{DbtExtractor.IMPORT_TAGS_AS}': 'badges',\n    f'extractor.dbt.{DbtExtractor.EXTRACT_LINEAGE}': True,\n})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=DbtExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#restapiextractor","title":"RestAPIExtractor","text":"<p>A extractor that utilizes RestAPIQuery to extract data. RestAPIQuery needs to be constructed (example) and needs to be injected to RestAPIExtractor.</p>"},{"location":"databuilder/#mode-dashboard-extractor","title":"Mode Dashboard Extractor","text":"<p>Here are extractors that extracts metadata information from Mode via Mode\u2019s REST API.</p> <p>Prerequisite:</p> <ol> <li>You will need to create API access token that has admin privilege.</li> <li>You will need organization code. This is something you can easily get by looking at one of Mode report\u2019s URL.      <code>https://app.mode.com/&lt;organization code&gt;/reports/report_token</code></li> </ol>"},{"location":"databuilder/#modedashboardextractor","title":"ModeDashboardExtractor","text":"<p>A Extractor that extracts core metadata on Mode dashboard. https://app.mode.com/</p> <p>It extracts list of reports that consists of: Dashboard group name (Space name) Dashboard group id (Space token) Dashboard group description (Space description) Dashboard name (Report name) Dashboard id (Report token) Dashboard description (Report description)</p> <p>Other information such as report run, owner, chart name, query name is in separate extractor.</p> <p>It calls two APIs (spaces API and reports API) joining together.</p> <p>You can create Databuilder job config like this. <pre><code>task = DefaultTask(extractor=ModeDashboardExtractor(),\n                   loader=FsNeo4jCSVLoader(), )\n\ntmp_folder = '/var/tmp/amundsen/mode_dashboard_metadata'\nnode_files_folder = '{tmp_folder}/nodes'.format(tmp_folder=tmp_folder)\nrelationship_files_folder = '{tmp_folder}/relationships'.format(tmp_folder=tmp_folder)\n\njob_config = ConfigFactory.from_dict({\n    'extractor.mode_dashboard.{}'.format(ORGANIZATION): organization,\n    'extractor.mode_dashboard.{}'.format(MODE_BEARER_TOKEN): mode_bearer_token,\n    'extractor.mode_dashboard.{}'.format(DASHBOARD_GROUP_IDS_TO_SKIP): [space_token_1, space_token_2, ...],\n    'loader.filesystem_csv_neo4j.{}'.format(FsNeo4jCSVLoader.NODE_DIR_PATH): node_files_folder,\n    'loader.filesystem_csv_neo4j.{}'.format(FsNeo4jCSVLoader.RELATION_DIR_PATH): relationship_files_folder,\n    'loader.filesystem_csv_neo4j.{}'.format(FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR): True,\n    'task.progress_report_frequency': 100,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NODE_FILES_DIR): node_files_folder,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.RELATION_FILES_DIR): relationship_files_folder,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_END_POINT_KEY): neo4j_endpoint,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_USER): neo4j_user,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_PASSWORD): neo4j_password,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_ENCRYPTED): True,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_CREATE_ONLY_NODES): [DESCRIPTION_NODE_LABEL],\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.JOB_PUBLISH_TAG): job_publish_tag\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#modedashboardownerextractor","title":"ModeDashboardOwnerExtractor","text":"<p>An Extractor that extracts Dashboard owner. Mode itself does not have concept of owner and it will use creator as owner. Note that if user left the organization, it would skip the dashboard.</p> <p>You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = ModeDashboardOwnerExtractor()\ntask = DefaultTask(extractor=extractor,\n                   loader=FsNeo4jCSVLoader(), )\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_BEARER_TOKEN): mode_bearer_token,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#modedashboardlastsuccessfulexecutionextractor","title":"ModeDashboardLastSuccessfulExecutionExtractor","text":"<p>A Extractor that extracts Mode dashboard\u2019s last successful run (execution) timestamp.</p> <p>You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = ModeDashboardLastSuccessfulExecutionExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_BEARER_TOKEN): mode_bearer_token,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#modedashboardexecutionsextractor","title":"ModeDashboardExecutionsExtractor","text":"<p>A Extractor that extracts last run (execution) status and timestamp.</p> <p>You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = ModeDashboardExecutionsExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_BEARER_TOKEN): mode_bearer_token,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#modedashboardlastmodifiedtimestampextractor","title":"ModeDashboardLastModifiedTimestampExtractor","text":"<p>A Extractor that extracts Mode dashboard\u2019s last modified timestamp.</p> <p>You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = ModeDashboardLastModifiedTimestampExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_BEARER_TOKEN): mode_bearer_token,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#modedashboardqueriesextractor","title":"ModeDashboardQueriesExtractor","text":"<p>A Extractor that extracts Mode\u2019s query information.</p> <p>You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = ModeDashboardQueriesExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_BEARER_TOKEN): mode_bearer_token,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#modedashboardchartsbatchextractor","title":"ModeDashboardChartsBatchExtractor","text":"<p>A Extractor that extracts Mode Dashboard charts metadata.</p> <p>You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = ModeDashboardChartsBatchExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_BEARER_TOKEN): mode_bearer_token,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#modedashboarduserextractor","title":"ModeDashboardUserExtractor","text":"<p>A Extractor that extracts Mode user_id and then update User node.</p> <p>You can create Databuilder job config like this. (configuration related to loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = ModeDashboardUserExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_ACCESS_TOKEN): mode_token,\n    '{}.{}'.format(extractor.get_scope(), MODE_PASSWORD_TOKEN): mode_password,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#modedashboardusageextractor","title":"ModeDashboardUsageExtractor","text":"<p>A Extractor that extracts Mode dashboard\u2019s accumulated view count.</p> <p>Note that this provides accumulated view count which does not effectively show relevancy. Thus, fields from this extractor is not directly compatible with DashboardUsage model.</p> <p>If you are fine with <code>accumulated usage</code>, you could use TemplateVariableSubstitutionTransformer to transform Dict payload from ModeDashboardUsageExtractor to fit DashboardUsage and transform Dict to  DashboardUsage by TemplateVariableSubstitutionTransformer, and DictToModel transformers. (Example on how to combining these two transformers)</p>"},{"location":"databuilder/#openlineagetablelineageextractor","title":"OpenLineageTableLineageExtractor","text":"<p>A Extractor that extracts table lineage information from OpenLineage events.</p> <p>:warning: Extractor expects input data in the form of openLineage events in ndjson format</p> <p>Custom Openlineage json extraction keys may be set by passing those values: * OpenLineageTableLineageExtractor.OL_INPUTS_KEY - json key for inputs list * OpenLineageTableLineageExtractor.OL_OUTPUTS_KEY- json key for output list * OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_KEY - json key for namespace name (inputs/outputs scope) * OpenLineageTableLineageExtractor.OL_DATASET_DATABASE_KEY - json key for database name (inputs/outputs scope) * OpenLineageTableLineageExtractor.OL_DATASET_NAME_KEY - json key for dataset name (inputs/outputs scope) <pre><code>tmp_folder = f'/tmp/amundsen/lineage'\n\ndict_config = {\n    f'loader.filesystem_csv_atlas.{FsAtlasCSVLoader.ENTITY_DIR_PATH}': f'{tmp_folder}/entities',\n    f'loader.filesystem_csv_atlas.{FsAtlasCSVLoader.RELATIONSHIP_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'loader.filesystem_csv_atlas.{FsAtlasCSVLoader.SHOULD_DELETE_CREATED_DIR}': False,\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.ATLAS_CLIENT}': AtlasClient('http://localhost:21000', ('admin', 'admin')),\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.ENTITY_DIR_PATH}': f'{tmp_folder}/entities',\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.RELATIONSHIP_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.ATLAS_ENTITY_CREATE_BATCH_SIZE}': 10,\n    f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.CLUSTER_NAME}': 'datalab',\n    f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.OL_DATASET_NAMESPACE_OVERRIDE}': 'hive_table',\n    f'extractor.openlineage_tablelineage.{OpenLineageTableLineageExtractor.TABLE_LINEAGE_FILE_LOCATION}': 'input_dir/openlineage_nd.json',\n}\n\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=OpenLineageTableLineageExtractor(), loader=FsAtlasCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=AtlasCSVPublisher())\n\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#redashdashboardextractor","title":"RedashDashboardExtractor","text":"<p>The included <code>RedashDashboardExtractor</code> provides support for extracting basic metadata for Redash dashboards (dashboard name, owner, URL, created/updated timestamps, and a generated description) and their associated queries (query name, URL, and raw query). It can be extended with a configurable table parser function to also support extraction of <code>DashboardTable</code> metadata. (See below for example usage.)</p> <p>Note: <code>DashboardUsage</code> and <code>DashboardExecution</code> metadata are not supported in this extractor, as these concepts are not supported by the Redash API.</p> <p>The <code>RedashDashboardExtractor</code> depends on the following Redash API endpoints: <code>GET /api/dashboards</code>, <code>GET /api/dashboards/&lt;dashboard-slug&gt;</code>. It has been tested against Redash 8 and is also expected to work with Redash 9.</p> <pre><code>extractor = RedashDashboardExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    'extractor.redash_dashboard.redash_base_url': redash_base_url, # ex: https://redash.example.org\n    'extractor.redash_dashboard.api_base_url': api_base_url, # ex: https://redash.example.org/api\n    'extractor.redash_dashboard.api_key': api_key, # ex: abc1234\n    'extractor.redash_dashboard.table_parser': table_parser, # ex: my_library.module.parse_tables\n    'extractor.redash_dashboard.redash_version': redash_version # ex: 8. optional, default=9\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#redashdashboardextractor-table_parser","title":"RedashDashboardExtractor: table_parser","text":"<p>The <code>RedashDashboardExtractor</code> extracts raw queries from each dashboard. You may optionally use these queries to parse out relations to tables in Amundsen. A table parser can be provided in the configuration for the <code>RedashDashboardExtractor</code>, as seen above. This function should have type signature <code>(RedashVisualizationWidget) -&gt; Iterator[TableRelationData]</code>. For example:</p> <pre><code>def parse_tables(viz_widget: RedashVisualizationWidget) -&gt; Iterator[TableRelationData]:\n    # Each viz_widget corresponds to one query.\n    # viz_widget.data_source_id is the ID of the target DB in Redash.\n    # viz_widget.raw_query is the raw query (e.g., SQL).\n    if viz_widget.data_source_id == 123:\n        table_names = some_sql_parser(viz_widget.raw_query)\n        return [TableRelationData('some_db', 'prod', 'some_schema', tbl) for tbl in table_names]\n    return []\n</code></pre>"},{"location":"databuilder/#tableaudashboardextractor","title":"TableauDashboardExtractor","text":"<p>The included <code>TableauDashboardExtractor</code> provides support for extracting basic metadata for Tableau workbooks. All Tableau extractors including this one use the Tableau Metadata GraphQL API to gather the metadata. Tableau \u201cworkbooks\u201d are mapped to Amundsen dashboards, and the top-level project in which these workbooks preside is the dashboard group. The metadata it gathers is as follows: - Dashboard name (Workbook name) - Dashboard description (Workbook description) - Dashboard creation timestamp (Workbook creation timestamp) - Dashboard group name (Workbook top-level folder name) - Dashboard and dashboard group URL</p> <p>If you wish to exclude top-level projects from being loaded, specify their names in the <code>tableau_excluded_projects</code> list and workbooks from any of those projects will not be indexed.</p> <p>Tableau\u2019s concept of \u201cowners\u201d does not map cleanly into Amundsen\u2019s understanding of owners, as the owner of a Tableau workbook is simply whoever updated it last, even if they made a very small change. This can prove problematic in determining the true point of contact for a workbook, so it\u2019s simply omitted for now. Similarly, the hierachy of <code>dashboard/query/chart</code> in Amundsen does not map into Tableau, where <code>charts</code> have only an optional relation to queries and vice versa. For these reasons, there are not extractors for either entity.</p> <p>The Tableau Metadata API also does not support usage or execution statistics, so there are no extractors for these entities either.</p> <p>Sample job config: <pre><code>extractor = TableauDashboardExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    'extractor.tableau_dashboard_metadata.tableau_host': tableau_host,\n    'extractor.tableau_dashboard_metadata.api_version': tableau_api_version,\n    'extractor.tableau_dashboard_metadata.site_name': tableau_site_name,\n    'extractor.tableau_dashboard_metadata.tableau_personal_access_token_name': tableau_personal_access_token_name,\n    'extractor.tableau_dashboard_metadata.tableau_personal_access_token_secret': tableau_personal_access_token_secret,\n    'extractor.tableau_dashboard_metadata.excluded_projects': tableau_excluded_projects,\n    'extractor.tableau_dashboard_metadata.cluster': tableau_dashboard_cluster,\n    'extractor.tableau_dashboard_metadata.database': tableau_dashboard_database,\n    'extractor.tableau_dashboard_metadata.transformer.timestamp_str_to_epoch.timestamp_format': \"%Y-%m-%dT%H:%M:%SZ\",\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#tableaudashboardtableextractor","title":"TableauDashboardTableExtractor","text":"<p>The included <code>TableauDashboardTableExtractor</code> provides support for extracting table metadata from Tableau workbooks. The extractor assumes all the table entities have already been created; if you are interested in using the provided <code>TableauExternalTableExtractor</code>, make sure that job runs before this one, as it will create the tables required by this job. It also assumes that the dashboards are using their names as the primary ID.</p> <p>A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = TableauDashboardTableExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    'extractor.tableau_dashboard_table.tableau_host': tableau_host,\n    'extractor.tableau_dashboard_table.api_version': tableau_api_version,\n    'extractor.tableau_dashboard_table.site_name': tableau_site_name,\n    'extractor.tableau_dashboard_table.tableau_personal_access_token_name': tableau_personal_access_token_name,\n    'extractor.tableau_dashboard_table.tableau_personal_access_token_secret': tableau_personal_access_token_secret,\n    'extractor.tableau_dashboard_table.excluded_projects': tableau_excluded_projects,\n    'extractor.tableau_dashboard_table.cluster': tableau_dashboard_cluster,\n    'extractor.tableau_dashboard_table.database': tableau_dashboard_database,\n    'extractor.tableau_dashboard_table.external_cluster_name': tableau_external_table_cluster,\n    'extractor.tableau_dashboard_table.external_schema_name': tableau_external_table_schema,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#tableaudashboardqueryextractor","title":"TableauDashboardQueryExtractor","text":"<p>The included <code>TableauDashboardQueryExtractor</code> provides support for extracting query metadata from Tableau workbooks. It retrives the name and query text for each custom SQL query.</p> <p>A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = TableauDashboardQueryExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    'extractor.tableau_dashboard_query.tableau_host': tableau_host,\n    'extractor.tableau_dashboard_query.api_version': tableau_api_version,\n    'extractor.tableau_dashboard_query.site_name': tableau_site_name,\n    'extractor.tableau_dashboard_query.tableau_personal_access_token_name': tableau_personal_access_token_name,\n    'extractor.tableau_dashboard_query.tableau_personal_access_token_secret': tableau_personal_access_token_secret,\n    'extractor.tableau_dashboard_query.excluded_projects': tableau_excluded_projects,\n    'extractor.tableau_dashboard_query.cluster': tableau_dashboard_cluster,\n    'extractor.tableau_dashboard_query.database': tableau_dashboard_database,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#tableaudashboardlastmodifiedextractor","title":"TableauDashboardLastModifiedExtractor","text":"<p>The included <code>TableauDashboardLastModifiedExtractor</code> provides support for extracting the last updated timestamp for Tableau workbooks.</p> <p>A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = TableauDashboardQueryExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    'extractor.tableau_dashboard_last_modified.tableau_host': tableau_host,\n    'extractor.tableau_dashboard_last_modified.api_version': tableau_api_version,\n    'extractor.tableau_dashboard_last_modified.site_name': tableau_site_name,\n    'extractor.tableau_dashboard_last_modified.tableau_personal_access_token_name': tableau_personal_access_token_name,\n    'extractor.tableau_dashboard_last_modified.tableau_personal_access_token_secret': tableau_personal_access_token_secret,\n    'extractor.tableau_dashboard_last_modified.excluded_projects': tableau_excluded_projects,\n    'extractor.tableau_dashboard_last_modified.cluster': tableau_dashboard_cluster,\n    'extractor.tableau_dashboard_last_modified.database': tableau_dashboard_database,\n    'extractor.tableau_dashboard_last_modified.transformer.timestamp_str_to_epoch.timestamp_format': \"%Y-%m-%dT%H:%M:%SZ\",\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#tableauexternaltableextractor","title":"TableauExternalTableExtractor","text":"<p>The included <code>TableauExternalTableExtractor</code> provides support for extracting external table entities referenced by Tableau workbooks. In this context, \u201cexternal\u201d tables are \u201ctables\u201d that are not from a typical database, and are loaded using some other data format, like CSV files. This extractor has been tested with the following types of external tables; feel free to add others, but it\u2019s recommended to test them in a non-production instance first to be safe. - Excel spreadsheets - Text files (including CSV files) - Salesforce connections - Google Sheets connections</p> <p>Use the <code>external_table_types</code> list config option to specify which external connection types you would like to index; refer to your Tableau instance for the exact formatting of each connection type string.</p> <p>Excel spreadsheets, Salesforce connections, and Google Sheets connections are all classified as \u201cdatabases\u201d in terms of Tableau\u2019s Metadata API, with their \u201csubsheets\u201d forming their \u201ctables\u201d when present. However, these tables are not assigned a schema, this extractor chooses to use the name of the parent sheet as the schema, and assign a new table to each subsheet. The connection type is always used as the database, and for text files, the schema is set using the <code>external_schema_name</code> config option. Since these external tables are usually named for human consumption only and often contain a wider range of characters, all inputs are sanitized to remove any problematic occurences before they are inserted: see the <code>sanitize</code> methods <code>TableauDashboardUtils</code> for specifics.</p> <p>A more concrete example: if one had a Google Sheet titled \u201cGrowth by Region\u201d with 2 subsheets called \u201cFY19 Report\u201d and \u201cFY20 Report\u201d, two tables would be generated with the following keys: <code>googlesheets://external.growth_by_region/FY_19_Report</code> <code>googlesheets://external.growth_by_region/FY_20_Report</code></p> <p>A sample job config is shown below. Configuration related to the loader and publisher is omitted as it is mostly the same. Please take a look at this example for the configuration that holds loader and publisher.</p> <pre><code>extractor = TableauExternalTableExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    'extractor.tableau_external_table.tableau_host': tableau_host,\n    'extractor.tableau_external_table.api_version': tableau_api_version,\n    'extractor.tableau_external_table.site_name': tableau_site_name,\n    'extractor.tableau_external_table.tableau_personal_access_token_name': tableau_personal_access_token_name,\n    'extractor.tableau_external_table.tableau_personal_access_token_secret': tableau_personal_access_token_secret,\n    'extractor.tableau_external_table.excluded_projects': tableau_excluded_projects,\n    'extractor.tableau_external_table.cluster': tableau_dashboard_cluster,\n    'extractor.tableau_external_table.database': tableau_dashboard_database,\n    'extractor.tableau_external_table.external_cluster_name': tableau_external_table_cluster,\n    'extractor.tableau_external_table.external_schema_name': tableau_external_table_schema,\n    'extractor.tableau_external_table.external_table_types': tableau_external_table_types\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#databrickssqldashboardextractor","title":"DatabricksSQLDashboardExtractor","text":"<p>The <code>DatabricksSQLDashboardExtractor</code> extracts metadata about dashboards created in Databricks SQL</p> <p>The only configuration you need is a Databricks Host Name (i.e <code>https://my-company.cloud.databricks.com</code>) and a valid Databricks API Token. Make sure that the user that generated this token has permissions to read dashboards.</p> <p>Example: <pre><code>extractor = DatabricksSQLDashboardExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\njob_config = ConfigFactory.from_dict({\n    f\"extractor.databricks_sql_extractor.{DatabricksSQLDashboardExtractor.DATABRICKS_HOST_KEY}\": \"MY-DATABRICKS-API-TOKEN\",\n    f\"extractor.databricks_sql_extractor.{DatabricksSQLDashboardExtractor.DATABRICKS_API_TOKEN_KEY}\": \"https://my-company.cloud.databricks.com\",\n    # ...plus nessescary configs for neo4j...\n})\n\njob = DefaultJob(\n    conf=job_config,\n    task=task,\n    publisher=Neo4jCsvPublisher(),\n)\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#apachesupersetmetadataextractor","title":"ApacheSupersetMetadataExtractor","text":"<p>The included <code>ApacheSupersetMetadataExtractor</code> provides support for extracting basic metadata for Apache Superset dashboards.</p> <p>All Apache Superset extractors including this one use Apache Superset REST API (<code>/api/v1</code>) and were developed based on Apache Superset version <code>1.1</code>.</p>"},{"location":"databuilder/#caution","title":"Caution!","text":"<p>Apache Superset does not contain metadata fulfilling the concept of <code>DashboardGroup</code>. For that reasons, when configuring extractor following parameters must be provided: - dashboard_group_id (required) - dashboard_group_name (required) - cluster (required) - dashboard_group_description (optional)</p>"},{"location":"databuilder/#dashboardmetadata","title":"DashboardMetadata","text":"<p><code>ApacheSupersetMetadataExtractor</code> extracts metadata into <code>DashboardMetadata</code> model.</p>"},{"location":"databuilder/#metadata-available-in-rest-api","title":"Metadata available in REST API","text":"<ul> <li>Dashboard id (id)</li> <li>Dashboard name (dashboard_title)</li> <li>Dashboard URL (url)</li> </ul>"},{"location":"databuilder/#metadata-not-available-in-apache-superset-rest-api","title":"Metadata not available in Apache Superset REST API","text":"<ul> <li>Dashboard description</li> <li>Dashboard creation timestamp</li> </ul>"},{"location":"databuilder/#dashboardlastmodifiedtimestamp","title":"DashboardLastModifiedTimestamp","text":"<p><code>ApacheSupersetLastModifiedTimestampExtractor</code> extracts metadata into <code>DashboardLastModifiedTimestamp</code> model.</p>"},{"location":"databuilder/#available-in-rest-api","title":"Available in REST API","text":"<ul> <li>Dashboard last modified timestamp (changed_on property of dashboard)</li> </ul>"},{"location":"databuilder/#caution_1","title":"Caution!","text":"<p><code>changed_on</code> value does not provide timezone info so we assume it\u2019s UTC.</p>"},{"location":"databuilder/#sample-job-config","title":"Sample job config","text":"<pre><code>tmp_folder = f'/tmp/amundsen/dashboard'\n\ndict_config = {\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': f'{tmp_folder}/nodes',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR}': True,\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_ID}': '1',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_NAME}': 'dashboard group',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_DESCRIPTION}': 'dashboard group description',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.CLUSTER}': 'gold',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.APACHE_SUPERSET_SECURITY_SETTINGS_DICT}': dict(\n        username='admin',\n        password='admin',\n        provider='db')\n}\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=ApacheSupersetMetadataExtractor(), loader=FsNeo4jCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task)\n\njob.launch()\n</code></pre>"},{"location":"databuilder/#apachesupersettableextractor","title":"ApacheSupersetTableExtractor","text":"<p>The included <code>ApacheSupersetTableExtractor</code> provides support for extracting relationships between dashboards and tables. All Apache Superset extractors including this one use Apache Superset REST API (<code>api/v1</code>).</p>"},{"location":"databuilder/#caution_2","title":"Caution!","text":"<p>As table information in Apache Superset is minimal, following configuration options enable parametrization required to achieve proper relationship information: - <code>driver_to_database_mapping</code> - mapping between sqlalchemy <code>drivername</code> and actual <code>database</code> property of <code>TableMetadata</code> model. - <code>database_to_cluster_mapping</code> - mapping between Apache Superset Database ID and <code>cluster</code> from <code>TableMedata</code> model (defaults to <code>cluster</code> config of <code>extractor.apache_superset</code>)</p>"},{"location":"databuilder/#dashboardtable","title":"DashboardTable","text":""},{"location":"databuilder/#metadata-available-in-rest-api_1","title":"Metadata available in REST API","text":"<ul> <li>Table keys</li> </ul>"},{"location":"databuilder/#sample-job-config_1","title":"Sample job config","text":"<pre><code>tmp_folder = f'/tmp/amundsen/dashboard'\n\ndict_config = {\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': f'{tmp_folder}/nodes',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR}': True,\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_ID}': '1',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_NAME}': 'dashboard group',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_DESCRIPTION}': 'dashboard group description',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.CLUSTER}': 'gold',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.APACHE_SUPERSET_SECURITY_SETTINGS_DICT}': dict(\n        username='admin',\n        password='admin',\n        provider='db')\n}\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=ApacheSupersetTableExtractor(), loader=FsNeo4jCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task)\n\njob.launch()\n</code></pre>"},{"location":"databuilder/#apachesupersetchartextractor","title":"ApacheSupersetChartExtractor","text":"<p>The included <code>ApacheSupersetChartExtractor</code> provides support for extracting information on charts connected to given dashboard.</p>"},{"location":"databuilder/#caution_3","title":"Caution!","text":"<p>Currently there is no way to connect Apache Superset <code>Query</code> model to neither <code>Chart</code> nor <code>Dashboard</code> model. For that reason, to comply with Amundsen Databuilder data model, we register single <code>DashboardQuery</code> node serving as a bridge to which all the <code>DashboardChart</code> nodes are connected.</p>"},{"location":"databuilder/#dashboardchart","title":"DashboardChart","text":""},{"location":"databuilder/#metadata-available-in-rest-api_2","title":"Metadata available in REST API","text":"<ul> <li>Chart id (id)</li> <li>Chart name (chart_name)</li> <li>Chart type (viz_type)</li> </ul>"},{"location":"databuilder/#metadata-not-available-in-rest-api","title":"Metadata not available in REST API","text":"<ul> <li>Chart url</li> </ul>"},{"location":"databuilder/#sample-job-config_2","title":"Sample job config","text":"<pre><code>tmp_folder = f'/tmp/amundsen/dashboard'\n\ndict_config = {\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': f'{tmp_folder}/nodes',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR}': True,\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_ID}': '1',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_NAME}': 'dashboard group',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.DASHBOARD_GROUP_DESCRIPTION}': 'dashboard group description',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.CLUSTER}': 'gold',\n    f'extractor.apache_superset.{ApacheSupersetBaseExtractor.APACHE_SUPERSET_SECURITY_SETTINGS_DICT}': dict(\n        username='admin',\n        password='admin',\n        provider='db')\n}\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=ApacheSupersetChartExtractor(), loader=FsNeo4jCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task)\n\njob.launch()\n</code></pre>"},{"location":"databuilder/#pandasprofilingcolumnstatsextractor","title":"PandasProfilingColumnStatsExtractor","text":"<p>Pandas profiling is a library commonly used by Data Engineer and Scientists to calculate advanced data profiles on data. It is run on pandas dataframe and results in json file containing (amongst other things) descriptive and quantile statistics on columns.</p>"},{"location":"databuilder/#required-input-parameters","title":"Required input parameters","text":"<ul> <li><code>FILE_PATH</code> - file path to pandas-profiling json report</li> <li><code>TABLE_NAME</code> - name of the table for which report was calculated</li> <li><code>SCHEMA_NAME</code> - name of the schema from which table originates</li> <li><code>DATABASE_NAME</code> - name of database technology from which table originates</li> <li><code>CLUSTER_NAME</code> - name of the cluster from which table originates</li> </ul>"},{"location":"databuilder/#optional-input-parameters","title":"Optional input parameters","text":"<ul> <li><code>PRECISION</code> - precision for metrics of <code>float</code> type. Defaults to <code>3</code> meaning up to 3 digits after decimal point.</li> <li><code>STAT_MAPPINGS</code> - if you wish to collect only selected set of metrics configure this option with dictionary of following format:<ul> <li>key - raw name of the stat in pandas-profiling</li> <li>value - tuple of 2 elements:<ul> <li>first value of the tuple - full name of the stat (this influences what will be rendered for user in UI)</li> <li>second value of the tuple - function modifying the stat (by default we just do type casting)</li> </ul> </li> </ul> </li> </ul> <p>Such dictionary should in that case contain only keys of metrics you wish to collect.</p> <p>For example - if you want only min and max value of a column, provide extractor with configuration option:</p> <pre><code>PandasProfilingColumnStatsExtractor.STAT_MAPPINGS = {'max': ('Maximum', float), 'min': ('Minimum', float)}\n</code></pre> <p>Complete set of available metrics is defined as DEFAULT_STAT_MAPPINGS attribute of PandasProfilingColumnStatsExtractor.</p>"},{"location":"databuilder/#common-usage-patterns","title":"Common usage patterns","text":"<p>As pandas profiling is executed on top of pandas dataframe, it is up to the user to populate the dataframe before running the report calculation (and subsequently the extractor). While doing so remember that it might not be a good idea to run the report on a complete set of rows if your tables are very sparse. In such case it is recommended to dump a subset of rows to pandas dataframe beforehand and calculate the report on just a sample of original data.</p>"},{"location":"databuilder/#spark-support","title":"Spark support","text":"<p>Support for native execution of pandas-profiling on Spark Dataframe is currently worked on and should come in the future.</p>"},{"location":"databuilder/#sample-job-config_3","title":"Sample job config","text":"<pre><code>import pandas as pd\nimport pandas_profiling\nfrom pyhocon import ConfigFactory\nfrom sqlalchemy import create_engine\n\nfrom databuilder.extractor.pandas_profiling_column_stats_extractor import PandasProfilingColumnStatsExtractor\nfrom databuilder.job.job import DefaultJob\nfrom databuilder.loader.file_system_neo4j_csv_loader import FsNeo4jCSVLoader\nfrom databuilder.task.task import DefaultTask\n\ntable_name = 'video_game_sales'\nschema_name = 'superset'\n\n# Load table contents to pandas dataframe\ndb_uri = f'postgresql://superset:superset@localhost:5432/{schema_name}'\nengine = create_engine(db_uri, echo=True)\n\ndf = pd.read_sql_table(\n    table_name,\n    con=engine\n)\n\n# Calculate pandas-profiling report on a table\nreport_file = '/tmp/table_report.json'\n\nreport = df.profile_report(sort=None)\nreport.to_file(report_file)\n\n# Run PandasProfilingColumnStatsExtractor on calculated report\ntmp_folder = f'/tmp/amundsen/column_stats'\n\ndict_config = {\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': f'{tmp_folder}/nodes',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR}': False,\n    'extractor.pandas_profiling.table_name': table_name,\n    'extractor.pandas_profiling.schema_name': schema_name,\n    'extractor.pandas_profiling.database_name': 'postgres',\n    'extractor.pandas_profiling.cluster_name': 'dev',\n    'extractor.pandas_profiling.file_path': report_file\n}\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=PandasProfilingColumnStatsExtractor(), loader=FsNeo4jCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task)\n\njob.launch()\n</code></pre>"},{"location":"databuilder/#elasticsearchmetadataextractor","title":"ElasticsearchMetadataExtractor","text":"<p>The included <code>ElasticsearchMetadataExtractor</code> provides support for extracting basic metadata for Elasticsearch indexes.</p> <p>It extracts index metadata into <code>TableMetadata</code> model so the results are retrievable the same way as table metadata.</p> <p>Index properties (fields) are treated as <code>ColumnMetadata</code>.</p>"},{"location":"databuilder/#technical-indexes","title":"Technical indexes","text":"<p>This extractor will collect metadata for all indexes of your Elasticsearch instance except for technical indices (which names start with <code>.</code>)</p>"},{"location":"databuilder/#configuration","title":"Configuration","text":"<p>Following configuration options are supported under <code>extractor.es_metadata</code> scope: - <code>cluster</code> (required) - name of the cluster of Elasticsearch instance we are extracting metadata from. - <code>schema</code> (required) - name of the schema of Elasticsearch instance we are extracting metadata from. - <code>client</code> (required) - object containing <code>Elasticsearch</code> class instance for connecting to Elasticsearch. - <code>extract_technical_details</code> (defaults to <code>False</code>) - if <code>True</code> index <code>aliases</code> and <code>settings</code> will be extracted as <code>Programmatic Descriptions</code>. - <code>correct_sort_order</code> (defaults to <code>False</code>) - if <code>True</code> column sort order will match Elasticsearch mapping order.</p>"},{"location":"databuilder/#sample-job-config_4","title":"Sample job config","text":"<pre><code>import os\n\nfrom elasticsearch import Elasticsearch\nfrom pyhocon import ConfigFactory\n\nfrom databuilder.extractor.es_metadata_extractor import ElasticsearchMetadataExtractor\nfrom databuilder.job.job import DefaultJob\nfrom databuilder.loader.file_system_neo4j_csv_loader import FsNeo4jCSVLoader\nfrom databuilder.task.task import DefaultTask\n\ntmp_folder = '/tmp/es_metadata'\n\nnode_files_folder = f'{tmp_folder}/nodes'\nrelationship_files_folder = f'{tmp_folder}/relationships'\n\ndict_config = {\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': node_files_folder,\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': relationship_files_folder,\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR}': True,\n    f'extractor.es_metadata.{ElasticsearchMetadataExtractor.CLUSTER}': 'demo',\n    f'extractor.es_metadata.{ElasticsearchMetadataExtractor.SCHEMA}': 'dev',\n    f'extractor.es_metadata.{ElasticsearchMetadataExtractor.ELASTICSEARCH_CLIENT_CONFIG_KEY}': Elasticsearch()\n}\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=ElasticsearchMetadataExtractor(), loader=FsNeo4jCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task)\n</code></pre>"},{"location":"databuilder/#elasticsearchcolumnstatsextractor","title":"ElasticsearchColumnStatsExtractor","text":"<p>The included <code>ElasticsearchColumnStatsExtractor</code> provides support for extracting basic statistics on numerical properties of Elasticsearch indexes.</p> <p>It extracts statistics using Elasticsearch aggregation <code>matrix_stats</code>. It disregards statistics named <code>covariance</code> and <code>correlation</code>.</p>"},{"location":"databuilder/#technical-indexes_1","title":"Technical indexes","text":"<p>This extractor will collect metadata for all indexes of your Elasticsearch instance except for technical indices (which names start with <code>.</code>)</p>"},{"location":"databuilder/#configuration_1","title":"Configuration","text":"<p>Following configuration options are supported under <code>extractor.es_column_stats</code> scope: - <code>cluster</code> (required) - name of the cluster of Elasticsearch instance we are extracting metadata from. - <code>schema</code> (required) - name of the schema of Elasticsearch instance we are extracting metadata from. - <code>client</code> (required) - object containing <code>Elasticsearch</code> class instance for connecting to Elasticsearch. - <code>extract_technical_details</code> (defaults to <code>False</code>) - if <code>True</code> index <code>aliases</code> and <code>settings</code> will be extracted as <code>Programmatic Descriptions</code>.</p>"},{"location":"databuilder/#sample-job-config_5","title":"Sample job config","text":"<pre><code>import os\n\nfrom elasticsearch import Elasticsearch\nfrom pyhocon import ConfigFactory\n\nfrom databuilder.extractor.es_column_stats_extractor import ElasticsearchColumnStatsExtractor\nfrom databuilder.job.job import DefaultJob\nfrom databuilder.loader.file_system_neo4j_csv_loader import FsNeo4jCSVLoader\nfrom databuilder.task.task import DefaultTask\n\ntmp_folder = '/tmp/es_column_stats'\n\nnode_files_folder = f'{tmp_folder}/nodes'\nrelationship_files_folder = f'{tmp_folder}/relationships'\n\ndict_config = {\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': node_files_folder,\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': relationship_files_folder,\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR}': True,\n    f'extractor.es_column_stats.{ElasticsearchColumnStatsExtractor.CLUSTER}': 'demo',\n    f'extractor.es_column_stats.{ElasticsearchColumnStatsExtractor.SCHEMA}': 'dev',\n    f'extractor.es_column_stats.{ElasticsearchColumnStatsExtractor.ELASTICSEARCH_CLIENT_CONFIG_KEY}': Elasticsearch()\n}\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=ElasticsearchColumnStatsExtractor(), loader=FsNeo4jCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task)\n</code></pre>"},{"location":"databuilder/#elasticsearchwatermarkextractor","title":"ElasticsearchWatermarkExtractor","text":"<p>The included <code>ElasticsearchWatermarkExtractor</code> provides support for extracting watermarks for Elasticsearch indexes.</p>"},{"location":"databuilder/#technical-indexes_2","title":"Technical indexes","text":"<p>This extractor will collect metadata for all indexes of your Elasticsearch instance except for technical indices (which names start with <code>.</code>)</p>"},{"location":"databuilder/#configuration_2","title":"Configuration","text":"<p>Following configuration options are supported under <code>extractor.es_watermark</code> scope: - <code>cluster</code> (required) - name of the cluster of Elasticsearch instance we are extracting metadata from. - <code>schema</code> (required) - name of the schema of Elasticsearch instance we are extracting metadata from. - <code>client</code> (required) - object containing <code>Elasticsearch</code> class instance for connecting to Elasticsearch. - <code>time_field</code> (defaults to <code>@timestamp</code>) - name of the field representing time.</p>"},{"location":"databuilder/#sample-job-config_6","title":"Sample job config","text":"<pre><code>from elasticsearch import Elasticsearch\nfrom pyhocon import ConfigFactory\n\nfrom databuilder.extractor.es_watermark_extractor import ElasticsearchWatermarkExtractor\nfrom databuilder.job.job import DefaultJob\nfrom databuilder.loader.file_system_neo4j_csv_loader import FsNeo4jCSVLoader\nfrom databuilder.task.task import DefaultTask\n\ntmp_folder = '/tmp/es_watermark'\n\nnode_files_folder = f'{tmp_folder}/nodes'\nrelationship_files_folder = f'{tmp_folder}/relationships'\n\ndict_config = {\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.NODE_DIR_PATH}': node_files_folder,\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.RELATION_DIR_PATH}': relationship_files_folder,\n    f'loader.filesystem_csv_neo4j.{FsNeo4jCSVLoader.SHOULD_DELETE_CREATED_DIR}': True,\n    f'extractor.es_watermark.{ElasticsearchWatermarkExtractor.CLUSTER}': 'demo',\n    f'extractor.es_watermark.{ElasticsearchWatermarkExtractor.SCHEMA}': 'dev',\n    f'extractor.es_watermark.{ElasticsearchWatermarkExtractor.ELASTICSEARCH_TIME_FIELD}': 'time',\n    f'extractor.es_watermark.{ElasticsearchWatermarkExtractor.ELASTICSEARCH_CLIENT_CONFIG_KEY}': Elasticsearch()\n}\n\njob_config = ConfigFactory.from_dict(dict_config)\n\ntask = DefaultTask(extractor=ElasticsearchWatermarkExtractor(), loader=FsNeo4jCSVLoader())\n\njob = DefaultJob(conf=job_config,\n                 task=task)\n</code></pre>"},{"location":"databuilder/#bamboohruserextractor","title":"BamboohrUserExtractor","text":"<p>The included <code>BamboohrUserExtractor</code> provides support for extracting basic user metadata from BambooHR.  For companies and organizations that use BambooHR to store employee information such as email addresses, first names, last names, titles, and departments, use the <code>BamboohrUserExtractor</code> to populate Amundsen user data.</p> <p>A sample job config is shown below.</p> <pre><code>extractor = BamboohrUserExtractor()\ntask = DefaultTask(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    'extractor.bamboohr_user.api_key': api_key,\n    'extractor.bamboohr_user.subdomain': subdomain,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#salesforceextractor","title":"SalesForceExtractor","text":"<p>The included <code>SalesForceExtractor</code> provides support for extracting basic SalesForce object metadata  from SalesForce.</p> <p>This extractor depends on the Python client simple-salesforce.</p> <p>A sample job config is shown below. Some notes about the configuration keys. </p> <p>This extractor currently only supports connecting to SalesForce with a username, password, and security token.  You pass these values in as configuration keys. </p> <p>The extractor will by default pull all SalesForce metadata objects which is likely not what you want. To only pull specific SalesForce metadata objects specify their names with the <code>SalesForceExtractor.OBJECT_NAMES_KEY</code>.</p> <p>There is no real notion of a schema for the SalesForce metadata objects but you still need to specify one. </p> <pre><code>from databuilder.extractor.salesforce_extractor import SalesForceExtractor\nextractor = SalesForceExtractor()\ntask = SalesForceExtractor(extractor=extractor, loader=FsNeo4jCSVLoader())\n\njob_config = ConfigFactory.from_dict({\n    f\"extractor.salesforce_metadata.{SalesForceExtractor.USERNAME_KEY}\": \"user\",\n    f\"extractor.salesforce_metadata.{SalesForceExtractor.PASSWORD_KEY}\": \"password\",\n    f\"extractor.salesforce_metadata.{SalesForceExtractor.SECURITY_TOKEN_KEY}\": \"token\",\n    f\"extractor.salesforce_metadata.{SalesForceExtractor.SCHEMA_KEY}\": \"default\",\n    f\"extractor.salesforce_metadata.{SalesForceExtractor.CLUSTER_KEY}\": \"gold\",\n    f\"extractor.salesforce_metadata.{SalesForceExtractor.DATABASE_KEY}\": \"salesforce\",\n    f\"extractor.salesforce_metadata.{SalesForceExtractor.OBJECT_NAMES_KEY}\": [\"Account\", \"Profile\"]\n})\n\njob = DefaultJob(conf=job_config,\n                 task=task,\n                 publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#eventbridgeextractor","title":"EventBridgeExtractor","text":"<p>An extractor that extracts schema metadata from AWS EventBridge schema registries.</p> <p>A sample job config is shown below.</p> <pre><code>job_config = ConfigFactory.from_dict({\n    f\"extractor.eventbridge.{EventBridgeExtractor.REGION_NAME_KEY}\": \"aws_region\",\n    f\"extractor.eventbridge.{EventBridgeExtractor.REGISTRY_NAME_KEY}\": \"eventbridge_schema_registry_name\",\n})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=EventBridgeExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre>"},{"location":"databuilder/#kafkaschemaregistryextractor","title":"KafkaSchemaRegistryExtractor","text":"<p>An extractor that extracts schema metadata Confluent Kafka Schema registry with Avro format.</p> <p>A sample job config is shown below.</p> <pre><code>job_config = ConfigFactory.from_dict({\n    f\"extractor.kafka_schema_registry.{KafkaSchemaRegistryExtractor.REGISTRY_URL_KEY}\": \"http://localhost:8081\",\n    f\"extractor.kafka_schema_registry.{KafkaSchemaRegistryExtractor.REGISTRY_USERNAME_KEY}\": \"username\",\n    f\"extractor.kafka_schema_registry.{KafkaSchemaRegistryExtractor.REGISTRY_PASSWORD_KEY}\": \"password\",\n})\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=KafkaSchemaRegistryExtractor(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre> <p>Note: username and password are not mandatory. Only provide if you schema registry need authorization.</p>"},{"location":"databuilder/#list-of-transformers","title":"List of transformers","text":"<p>Transformers are implemented by subclassing Transformer and implementing <code>transform(self, record)</code>. A transformer can:</p> <ul> <li>Modify a record and return it,</li> <li>Return <code>None</code> to filter a record out,</li> <li>Yield multiple records. This is useful for e.g. inferring metadata (such as ownership) from table descriptions.</li> </ul>"},{"location":"databuilder/#chainedtransformer","title":"ChainedTransformer","text":"<p>A chanined transformer that can take multiple transformers, passing each record through the chain.</p>"},{"location":"databuilder/#regexstrreplacetransformer","title":"RegexStrReplaceTransformer","text":"<p>Generic string replacement transformer using REGEX. User can pass list of tuples where tuple contains regex and replacement pair. <pre><code>job_config = ConfigFactory.from_dict({\n    'transformer.regex_str_replace.{}'.format(REGEX_REPLACE_TUPLE_LIST): [(',', ' '), ('\"', '')],\n    'transformer.regex_str_replace.{}'.format(ATTRIBUTE_NAME): 'instance_field_name',})\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=AnyExtractor(),\n        transformer=RegexStrReplaceTransformer(),\n        loader=AnyLoader()))\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#templatevariablesubstitutiontransformer","title":"TemplateVariableSubstitutionTransformer","text":"<p>Adds or replaces field in Dict by string.format based on given template and provide record Dict as a template parameter.</p>"},{"location":"databuilder/#dicttomodel","title":"DictToModel","text":"<p>Transforms dictionary into model.</p>"},{"location":"databuilder/#timestampstringtoepoch","title":"TimestampStringToEpoch","text":"<p>Transforms string timestamp into int epoch.</p>"},{"location":"databuilder/#removefieldtransformer","title":"RemoveFieldTransformer","text":"<p>Remove fields from the Dict.</p>"},{"location":"databuilder/#tabletagtransformer","title":"TableTagTransformer","text":"<p>Adds the same set of tags to all tables produced by the job.</p>"},{"location":"databuilder/#generictransformer","title":"GenericTransformer","text":"<p>Transforms dictionary based on callback function that user provides.</p>"},{"location":"databuilder/#complextypetransformer","title":"ComplexTypeTransformer","text":"<p>Transforms complex types for columns in a table by using a configured parsing function. The transformer takes a <code>TableMetadata</code> object and iterates over its list of <code>ColumnMetadata</code> objects. The configured parser takes each column as input and sets the column\u2019s <code>type_metadata</code> field with the parsed results contained in a <code>TypeMetadata</code> object.</p> <p>If you use Hive as a data store: Configure this transformer with the Hive parser.</p> <p>If you do not use Hive as a data store: You will need to write a custom parsing function for transforming column type strings into nested <code>TypeMetadata</code> objects. You are free to use the Hive parser as a starting point. You can also look online to try to find either a grammar or some OSS prior art, as writing a parser from scratch can get a little involved. We strongly recommend leveraging PyParsing instead of regex, etc.</p> <p>New parsing functions should take the following arguments: - Column type string - Column name - <code>ColumnMetadata</code> object itself</p> <p>Within the parsing function, TypeMetadata objects should be created by passing its name, parent object, and type string.</p> <p>Things to know about TypeMetadata - If the existing subclasses do not cover all the required complex types, the base class can be extended to create any new ones that are needed. - Each new subclass should implement a <code>is_terminal_type</code> function, which allows the node and relation iterators to check whether to continue creating the next nested level or to stop due to reaching a terminal node. - <code>ScalarTypeMetadata</code> is the default type class that represents a terminal state. This should be used to set any column\u2019s <code>type_metadata</code> when it is not a complex type, or for the innermost terminal state for any complex type. Having all the columns set the <code>type_metadata</code> field allows the frontend to know to use the correct nested column display. - Subclasses should set a <code>kind</code> field that specifies what kind of complex type they are. This is used by the frontend for specific type handling. For example, for arrays and maps a smaller row is inserted in the display table to differentiate them from named nested columns such as structs.</p>"},{"location":"databuilder/#list-of-loader","title":"List of loader","text":""},{"location":"databuilder/#fsneo4jcsvloader","title":"FsNeo4jCSVLoader","text":"<p>Write node and relationship CSV file(s) that can be consumed by Neo4jCsvPublisher. It assumes that the record it consumes is instance of Neo4jCsvSerializable.</p> <pre><code>job_config = ConfigFactory.from_dict({\n    'loader.filesystem_csv_neo4j.{}'.format(FsNeo4jCSVLoader.NODE_DIR_PATH): node_files_folder,\n    'loader.filesystem_csv_neo4j.{}'.format(FsNeo4jCSVLoader.RELATION_DIR_PATH): relationship_files_folder},)\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=AnyExtractor(),\n        loader=FsNeo4jCSVLoader()),\n    publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#genericloader","title":"GenericLoader","text":"<p>Loader class that calls user provided callback function with record as a parameter</p> <p>Example that pushes Mode Dashboard accumulated usage via GenericLoader where callback_function expected to insert record to data warehouse.</p> <pre><code>extractor = ModeDashboardUsageExtractor()\ntask = DefaultTask(extractor=extractor,\n                   loader=GenericLoader(), )\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_BEARER_TOKEN): mode_bearer_token,\n    'loader.generic.callback_function': callback_function\n})\n\njob = DefaultJob(conf=job_config, task=task)\njob.launch()\n</code></pre>"},{"location":"databuilder/#fselasticsearchjsonloader","title":"FSElasticsearchJSONLoader","text":"<p>Write Elasticsearch document in JSON format which can be consumed by ElasticsearchPublisher. It assumes that the record it consumes is instance of ElasticsearchDocument.</p> <pre><code>data_file_path = '/var/tmp/amundsen/search_data.json'\n\njob_config = ConfigFactory.from_dict({\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY): data_file_path,\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY): 'w',})\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=AnyExtractor(),\n        loader=FSElasticsearchJSONLoader()),\n    publisher=ElasticsearchPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#fsatlascsvloader","title":"FsAtlasCSVLoader","text":"<p>Write node and relationship CSV file(s) that can be consumed by AtlasCsvPublisher. It assumes that the record it  consumes is instance of AtlasSerializable.</p> <pre><code>from pyhocon import ConfigFactory\nfrom databuilder.job.job import DefaultJob\nfrom databuilder.loader.file_system_atlas_csv_loader import FsAtlasCSVLoader\nfrom databuilder.task.task import DefaultTask\n\ntmp_folder = f'/tmp/amundsen/dashboard'\n\njob_config = ConfigFactory.from_dict({\n    f'loader.filesystem_csv_atlas.{FsAtlasCSVLoader.ENTITY_DIR_PATH}': f'{tmp_folder}/entities',\n    f'loader.filesystem_csv_atlas.{FsAtlasCSVLoader.RELATIONSHIP_DIR_PATH}': f'{tmp_folder}/relationships'\n})\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=AnyExtractor(),\n        loader=FsAtlasCSVLoader()),\n    publisher=AnyPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#list-of-publisher","title":"List of publisher","text":""},{"location":"databuilder/#neo4jcsvpublisher","title":"Neo4jCsvPublisher","text":"<p>A Publisher takes two folders for input and publishes to Neo4j. One folder will contain CSV file(s) for Node where the other folder will contain CSV file(s) for Relationship. Neo4j follows Label Node properties Graph and refer to here for more information</p> <pre><code>node_files_folder = '{tmp_folder}/nodes/'.format(tmp_folder=tmp_folder)\nrelationship_files_folder = '{tmp_folder}/relationships/'.format(tmp_folder=tmp_folder)\n\njob_config = ConfigFactory.from_dict({\n    'loader.filesystem_csv_neo4j.{}'.format(FsNeo4jCSVLoader.NODE_DIR_PATH): node_files_folder,\n    'loader.filesystem_csv_neo4j.{}'.format(FsNeo4jCSVLoader.RELATION_DIR_PATH): relationship_files_folder,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NODE_FILES_DIR): node_files_folder,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.RELATION_FILES_DIR): relationship_files_folder,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_END_POINT_KEY): neo4j_endpoint,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_USER): neo4j_user,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_PASSWORD): neo4j_password,\n    'publisher.neo4j.{}'.format(neo4j_csv_publisher.NEO4J_ENCRYPTED): True,\n})\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=AnyExtractor(),\n        loader=FsNeo4jCSVLoader()),\n    publisher=Neo4jCsvPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#elasticsearchpublisher","title":"ElasticsearchPublisher","text":"<p>Elasticsearch Publisher uses Bulk API to load data from JSON file. Elasticsearch publisher supports atomic operation by utilizing alias in Elasticsearch. A new index is created and data is uploaded into it. After the upload is complete, index alias is swapped to point to new index from old index and traffic is routed to new index. <pre><code>data_file_path = '/var/tmp/amundsen/search_data.json'\n\njob_config = ConfigFactory.from_dict({\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY): data_file_path,\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY): 'w',\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY): data_file_path,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY): 'r',\n    'publisher.elasticsearch{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY): elasticsearch_client,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY): elasticsearch_new_index,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY): elasticsearch_doc_type,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY): elasticsearch_index_alias,\n})\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=AnyExtractor(),\n        loader=FSElasticsearchJSONLoader()),\n    publisher=ElasticsearchPublisher())\njob.launch()\n</code></pre></p>"},{"location":"databuilder/#atlascsvpublisher","title":"AtlasCsvPublisher","text":"<p>A Publisher takes two folders for input and publishes to Atlas. One folder will contain CSV file(s) for Entity where the other folder will contain CSV file(s) for Relationship.</p>"},{"location":"databuilder/#amundsen-atlas-types","title":"Amundsen &lt;&gt; Atlas Types","text":"<p>Atlas publisher requires registering appropriate entity types in Atlas. This can be achieved in two ways:</p>"},{"location":"databuilder/#register-entity-types-directly-in-publisher","title":"Register entity types directly in publisher","text":"<p>By default publisher will register proper entity types for you. This is achieved with <code>register_entity_types</code> configuration option of the publisher, which defaults to <code>True</code>.</p> <p>If your metadata synchronization job consists of several extractors leveraging <code>AtlasCSVPublisher</code> it is recommended to have this option turned on only for the first extractor.</p>"},{"location":"databuilder/#register-entity-types-using-standalone-script","title":"Register entity types using standalone script","text":"<p>You can register entity types separately - below script might serve as a baseline and will probably need adjusting AtlasClient to your environment. <pre><code>from apache_atlas.client.base_client import AtlasClient\n\nfrom databuilder.types.atlas import AtlasEntityInitializer\n\nclient = AtlasClient('http://localhost:21000', ('admin', 'admin'))\n\ninit = AtlasEntityInitializer(client)\n\ninit.create_required_entities()\n</code></pre></p>"},{"location":"databuilder/#caution_4","title":"Caution!","text":"<p>Whenever you upgrade your databuilder version it is important to re-run <code>AtlasEntityInitializer</code> as there might be new changes to entity types required for Atlas integration to work properly.</p>"},{"location":"databuilder/#sample-script","title":"Sample script","text":"<pre><code>from apache_atlas.client.base_client import AtlasClient\nfrom pyhocon import ConfigFactory\nfrom databuilder.job.job import DefaultJob\nfrom databuilder.loader.file_system_atlas_csv_loader import FsAtlasCSVLoader\nfrom databuilder.publisher.atlas_csv_publisher import AtlasCSVPublisher\nfrom databuilder.task.task import DefaultTask\n\ntmp_folder = f'/tmp/amundsen/dashboard'\n\njob_config = ConfigFactory.from_dict({\n    f'loader.filesystem_csv_atlas.{FsAtlasCSVLoader.ENTITY_DIR_PATH}': f'{tmp_folder}/entities',\n    f'loader.filesystem_csv_atlas.{FsAtlasCSVLoader.RELATIONSHIP_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.ATLAS_CLIENT}': AtlasClient('http://localhost:21000', ('admin', 'admin')) ,\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.ENTITY_DIR_PATH}': f'{tmp_folder}/entities',\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.RELATIONSHIP_DIR_PATH}': f'{tmp_folder}/relationships',\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.ATLAS_ENTITY_CREATE_BATCH_SIZE}': 10,\n    f'publisher.atlas_csv_publisher.{AtlasCSVPublisher.REGISTER_ENTITY_TYPES}': True\n})\n\njob = DefaultJob(\n    conf=job_config,\n    task=DefaultTask(\n        extractor=AnyExtractor(),\n        loader=FsAtlasCSVLoader()),\n    publisher=AtlasCSVPublisher())\njob.launch()\n</code></pre>"},{"location":"databuilder/#callback","title":"Callback","text":"<p>Callback interface is built upon a Observer pattern where the participant want to take any action when target\u2019s state changes.</p> <p>Publisher is the first one adopting Callback where registered Callback will be called either when publish succeeded or when publish failed. In order to register callback, Publisher provides register_call_back method.</p> <p>One use case is for Extractor that needs to commit when job is finished (e.g: Kafka). Having Extractor register a callback to Publisher to commit when publish is successful, extractor can safely commit by implementing commit logic into on_success method.</p>"},{"location":"databuilder/#rest-api-query","title":"REST API Query","text":"<p>Databuilder now has a generic REST API Query capability that can be joined each other. Most of the cases of extraction is currently from Database or Datawarehouse that is queryable via SQL. However, not all metadata sources provide our access to its Database and they mostly provide REST API to consume their metadata.</p> <p>The challenges come with REST API is that:</p> <ol> <li>there\u2019s no explicit standard in REST API. Here, we need to conform to majority of cases (HTTP call with JSON payload &amp; response) but open for extension for different authentication scheme, and different way of pagination, etc.</li> <li>It is hardly the case that you would get what you want from one REST API call. It is usually the case that you need to snitch (JOIN) multiple REST API calls together to get the information you want.</li> </ol> <p>To solve this challenges, we introduce RestApiQuery</p> <p>RestAPIQuery is:  1. Assuming that REST API is using HTTP(S) call with GET method \u2013 RestAPIQuery intention\u2019s is read, not write \u2013 where basic HTTP auth is supported out of the box. There\u2019s extension point on other authentication scheme such as Oauth, and pagination, etc. (See ModePaginatedRestApiQuery for pagination)  2. Usually, you want the subset of the response you get from the REST API call \u2013 value extraction. To extract the value you want, RestApiQuery uses JSONPath which is similar product as XPATH of XML.  3. You can JOIN multiple RestApiQuery together.</p> <p>More detail on JOIN operation in RestApiQuery:  1. It joins multiple RestApiQuery together by accepting prior RestApiQuery as a constructor \u2013 a Decorator pattern  2. In REST API, URL is the one that locates the resource we want. Here, JOIN simply means we need to find resource based on the identifier that other query\u2019s result has. In other words, when RestApiQuery forms URL, it uses previous query\u2019s result to compute the URL <code>e.g: Previous record: {\"dashboard_id\": \"foo\"}, URL before: http://foo.bar/dashboard/{dashboard_id} URL after compute: http://foo.bar/dashboard/foo</code> With this pattern RestApiQuery supports 1:1 and 1:N JOIN relationship. (GROUP BY or any other aggregation, sub-query join is not supported)</p> <p>To see in action, take a peek at ModeDashboardExtractor Also, take a look at how it extends to support pagination at ModePaginatedRestApiQuery.</p>"},{"location":"databuilder/#removing-stale-data-in-neo4j-neo4jstalenessremovaltask","title":"Removing stale data in Neo4j \u2013 Neo4jStalenessRemovalTask:","text":"<p>As Databuilder ingestion mostly consists of either INSERT OR UPDATE, there could be some stale data that has been removed from metadata source but still remains in Neo4j database. Neo4jStalenessRemovalTask basically detects staleness and removes it.</p> <p>In Neo4jCsvPublisher, it adds attributes \u201cpublished_tag\u201d and \u201cpublisher_last_updated_epoch_ms\u201d on every nodes and relations. You can use either of these two attributes to detect staleness and remove those stale node or relation from the database.</p> <p>NOTE: data can exist without either attributes \u201cpublished_tag\u201d or \u201cpublisher_last_updated_epoch_ms\u201d if it is created by an Amundsen user rather than by the publisher. In this case you may not want to have these nodes marked as stale and deleted. To keep these nodes, you can set a configured value <code>retain_data_with_no_publisher_metadata</code> to <code>True</code>:</p> <pre><code>task = Neo4jStalenessRemovalTask()\njob_config_dict = {\n    'job.identifier': 'remove_stale_data_job',\n    'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint,\n    'task.remove_stale_data.neo4j_user': neo4j_user,\n    'task.remove_stale_data.neo4j_password': neo4j_password,\n    'task.remove_stale_data.staleness_max_pct': 10,\n    'task.remove_stale_data.target_nodes': ['Table', 'Column'],\n    'task.remove_stale_data.job_publish_tag': '2020-03-31',\n    'task.remove_stale_data.retain_data_with_no_publisher_metadata': True\n}\njob_config = ConfigFactory.from_dict(job_config_dict)\njob = DefaultJob(conf=job_config, task=task)\njob.launch()\n</code></pre>"},{"location":"databuilder/#using-published_tag-to-remove-stale-data","title":"Using \u201cpublished_tag\u201d to remove stale data","text":"<p>Use published_tag to remove stale data, when it is certain that non-matching tag is stale once all the ingestion is completed. For example, suppose that you use current date (or execution date in Airflow) as a published_tag, \u201c2020-03-31\u201d. Once Databuilder ingests all tables and all columns, all table nodes and column nodes should have published_tag as \u201c2020-03-31\u201d. It is safe to assume that table nodes and column nodes whose published_tag is different \u2013 such as \u201c2020-03-30\u201d or \u201c2020-02-10\u201d \u2013 means that it is deleted from the source metadata. You can use Neo4jStalenessRemovalTask to delete those stale data.</p> <pre><code>task = Neo4jStalenessRemovalTask()\njob_config_dict = {\n    'job.identifier': 'remove_stale_data_job',\n    'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint,\n    'task.remove_stale_data.neo4j_user': neo4j_user,\n    'task.remove_stale_data.neo4j_password': neo4j_password,\n    'task.remove_stale_data.staleness_max_pct': 10,\n    'task.remove_stale_data.target_nodes': ['Table', 'Column'],\n    'task.remove_stale_data.job_publish_tag': '2020-03-31'\n}\njob_config = ConfigFactory.from_dict(job_config_dict)\njob = DefaultJob(conf=job_config, task=task)\njob.launch()\n</code></pre> <p>Note that there\u2019s protection mechanism, staleness_max_pct, that protect your data being wiped out when something is clearly wrong. \u201cstaleness_max_pct\u201d basically first measure the proportion of elements that will be deleted and if it exceeds threshold per type ( 10% on the configuration above ), the deletion won\u2019t be executed and the task aborts.</p>"},{"location":"databuilder/#using-publisher_last_updated_epoch_ms-to-remove-stale-data","title":"Using \u201cpublisher_last_updated_epoch_ms\u201d to remove stale data","text":"<p>You can think this approach as TTL based eviction. This is particularly useful when there are multiple ingestion pipelines and you cannot be sure when all ingestion is done. In this case, you might still can say that if specific node or relation has not been published past 3 days, it\u2019s stale data.</p> <pre><code>task = Neo4jStalenessRemovalTask()\njob_config_dict = {\n    'job.identifier': 'remove_stale_data_job',\n    'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint,\n    'task.remove_stale_data.neo4j_user': neo4j_user,\n    'task.remove_stale_data.neo4j_password': neo4j_password,\n    'task.remove_stale_data.staleness_max_pct': 10,\n    'task.remove_stale_data.target_relations': ['READ', 'READ_BY'],\n    'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3\n}\njob_config = ConfigFactory.from_dict(job_config_dict)\njob = DefaultJob(conf=job_config, task=task)\njob.launch()\n</code></pre> <p>Above configuration is trying to delete stale usage relation (READ, READ_BY), by deleting READ or READ_BY relation that has not been published past 3 days. If number of elements to be removed is more than 10% per type, this task will be aborted without executing any deletion.</p>"},{"location":"databuilder/#using-node-and-relation-conditions-to-remove-stale-data","title":"Using node and relation conditions to remove stale data","text":"<p>You may want to remove stale nodes and relations that meet certain conditions rather than all of a given type. To do this, you can specify the inputs to be a list of TargetWithCondition objects that each define a target type and a condition. Only stale nodes or relations of that type and that meet the condition will be removed when using this type of input.</p> <p>Node conditions can make use of the predefined variable <code>target</code> which represents the node. Relation conditions can include the variables <code>target</code>, <code>start_node</code>, and <code>end_node</code> where <code>target</code> represents the relation and <code>start_node</code>/<code>end_node</code> represent the nodes on either side of the target relation. For some examples of conditions see below.</p> <pre><code>from databuilder.task.neo4j_staleness_removal_task import TargetWithCondition\n\ntask = Neo4jStalenessRemovalTask()\njob_config_dict = {\n    'job.identifier': 'remove_stale_data_job',\n    'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint,\n    'task.remove_stale_data.neo4j_user': neo4j_user,\n    'task.remove_stale_data.neo4j_password': neo4j_password,\n    'task.remove_stale_data.staleness_max_pct': 10,\n    'task.remove_stale_data.target_nodes': [TargetWithCondition('Table', '(target)-[:COLUMN]-&gt;(:Column)'),  # All Table nodes that have a directional COLUMN relation to a Column node\n                                            TargetWithCondition('Column', '(target)-[]-(:Table) AND target.name=\\'column_name\\'')],  # All Column nodes named 'column_name' that have some relation to a Table node\n    'task.remove_stale_data.target_relations': [TargetWithCondition('COLUMN', '(start_node:Table)-[target]-&gt;(end_node:Column)'),  # All COLUMN relations that connect from a Table node to a Column node\n                                                TargetWithCondition('COLUMN', '(start_node:Column)-[target]-(end_node)')],  # All COLUMN relations that connect any direction between a Column node and another node\n    'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3\n}\njob_config = ConfigFactory.from_dict(job_config_dict)\njob = DefaultJob(conf=job_config, task=task)\njob.launch()\n</code></pre> <p>You can include multiple inputs of the same type with different conditions as seen in the target_relations list above. Attribute checks can also be added as shown in the target_nodes list.</p>"},{"location":"databuilder/#dry-run","title":"Dry run","text":"<p>Deletion is always scary and it\u2019s better to perform dryrun before put this into action. You can use Dry run to see what sort of Cypher query will be executed.</p> <pre><code>task = Neo4jStalenessRemovalTask()\njob_config_dict = {\n    'job.identifier': 'remove_stale_data_job',\n    'task.remove_stale_data.neo4j_endpoint': neo4j_endpoint,\n    'task.remove_stale_data.neo4j_user': neo4j_user,\n    'task.remove_stale_data.neo4j_password': neo4j_password,\n    'task.remove_stale_data.staleness_max_pct': 10,\n    'task.remove_stale_data.target_relations': ['READ', 'READ_BY'],\n    'task.remove_stale_data.milliseconds_to_expire': 86400000 * 3\n    'task.remove_stale_data.dry_run': True\n}\njob_config = ConfigFactory.from_dict(job_config_dict)\njob = DefaultJob(conf=job_config, task=task)\njob.launch()\n</code></pre>"},{"location":"databuilder/CHANGELOG/","title":"CHANGELOG","text":""},{"location":"databuilder/CHANGELOG/#440","title":"4.4.0","text":""},{"location":"databuilder/CHANGELOG/#features","title":"Features","text":"<ul> <li>Column lineage implementation &amp; sample ingest scripts (https://github.com/amundsen-io/amundsendatabuilder/pull/470)</li> <li>Add mysql staleness removal task (https://github.com/amundsen-io/amundsendatabuilder/pull/474)</li> <li>Add merge query results functionality to rest_api_query (https://github.com/amundsen-io/amundsen/pull/1058)</li> </ul>"},{"location":"databuilder/CHANGELOG/#bugfixes","title":"Bugfixes","text":"<ul> <li>Switch to discovery api for mode spaces (https://github.com/amundsen-io/amundsendatabuilder/pull/481)</li> <li>Update ModeDashboardExtractor to Mode discovery api (https://github.com/amundsen-io/amundsen/pull/1063)</li> <li>Updated redshift_metadata_extractor.py to extract external schema as well as local schema. (https://github.com/amundsen-io/amundsendatabuilder/pull/479)</li> <li>Upgrade dependency of Amundsen-gremlin (https://github.com/amundsen-io/amundsendatabuilder/pull/473)</li> <li>fix bug in rest_api_query (https://github.com/amundsen-io/amundsen/pull/1056)</li> </ul>"},{"location":"databuilder/CHANGELOG/#pre-440-changes","title":"Pre 4.4.0 changes","text":""},{"location":"databuilder/CHANGELOG/#feature","title":"Feature","text":"<ul> <li>Add support for tags based on atlas terms (#466) (<code>cc1caf3</code>)</li> <li>Make DescriptionMetadata inherit from GraphSerializable (#461) (<code>7f095fb</code>)</li> <li>Add TableSerializable and mysql_serializer (#459) (<code>4bb4452</code>)</li> <li>Neptune Data builder Integration (#438) (<code>303e8aa</code>)</li> <li>Add config key for connect_arg for SqlAlchemyExtractor (#434) (<code>7f3be0f</code>)</li> <li>Vertica metadata extractor (#433) (<code>f4bd207</code>)</li> <li>Multi-yield transformers (#396) (<code>49ae0ed</code>)</li> <li>Atlas_search_extractor | :tada: Initial commit. (#415) (<code>8c63307</code>)</li> <li>Sample Feast job with ES publisher (#425) (<code>453a18b</code>)</li> <li>Adding CsvTableBadgeExtractor (#417) (<code>592ee71</code>)</li> <li>Feast extractor (#414) (<code>2343a90</code>)</li> <li>Adding first pass of delta lake metadata extractor as well as a sample script on how it would be used. (#351) (<code>e8679aa</code>)</li> <li>Use parameters to allow special characters in neo4j cypher statement (#382) (<code>6fd5035</code>)</li> <li>Column level badges cont. (#381) (<code>af4b512</code>)</li> <li>Support dashboard chart in search (#383) (<code>6cced36</code>)</li> <li>Column level badges (#375) (<code>8beee3e</code>)</li> <li>Added Dremio extractor (#377) (<code>63f239f</code>)</li> <li>Add an extractor for pulling user information from BambooHR (#369) (<code>6802ab1</code>)</li> <li>Add sample_glue_loader script (#366) (<code>fa3f11b</code>)</li> <li>Parameterize Snowflake Schema in Snowflake Metadata Extractor (#361) (<code>aa4416c</code>)</li> <li>Mode Batch dashboard charrt API (#362) (<code>87213c5</code>)</li> <li>Create a RedshiftMetadataExtractor that supports late binding views (#356) (<code>4113cfd</code>)</li> <li>Add MySQL sample data loader (#359) (<code>871a176</code>)</li> <li>Add Snowflake table last updated timestamp extractor (#348) (<code>0bac11b</code>)</li> <li>Add Tableau dashboard metadata extractors (#333) (<code>46207ee</code>)</li> <li>Add github actions for databuilder (#336) (<code>236e7de</code>)</li> <li>Allow hive sql to be provided as config (#312) (<code>8075a6c</code>)</li> <li>Enhance glue extractor (#306) (<code>faa795c</code>)</li> <li>Add RedashDashboardExtractor for extracting dashboards from redash.io (#300) (<code>f1b0dfa</code>)</li> <li>Add a transformer that adds tags to all tables created in a job (#287) (<code>d2f4bd3</code>)</li> </ul>"},{"location":"databuilder/CHANGELOG/#fix","title":"Fix","text":"<ul> <li>Add support for Tableau multi-site deployment (#463) (<code>e35af58</code>)</li> <li>Avoid error by checking for existence before close. (#454) (<code>5cd0dc8</code>)</li> <li>Correct config getter (#455) (<code>4b37746</code>)</li> <li>Close SQL Alchemy connections. (#453) (<code>25124c1</code>)</li> <li>Add comma between bigquery requirements listings (#452) (<code>027edb9</code>)</li> <li>Increase the compatibility of id structure between the Databuilder and the Metadata Library (#445) (<code>6a13762</code>)</li> <li>Move \u2018grouped_tables\u2019 into <code>_retrieve_tables</code> (#430) (<code>26a0d0a</code>)</li> <li>Address PyAthena version (#429) (<code>7157c24</code>)</li> <li>Add csv badges back in Quickstart (#418) (<code>c0296b7</code>)</li> <li>Typo in Readme (#424) (<code>29bd72f</code>)</li> <li>Fix redash dashboard exporter (#422) (<code>fa626f5</code>)</li> <li>Update the key format of set \u2018grouped_tables\u2019 (#421) (<code>4c9e5f7</code>)</li> <li>Retry loop for exception caused by deadlock on badge node (#404) (<code>9fd1513</code>)</li> <li>FsNeo4jCSVLoader fails if nodes have disjoint keys (#408) (<code>c07cec9</code>)</li> <li>Cast dashboard usage to be int (#412) (<code>8bcc489</code>)</li> <li>Pandas \u2018nan\u2019 values (#409) (<code>3a28f46</code>)</li> <li>Add databuilder missing dependencies (#400) (<code>6718396</code>)</li> <li>Allow BigQuery Usage Extractor to extract usage for views (#399) (<code>8779229</code>)</li> <li>Hive metadata extractor not work on postgresql (#394) (<code>2992618</code>)</li> <li>Issues with inconsistency in case conversion (#388) (<code>9595866</code>)</li> <li>Update elasticsearch table index mapping (#373) (<code>88c0552</code>)</li> <li>Fix programmatic source data (#367) (<code>4f5df39</code>)</li> <li>Update connection string in Snowflake extractor to include wareh\u2026 (#357) (<code>a11d206</code>)</li> <li>Edge case in Snowflake information_schema.last_altered value (#360) (<code>c3e713e</code>)</li> <li>Correct typo in Snowflake Last Updated extract query (#358) (<code>5c2e98e</code>)</li> <li>Set Tableau URLs (base + API) via config (#349) (<code>1baec33</code>)</li> <li>Fix invalid timestamp handling in dashboard transformer (#339) (<code>030ef49</code>)</li> <li>Update postgres_sample_dag to set table extract job as upstream for elastic search publisher (#340) (<code>c79935e</code>)</li> <li>deps: Unpin attrs (#332) (<code>86f658d</code>)</li> <li>Cypher statement param issue in Neo4jStalenessRemovalTask (#307) (<code>0078761</code>)</li> <li>Added missing job tag key in hive_sample_dag.py (#308) (<code>d6714b7</code>)</li> <li>Fix sql for missing columns and mysql based dialects (#550) (#305) (<code>4b7b147</code>)</li> <li>Escape backslashes in Neo4jCsvPublisher  (<code>1faa713</code>)</li> <li>Variable organization in Model URL (#293) (<code>b4c24ef</code>)</li> </ul>"},{"location":"databuilder/CHANGELOG/#documentation","title":"Documentation","text":"<ul> <li>Minor fixes to README (#457) (<code>54e89ce</code>)</li> <li>Update DashboardMetadata docs (#402) (<code>093b3d6</code>)</li> <li>Fix broken doc link to dashboard_execution model (#296) (<code>24b3b0a</code>)</li> <li>Fix README.md (#301) (<code>ad5765a</code>)</li> </ul>"},{"location":"databuilder/docs/dashboard_ingestion_guide/","title":"Dashboard Ingestion guidance","text":"<p>(Currently this guidance is about using Databuilder to ingest Dashboard metadata into Neo4j and Elasticsearch)</p> <p>Dashboard ingestion consists of multiple Databuilder jobs and it can be described in four steps:</p> <ol> <li>Ingest base data to Neo4j.</li> <li>Ingest additional data and decorate Neo4j over base data.</li> <li>Update Elasticsearch index using Neo4j data</li> <li>Remove stale data</li> </ol> <p>Note that Databuilder jobs need to be sequenced as 1 -&gt; 2 -&gt; 3 -&gt; 4. To sequencing these jobs, Lyft uses Airflow to orchestrate the job, but Databuilder is not limited to Airflow and you can also simply use Python script to sequence it \u2013 not recommended for production though.</p> <p>Also, step 1, 3, 4 is expected to have one Databuilder job where Step 2 is expected to have multiple Databuilder jobs and number of Databuilder jobs in step 2 is expected to grow as we add more metadata into Dashboard. To improve performance, it is recommended, but not required, to execute Databuilder jobs in step 2 concurrently.</p> <p>Once finished step 1 and 2, you will have Graph like this: </p> <p>Here this documentation will be using Mode Dashboard as concrete example to show how to ingest Dashboard metadata. However, this ingestion process not limited to Mode Dashboard and any other Dashboard can follow this flow.</p>"},{"location":"databuilder/docs/dashboard_ingestion_guide/#1-ingest-base-data-to-neo4j","title":"1. Ingest base data to Neo4j.","text":"<p>Using ModeDashboardExtractor along with FsNeo4jCSVLoader and Neo4jCsvPublisher to add base information such as Dashboard group name, Dashboard group id, Dashboard group description, Dashboard name, Dashboard id, Dashboard description to Neo4j. Use this job configuration example to configure the job.</p>"},{"location":"databuilder/docs/dashboard_ingestion_guide/#2-ingest-additional-data-and-decorate-neo4j-over-base-data","title":"2. Ingest additional data and decorate Neo4j over base data.","text":"<p>Use other Mode dashboard\u2019s extractors in create &amp; launch multiple Databuilder jobs. Note that it all Databuilder job here will use FsNeo4jCSVLoader and Neo4jCsvPublisher where their configuration should be almost the same except the <code>NODE_FILES_DIR</code> and <code>RELATION_FILES_DIR</code> that is being used for temporary location to hold data.</p> <p>List of other Extractors can be found here</p>"},{"location":"databuilder/docs/dashboard_ingestion_guide/#21-ingest-dashboard-usage-data-and-decorate-neo4j-over-base-data","title":"2.1. Ingest Dashboard usage data and decorate Neo4j over base data.","text":"<p>Mode provide usage data (view count) per Dashboard, but this is accumulated usage data. The main use case of usage is search ranking and <code>accumulated usage</code> is not that much useful for Amundsen as we don\u2019t want to show certain Dashboard that was popular years ago and potentially deprecated.</p> <p>To bring recent usage information, we can <code>snapshot</code> accumulated usage per report daily and extract recent usage information (past 30 days, 60 days, 90 days that fits our view of recency). </p>"},{"location":"databuilder/docs/dashboard_ingestion_guide/#211-ingest-accumulated-usage-into-data-warehouse-eg-hive-bigquery-redshift-postgres-etc","title":"2.1.1. Ingest <code>accumulated usage</code> into Data warehouse (e.g: Hive, BigQuery, Redshift, Postgres, etc)","text":"<p>In this step, you can use ModeDashboardUsageExtractor to extract <code>accumulated_view_count</code> and load into Data warehouse of your choice by using GenericLoader.</p> <p>Note that GenericLoader just takes a callback function, and you need to provide a function that <code>INSERT</code> record into your Dataware house.</p> <p><pre><code>extractor = ModeDashboardUsageExtractor()\nloader = GenericLoader()\ntask = DefaultTask(extractor=extractor,\n                   loader=loader)\n\njob_config = ConfigFactory.from_dict({\n    '{}.{}'.format(extractor.get_scope(), ORGANIZATION): organization,\n    '{}.{}'.format(extractor.get_scope(), MODE_ACCESS_TOKEN): mode_token,\n    '{}.{}'.format(extractor.get_scope(), MODE_PASSWORD_TOKEN): mode_password,\n    '{}.{}'.format(loader.get_scope(), 'callback_function'): mode_dashboard_usage_loader_callback_function,\n})\n\njob = DefaultJob(conf=job_config, task=task)\njob.launch()\n</code></pre> Step 2. Extract past ? days usage data from your Data warehouse and publish it to Neo4j. You could use existing extractors to achieve this with DashboardUsage model along with FsNeo4jCSVLoader and Neo4jCsvPublisher.</p>"},{"location":"databuilder/docs/dashboard_ingestion_guide/#3-update-elasticsearch-index-using-neo4j-data","title":"3. Update Elasticsearch index using Neo4j data","text":"<p>Once data is ready in Neo4j, extract Neo4j data and push it to Elasticsearch using Neo4jSearchDataExtractor and ElasticsearchPublisher</p> <pre><code>tmp_es_file_path = '/var/tmp/amundsen_dashboard/elasticsearch_dashboard_upload/es_data.json'\n\nelasticsearch_new_index_name = 'dashboard_search_index_{ds}_{hex_str}'.format(ds='2020-05-12',\n                                                                              hex_str=uuid.uuid4().hex)\n\nelasticsearch_doc_type = 'dashboard'\nelasticsearch_index_alias = 'dashboard_search_index'\njob_config = ConfigFactory.from_dict({\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.GRAPH_URL_CONFIG_KEY):\n        neo4j_endpoint,\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.MODEL_CLASS_CONFIG_KEY):\n        'databuilder.models.dashboard_elasticsearch_document.DashboardESDocument',\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_USER):\n        neo4j_user,\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_AUTH_PW):\n        neo4j_password,\n    'extractor.search_data.extractor.neo4j.{}'.format(Neo4jExtractor.NEO4J_ENCRYPTED):\n        False,\n    'extractor.search_data.{}'.format(Neo4jSearchDataExtractor.CYPHER_QUERY_CONFIG_KEY):\n        Neo4jSearchDataExtractor.DEFAULT_NEO4J_DASHBOARD_CYPHER_QUERY,\n    'extractor.search_data.{}'.format(neo4j_csv_publisher.JOB_PUBLISH_TAG):\n        job_publish_tag,\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_PATH_CONFIG_KEY):\n        tmp_es_file_path,\n    'loader.filesystem.elasticsearch.{}'.format(FSElasticsearchJSONLoader.FILE_MODE_CONFIG_KEY):\n        'w',\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_PATH_CONFIG_KEY):\n        tmp_es_file_path,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.FILE_MODE_CONFIG_KEY):\n        'r',\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_CLIENT_CONFIG_KEY):\n        elasticsearch_client,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_NEW_INDEX_CONFIG_KEY):\n        elasticsearch_new_index,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_DOC_TYPE_CONFIG_KEY):\n        elasticsearch_doc_type,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_MAPPING_CONFIG_KEY):\n        DASHBOARD_ELASTICSEARCH_INDEX_MAPPING,\n    'publisher.elasticsearch.{}'.format(ElasticsearchPublisher.ELASTICSEARCH_ALIAS_CONFIG_KEY):\n        elasticsearch_index_alias,\n})\n\njob = DefaultJob(conf=job_config,\n                 task=DefaultTask(extractor=Neo4jSearchDataExtractor(),\n                                  loader=FSElasticsearchJSONLoader()),\n                 publisher=ElasticsearchPublisher())\njob.launch()\n</code></pre> <p>*Note that <code>DASHBOARD_ELASTICSEARCH_INDEX_MAPPING</code> is defined here.  </p>"},{"location":"databuilder/docs/dashboard_ingestion_guide/#4-remove-stale-data","title":"4. Remove stale data","text":"<p>Dashboard ingestion, like Table ingestion, is UPSERT (CREATE OR UPDATE) operation and there could be some data deleted on source. Not removing it in Neo4j basically leaving a stale data in Amundsen.</p> <p>You can use Neo4jStalenessRemovalTask to remove stale data.</p> <p>There are two strategies to remove stale data. One is to use <code>job_publish_tag</code> and the other one is to use <code>milliseconds_to_expire</code>.</p> <p>For example, you could use <code>job_publish_tag</code> to remove stale <code>DashboardGroup</code>, <code>Dashboard</code>, and  <code>Query</code> nodes.  And you could use <code>milliseconds_to_expire</code> on <code>Timestamp</code> node,  <code>READ</code> relation, and <code>READ_BY</code>.  One of the main reasons to use <code>milliseconds_to_expire</code> is to avoid race condition and it is explained more here</p>"},{"location":"databuilder/docs/models/","title":"Amundsen Models","text":""},{"location":"databuilder/docs/models/#overview","title":"Overview","text":"<p>These are the python classes that live in databuilder/models/.</p> <p>Models represent the data structures that live in either neo4j (if the model extends Neo4jSerializable) or in elasticsearch.</p> <p>Models that extend Neo4jSerializable have methods to create: - the nodes - the relationships</p> <p>In this way, amundsendatabuilder pipelines can create python objects that can then be loaded into neo4j / elastic search without developers needing to know the internals of the neo4j schema. </p>"},{"location":"databuilder/docs/models/#the-models","title":"The Models","text":""},{"location":"databuilder/docs/models/#tablemetadata","title":"TableMetadata","text":"<p>What datasets does my org have?</p>"},{"location":"databuilder/docs/models/#description","title":"Description","text":"<p>This corresponds to a dataset in amundsen and is the core building block. In addition to ColumnMetadata, tableMetadata is one of the first datasets you should extract as almost everything else depends on these being populated. </p>"},{"location":"databuilder/docs/models/#extraction","title":"Extraction","text":"<p>In general, for Table and Column Metadata, you should be able to use one of the pre-made extractors in the extractor package</p>"},{"location":"databuilder/docs/models/#watermark","title":"Watermark","text":"<p>What is the earliest data that this table has? What is the latest data? This is NOT the same as when the data was last updated.</p>"},{"location":"databuilder/docs/models/#description_1","title":"Description","text":"<p>Corresponds to the earliest and latest date that a dataset has. Only makes sense if the dataset is timeseries data. For example, a given table may have data from 2019/01/01 -&gt; 2020/01/01 In that case the low watermark is 2019/01/01 and the high watermark is 2020/01/01.</p>"},{"location":"databuilder/docs/models/#extraction_1","title":"Extraction","text":"<p>Depending on the datastore of your dataset, you would extract this by: - a query on the minimum and maximum partition (hive) - a query for the minimum and maximum record of a given timestamp column</p>"},{"location":"databuilder/docs/models/#tablecolumnusage","title":"TableColumnUsage","text":""},{"location":"databuilder/docs/models/#description_2","title":"Description","text":"<p>How many queries is a given column getting? By which users?</p> <p>Has query counts per a given table per a user. This can help identify  who uses given datasets so people can contact them if they have questions on how to use a given dataset or if a dataset is changing. It is also used as a  search boost so that the most used tables are put to the top of the search results.</p> <p>This model also populates the Popular Resources section on the homepage.  A table must have at least 10 unique users to appear on the homepage.  This configuration is defined here  and can be changed if your usage is less.</p>"},{"location":"databuilder/docs/models/#extraction_2","title":"Extraction","text":"<p>For more traditional databases, there should be system tables where you can obtain  these sorts of usage statistics.</p> <p>In other cases, you may need to use audit logs which could require a custom solution.</p> <p>Finally, for non-traditional data lakes, getting this information exactly maybe difficult and you may need to rely on a heuristic.</p>"},{"location":"databuilder/docs/models/#user","title":"User","text":"<p>What users are there out there? Which team is this user on?</p>"},{"location":"databuilder/docs/models/#description_3","title":"Description","text":"<p>Represents all of the metadata for a user at your company. This is required if you are going to be having authentication turned on.</p>"},{"location":"databuilder/docs/models/#extraction_3","title":"Extraction","text":"<p>TODO</p>"},{"location":"databuilder/docs/models/#tablecolumnstats","title":"TableColumnStats","text":"<ul> <li>What are the min/max values for this column? How many nulls are in this column? *</li> </ul>"},{"location":"databuilder/docs/models/#description_4","title":"Description","text":"<p>This represents statistics on the column level (this is not for table level metrics). The idea is that different companies will want to track different things about different columns, so this is highly customizable. It also will probably require a distributed cluster in order to calculate these regularly and in general is probably the least accessible metrics to get at without a custom solution.</p>"},{"location":"databuilder/docs/models/#extraction_4","title":"Extraction","text":"<p>The idea here would be to implement something that does the following: For each table you care about: For each column you care about: Calculate statistics that you care about such as min/max/average etc.</p>"},{"location":"databuilder/docs/models/#application","title":"Application","text":"<ul> <li>What job/application is writing to this table? *</li> </ul>"},{"location":"databuilder/docs/models/#description_5","title":"Description","text":"<p>This is used to provide users a way to find out what job/application is responsible for writing to this dataset.</p>"},{"location":"databuilder/docs/models/#extraction_5","title":"Extraction","text":"<p>TODO</p>"},{"location":"databuilder/docs/models/#table-owner","title":"Table Owner","text":"<ul> <li>What team or user owns this dataset? *</li> </ul>"},{"location":"databuilder/docs/models/#description_6","title":"Description","text":"<p>A dataset can have one or more owners. These owners are used when requesting table descriptions or could be just a useful point of contact for a user inquiring about how to use a dataset.</p>"},{"location":"databuilder/docs/models/#extraction_6","title":"Extraction","text":"<p>Although the main point of entry for owners is through the WebUI, you could in theory extract this information based on who created a given table. </p>"},{"location":"databuilder/docs/models/#table-source","title":"Table Source","text":"<ul> <li>Where is the source code for the application that writes to this dataset? *</li> </ul>"},{"location":"databuilder/docs/models/#description_7","title":"Description","text":"<p>Generally there is going to be code that your company owns that describes how a dataset is created. This model is what represents the link and type of repository to this source code so it is available to users.</p>"},{"location":"databuilder/docs/models/#extraction_7","title":"Extraction","text":"<p>You will need a github/gitlab/your repository crawler in order to populate this automatically. The idea there would be to search for a given table name or something else that is a unique identifier such that you can be confident that the source correctly matches to this table.</p>"},{"location":"databuilder/docs/models/#tablelastupdated","title":"TableLastUpdated","text":"<ul> <li>When was the last time this data was updated? Is this table stale or deprecated? *</li> </ul>"},{"location":"databuilder/docs/models/#description_8","title":"Description","text":"<p>This value is used to describe the last time the table had datapoints inserted into it. It is a very useful value as it can help users identify if there are tables that are no longer being updated.</p>"},{"location":"databuilder/docs/models/#extraction_8","title":"Extraction","text":"<p>There are some extractors available for this like hive_table_last_updated_extractor that you can refer to. But you will need access to history that provides information on when the last data write happened on a given table. If this data isn\u2019t available for your data source, you maybe able to approximate it by looking at the max of some timestamp column.  </p>"},{"location":"databuilder/docs/models/#dashboard-models","title":"Dashboard models","text":"<p>Dashboard models are normalized which means that the model is separated so that it can be easily decoupled with how data is extracted. (If model is denormalized, all metadata is in model, then one extraction needs to able to pull all the data which makes extraction hard and complex) There\u2019s trade off in this decision of normalized design where it can be inefficient in the case that some ingestion can be done in one job for metadata source happen to provide all data it need. However, to make model flexible for most of metadata, it is normalized.  </p>"},{"location":"databuilder/docs/models/#dashboardmetadata","title":"DashboardMetadata","text":""},{"location":"databuilder/docs/models/#description_9","title":"Description","text":"<p>A baseline of Dashboard metadata that consists of  dashboard group name, dashboard group description, dashboard description, etc. This model needs to be ingested first as other model builds relation to this.</p>"},{"location":"databuilder/docs/models/#extraction_9","title":"Extraction","text":"<p>ModeDashboardExtractor </p>"},{"location":"databuilder/docs/models/#dashboardowner","title":"DashboardOwner","text":""},{"location":"databuilder/docs/models/#description_10","title":"Description","text":"<p>A model that encapsulate Dashboard\u2019s owner. Note that it does not create new user as it has insufficient information about user but it builds relation between User and Dashboard</p>"},{"location":"databuilder/docs/models/#extraction_10","title":"Extraction","text":"<p>ModeDashboardOwnerExtractor </p>"},{"location":"databuilder/docs/models/#dashboardtable","title":"DashboardTable","text":"<p>A model that link Dashboard with the tables used in various charts of the dashboard. Note that it does not create new dashboard, table as it has insufficient information but it builds relation between Tables and Dashboard.</p> <p>Supporting extractor: Currently there\u2019s no open sourced extractor for this. In Lyft, there\u2019s audit table that records SQL query, where it came from with identifier, along with tables that is used in SQL query. We basically query this table via DBAPIExtractor</p>"},{"location":"databuilder/docs/models/#dashboardusage","title":"DashboardUsage","text":""},{"location":"databuilder/docs/models/#description_11","title":"Description","text":"<p>A model that encapsulate Dashboard usage between Dashboard and User</p>"},{"location":"databuilder/docs/models/#extraction_11","title":"Extraction","text":"<p>You can use ModeDashboardUsageExtractor . However, currently Mode only provides accumulated view count where we need recent view counts (past 30, 60, or 90 days). To get recent view count, in Lyft, we use ModeDashboardUsageExtractor to extract accumulated view count and GenericLoader to load its record (no publisher here and publisher is not mandatory in DefaultJob) as a event where event materialized as daily snapshot. Once it captures daily accumulated view count, ingest recent view count by querying the datastore. In Lyft, we query via DBAPIExtractor through Presto.  </p>"},{"location":"databuilder/docs/models/#dashboardlastmodifiedtimestamp","title":"DashboardLastModifiedTimestamp","text":""},{"location":"databuilder/docs/models/#description_12","title":"Description","text":"<p>A model that encapsulate Dashboard\u2019s last modified timestamp in epoch</p>"},{"location":"databuilder/docs/models/#extraction_12","title":"Extraction","text":"<p>ModeDashboardLastModifiedTimestampExtractor </p>"},{"location":"databuilder/docs/models/#dashboardexecution","title":"DashboardExecution","text":"<p>A model that encapsulate Dashboard\u2019s execution timestamp in epoch and execution state. Note that this model supports last_execution and last_successful_execution by using different identifier in the URI.</p>"},{"location":"databuilder/docs/models/#extraction_13","title":"Extraction","text":"<p>ModeDashboardExecutionsExtractor which extracts last_execution.</p> <p>ModeDashboardLastSuccessfulExecutionExtractor </p>"},{"location":"databuilder/docs/models/#dashboardquery","title":"DashboardQuery","text":""},{"location":"databuilder/docs/models/#description_13","title":"Description","text":"<p>A model that encapsulate Dashboard\u2019s query information.</p> <p>Supporting extractor: ModeDashboardQueriesExtractor</p>"},{"location":"databuilder/docs/models/#dashboardchart","title":"DashboardChart","text":""},{"location":"databuilder/docs/models/#description_14","title":"Description","text":"<p>A model that encapsulate Dashboard\u2019s charts where chart is associated with query.</p>"},{"location":"databuilder/docs/models/#extraction_14","title":"Extraction","text":"<p>ModeDashboardChartsExtractor </p>"},{"location":"databuilder/docs/models/#feature-models","title":"Feature models","text":"<p>Feature models include FeatureMetadata, which encapsulates the basic feature details,  and supplemental models Feature_Generation_Code and  Feature_Watermark for adding extra metadata.  In addition, the Tag, Badge, Owner, and Programmatic_Description models work with features.</p>"},{"location":"databuilder/docs/models/#featuremetadata","title":"FeatureMetadata","text":""},{"location":"databuilder/docs/models/#description_15","title":"Description","text":"<p>A baseline of Feature metadata. This model needs to be ingested first as other models build relations to it.</p>"},{"location":"databuilder/docs/models/#extraction_15","title":"Extraction","text":"<p>No specific extractors are provided at this time. We expect users will either write custom extractors,  or use generic extractors (e.g. SQLAlchemyExtractor). </p>"},{"location":"databuilder/docs/models/#feature_generation_code","title":"Feature_Generation_Code","text":""},{"location":"databuilder/docs/models/#description_16","title":"Description","text":"<p>Allows ingesting the text of the generation code (SQL or otherwise) which was used to create a feature.  </p>"},{"location":"databuilder/docs/models/#extraction_16","title":"Extraction","text":"<p>No specific extractors are provided at this time. We expect users will either write custom extractors,  or use generic extractors (e.g. SQLAlchemyExtractor). </p>"},{"location":"databuilder/docs/models/#feature_watermark","title":"Feature_Watermark","text":""},{"location":"databuilder/docs/models/#description_17","title":"Description","text":"<p>Allows ingesting the high and low data range of a feature. Unlike Watermark,  which is specific to tables (requires a partition, for example), Feature_Watermark is more general and does not  care about how the feature is stored.</p>"},{"location":"databuilder/docs/models/#extraction_17","title":"Extraction","text":"<p>No specific extractors are provided at this time. We expect users will either write custom extractors,  or use generic extractors (e.g. SQLAlchemyExtractor).</p>"},{"location":"databuilder/docs/query_metadata_guide/","title":"Query Metadata Guide","text":"<p>This document provides guidance on how to ingest query metadata and query composition metadata into Amundsen.</p> <p>Query metadata and query composition metadata consists of four parts:</p> <ol> <li><code>QueryMetadata</code>: This represents a query</li> <li><code>QueryExecutionsMetadata</code>: This is an aggregation, representing the number of times a given query was executed wtihin an hour, day, week, etc.</li> <li><code>QueryJoinMetadata</code>: Represents a join between two columns</li> <li><code>QueryWhereMetadata</code>: Represents a whereclause used in a query, this may be associated to one or more columns and tables</li> </ol> <p>The <code>QueryExecutionsMetadata</code>, <code>QueryJoinMetadata</code>, <code>QueryWhereMetadata</code> and <code>QueryMetadata</code> can be seen here:</p> <p></p> <p>Amundsen uses <code>QueryExecutionsMetadata</code> to determine time-sensitive relevance. As new <code>QueryExecutionsMetadata</code> are added and old ones are removed, Amundsen is able to continue to keep the most recent queries and the related joins and wheres relevant.</p> <p>Since <code>QueryMetadata</code> can be related to <code>QueryExecutionsMetadata</code>, <code>QueryJoinMetadata</code> and <code>QueryWhereMetadata</code>, a single extractor is used to ingest these objects into Amundsen.</p>"},{"location":"frontend/","title":"Amundsen Frontend Service","text":"<p>Amundsen is a metadata driven application for improving the productivity of data analysts, data scientists and engineers when interacting with data. It does that today by indexing data resources (tables, dashboards, streams, etc.) and powering a page-rank style search based on usage patterns (e.g. highly queried tables show up earlier than less queried tables). Think of it as Google search for data. The project is named after Norwegian explorer Roald Amundsen, the first person to discover South Pole.</p> <p>The frontend service leverages a separate search service for allowing users to search for data resources, and a separate metadata service for viewing and editing metadata for a given resource. It is a Flask application with a React frontend.</p> <p>For information about Amundsen and our other services, refer to this README.md. Please also see our instructions for a quick start setup  of Amundsen with dummy data, and an overview of the architecture.</p>"},{"location":"frontend/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>Node = v12</li> <li>npm &gt;= 6.x.x</li> </ul>"},{"location":"frontend/#homepage","title":"Homepage","text":"<ul> <li>https://www.amundsen.io/</li> </ul>"},{"location":"frontend/#documentation","title":"Documentation","text":"<ul> <li>https://www.amundsen.io/amundsen/</li> </ul>"},{"location":"frontend/#user-interface","title":"User Interface","text":"<p>Please note that the mock images only served as demonstration purpose.</p> Landing Page: The landing page for Amundsen including 1. search bars; 2. popular used tables Search Preview: See inline search results as you type Table Detail Page: Visualization of a Hive / Redshift table Column detail: Visualization of columns of a Hive / Redshift table which includes an optional stats display Data Preview Page: Visualization of table data preview which could integrate with Apache Superset"},{"location":"frontend/#next-steps","title":"Next Steps","text":""},{"location":"frontend/#install","title":"Install","text":"<p>Please visit Installation guideline on how to install Amundsen.</p>"},{"location":"frontend/#configure","title":"Configure","text":"<p>Please visit Configuration doc on how to configure Amundsen various enviroment settings.</p>"},{"location":"frontend/#developer-guidelines","title":"Developer Guidelines","text":"<p>Please visit Developer guidelines if you want to build Amundsen in your local environment.</p>"},{"location":"frontend/#contribute","title":"Contribute","text":"<p>Check our Contribution Guide to get you started contributing to Amundsen. Read our Frontend Strategy doc to see where can you help today.</p>"},{"location":"frontend/CHANGELOG/","title":"CHANGELOG","text":""},{"location":"frontend/CHANGELOG/#feature","title":"Feature","text":"<ul> <li>Table and Column Lineage Polish (#970) (<code>cd2f4c4</code>)</li> <li>Table and Column Lineage Lists (#969) (<code>df9532a</code>)</li> <li>Add Table Notices (#957) (<code>e3be638</code>)</li> <li>Allows for splitting stats\u2019 distinct values into a different element that shows in modal (#960) (<code>fe04a06</code>)</li> </ul>"},{"location":"frontend/CHANGELOG/#fix","title":"Fix","text":"<ul> <li>Upgrade mypy version to build with Python3.8 (#975) (<code>18963ec</code>)</li> <li>Handles parsing errors when format not expected on distinct values (#966) (<code>473bbdb</code>)</li> <li>Made commit author consistent (#917) (<code>48441cd</code>)</li> <li>Yaml syntax error (#913) (<code>8f49627</code>)</li> <li>Add chore to monthly release PRs (#912) (<code>9323862</code>)</li> <li>Removed echo for changelog command (#910) (<code>bb22d4d</code>)</li> <li>Add changelog file (#907) (<code>f06c50e</code>)</li> <li>Made change to preserve format of changelog (#896) (<code>0d56d72</code>)</li> <li>Fixed reviewers field syntax error (#892) (<code>b7f99d4</code>)</li> <li>Made branch eval and added reviewers (#891) (<code>dd57d44</code>)</li> <li>Changed release workflow completely (#882) (<code>5dfcd09</code>)</li> <li>Index tag info into elasticsearch immediately after ui change (#883) (<code>b34151c</code>)</li> </ul>"},{"location":"frontend/docs/application_config/","title":"Application configuration","text":"<p>This document describes how to leverage the frontend service\u2019s application configuration to configure particular features. After modifying the <code>AppConfigCustom</code> object in config-custom.ts in the ways described in this document, be sure to rebuild your application. All default config values are set in the config-default.ts file.</p>"},{"location":"frontend/docs/application_config/#analytics","title":"Analytics","text":"<p>Amundsen supports pluggable user behavior analytics via the analytics library.</p> <p>To emit analytics to a given destination, you must use one of the provided plugins (open a PR if you need to install a different vendor), then specify it the config passing the configuration of your account. Multiple destinations are supported if you wish to emit to multiple backends simultaneously.</p> <p>We provide out of the box support for Mixpanel, Segment and Google Analytics. All <code>@analytics/</code> plugins are potentially supported, but you must first install the plugin: <code>npm install @analytics/&lt;provider&gt;</code> and send us a PR with it before you can use it.</p>"},{"location":"frontend/docs/application_config/#examples","title":"Examples","text":"<p>For example, to use Google analytics, you must add the import at the top of your <code>config-custom.ts</code> file: <code>import googleAnalytics from '@analytics/google-analytics';</code>, then add this config block:</p> <pre><code>analytics: {\n  plugins: [\n    googleAnalytics({\n      trackingId: '&lt;YOUR_UA_CODE&gt;',\n      sampleRate: 100\n    }),\n  ],\n}\n</code></pre>"},{"location":"frontend/docs/application_config/#announcements","title":"Announcements","text":"<p>Announcements is a feature that allows to disclose new features, changes or any other news to Amundsen\u2019s users using a panel in the homepage.</p> Announcements in the homepage <p>To enable this feature, change the <code>announcements.enabled</code> boolean value by overriding it on config-custom.ts. Once activated, an \u201cAnnouncements\u201d link will be available in the global navigation, and a new list of announcements will show up on the right sidebar on the Homepage.</p> <p>Refer to announcement_client.md for information about fetching announcements.</p>"},{"location":"frontend/docs/application_config/#examples_1","title":"Examples","text":"<p>To turn announcements on, change the flag as below: <pre><code>announcements: {\n  enabled: true,\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#badges","title":"Badges","text":"<p>Badges are a special type of tag that cannot be edited through the UI.</p> <p>The <code>BadgeStyleConfig</code> type can be used to customize the text and color of badges. This config defines a mapping of badge name to a <code>BadgeStyle</code> and optional <code>displayName</code>. Badges that are not defined will default to use the <code>BadgeStyle.default</code> style and <code>displayName</code> use the badge name with any <code>_</code> or <code>-</code> characters replaced with a space.</p>"},{"location":"frontend/docs/application_config/#examples_2","title":"Examples","text":"<p>Check below to see how to set two badges for two generic \u2018alpha\u2019 and \u2018beta\u2019 badges: <pre><code>alpha: {\n  style: BadgeStyle.DEFAULT,\n  displayName: \"Alpha\",\n},\nbeta: {\n  style: BadgeStyle.DEFAULT,\n  displayName: \"Beta\",\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#browse","title":"Browse","text":"<p>The browse page options are defined in the <code>BrowseConfig</code> type. Read below for a breakdown of them.</p>"},{"location":"frontend/docs/application_config/#curated-tags","title":"Curated Tags","text":"<p>The Curated tags list is an array of tags to show in a separate section at the top of the browser page. It is an empty array by default.</p>"},{"location":"frontend/docs/application_config/#examples_3","title":"Examples","text":"<p>Here is how you would add curated tags to the top of the Browse page: <pre><code>browse: {\n  curatedTags: ['tag1', 'tag2', 'tag3'],\n  //...\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#hide-non-clickable-badges","title":"Hide Non-Clickable Badges","text":"<p>The <code>BrowseConfig.hideNonClickableBadges</code> hides non-clickable badges in the homepage if <code>true</code>. By default is <code>false</code>.</p>"},{"location":"frontend/docs/application_config/#examples_4","title":"Examples","text":"<p>This is how you turn it to true: <pre><code>browse: {\n  hideNonClickableBadges: true,\n  //...\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#show-all-tags","title":"Show All Tags","text":"<p>The <code>BrowseConfig.showAllTags</code> flag allows us to configure whether we should show all the tags or only the curated ones. The default is <code>true</code>.</p>"},{"location":"frontend/docs/application_config/#examples_5","title":"Examples","text":"<p>This option shows all tags when true, or only curated tags if false: <pre><code>browse: {\n  showAllTags: false,\n  //...\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#show-badges-in-homepage","title":"Show Badges in homepage","text":"<p>By default, all available badges are shown on the homepage. The <code>browse.showBadgesInHome</code> configuration can be set to <code>false</code> to disable this. In addition, it is possible to hide the \u201cnon-clickable\u201d badges using <code>browse.hideNonClickableBadges</code> configuration.</p>"},{"location":"frontend/docs/application_config/#examples_6","title":"Examples","text":"<p>Here is how you would remove the badges list from the homepage <pre><code>browse: {\n  //...\n  showBadgesInHome: false,\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#column-lineage","title":"Column Lineage","text":"<p>This options allows you to configure column level lineage features in Amundsen.</p> <p>It includes:  * inAppListEnabled - whether the in-app column list lineage is enabled.  * inAppPageEnabled - whether the in-app column lineage page is enabled.  * urlGenerator - the lineage link for a given column</p>"},{"location":"frontend/docs/application_config/#examples_7","title":"Examples","text":"<p>Set these values to see column lineage: <pre><code>  columnLineage: {\n    inAppListEnabled: true,\n    inAppPageEnabled: true,\n    urlGenerator: (\n      database: string,\n      cluster: string,\n      schema: string,\n      table: string,\n      column: string\n    ) =&gt; {\n      // Some code here\n\n      return `https://DEFAULT_LINEAGE_URL?schema=${schema}&amp;cluster=${cluster}&amp;db=${database}&amp;table=${table}&amp;column=${column}`;\n    }\n  },\n</code></pre></p>"},{"location":"frontend/docs/application_config/#date","title":"Date","text":"<p>This config allows you to specify various date formats using moment.js across the app. There are three date formats in use shown below. These correspond to the <code>formatDate</code>, <code>formatDateTimeShort</code> and <code>formatDateTimeLong</code> utility functions, with the defaults shown below:</p> <pre><code>date: {\n  default: 'MMM DD, YYYY',\n  dateTimeShort: 'MMM DD, YYYY ha z',\n  dateTimeLong: 'MMMM Do YYYY [at] h:mm:ss a',\n}\n</code></pre> <p>Read here the reference for formatting.</p>"},{"location":"frontend/docs/application_config/#examples_8","title":"Examples","text":"<p>Set these values to get different formats: <pre><code>date: {\n  default: 'YYYY-MM-DD',\n  dateTimeShort: 'DD. MMM. YYYY hh:mm',\n  dateTimeLong: 'DD. MMM. YYYY hh:mm:ss',\n}\n</code></pre></p>"},{"location":"frontend/docs/application_config/#document-title","title":"Document Title","text":"<p>This configuration string specifies the root of the application title. By default this is \u2018Amundsen - Data Discovery Portal\u2019.</p>"},{"location":"frontend/docs/application_config/#examples_9","title":"Examples","text":"<p>You can set this value with your company name: <pre><code>documentTitle: 'ACME - Amundsen - Data Discovery Portal',\n</code></pre></p>"},{"location":"frontend/docs/application_config/#editable-text","title":"Editable Text","text":"<p>The <code>EditableTextConfig</code> configuration object allows us to configure the maximum length limits for editable fields.</p> <p>With it, we configure the max length for table and column descriptions, with defaults being: <pre><code>editableText: {\n  columnDescLength: 250,\n  tableDescLength: 750,\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#examples_10","title":"Examples","text":"<p>To change these values, simply update the character lengths: <pre><code>editableText: {\n  columnDescLength: 100,\n  tableDescLength: 500,\n},\n</code></pre></p>"},{"location":"frontend/docs/application_config/#feature-lineage","title":"Feature Lineage","text":"<p>This option allows you to configure the upstream lineage tab for features.</p>"},{"location":"frontend/docs/application_config/#examples_11","title":"Examples","text":"<p>Set this value to see feature lineage: <pre><code>  featureLineage: {\n    inAppListEnabled: true,\n  },\n</code></pre></p>"},{"location":"frontend/docs/application_config/#homepage-widgets","title":"Homepage Widgets","text":"<p>By default, a set of features are available on the homepage (e.g. the search bar, bookmarks). These can be customized in config-custom.ts by providing an alternate <code>homePageWidgets</code> value. The value is a list of <code>Widget</code> objects. Non-OSS widgets can be provided in the <code>widget.options.path</code> property, and props passed to widget components can be customized with the <code>widget.options.aditionalProps</code> property.</p> <p>If a custom <code>homePageWidgets</code> config is provided, the default config will be ignored. So, for example, if you wanted to have all the default widgets plus a custom non-OSS widget component, you should copy all the homePageWidgets from config-default.ts to your config-custom.ts, and then append your custom component. To omit one of the default widgets, you would copy the default list, and then delete the widget you didn\u2019t want.</p>"},{"location":"frontend/docs/application_config/#examples_12","title":"Examples","text":"<p>For example, if we wanted to place the Bookmarks widget at the top of the homepage, we could do: <pre><code>//...\nhomePageWidgets: {\n    widgets: [\n      {\n        name: \"MyBookmarksWidget\",\n        options: {\n          path: \"MyBookmarksWidget/index\",\n        },\n      },\n      {\n        name: \"SearchBarWidget\",\n        options: {\n          path: \"SearchBarWidget/index\",\n        },\n      },\n      {\n        name: \"BadgesWidget\",\n        options: {\n          path: \"BadgesWidget/index\",\n          additionalProps: {\n            shortBadgesList: true,\n          },\n        },\n      },\n      {\n        name: \"TagsWidget\",\n        options: {\n          path: \"TagsWidget/index\",\n          additionalProps: {\n            shortTagsList: true,\n          },\n        },\n      },\n    ],\n  },\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#indexing-optional-resources","title":"Indexing Optional Resources","text":"<p>In Amundsen, we currently support indexing other optional resources beyond tables.</p>"},{"location":"frontend/docs/application_config/#index-users","title":"Index Users","text":"<p>Users themselves are data resources and user metadata helps to facilitate network based discovery. When users are indexed they will show up in search results, and selecting a user surfaces a profile page that displays that user\u2019s relationships with different data resources.</p> <p>After ingesting user metadata into the search and metadata services, set <code>IndexUsersConfig.enabled</code> to <code>true</code> on the application configuration to display the UI for the aforementioned features.</p>"},{"location":"frontend/docs/application_config/#index-dashboards","title":"Index Dashboards","text":"<p>Introducing dashboards into Amundsen allows users to discovery data analysis that has been already done. When dashboards are indexed they will show up in search results, and selecting a dashboard surfaces a page where users can explore dashboard metadata.</p> <p>After ingesting dashboard metadata into the search and metadata services, set <code>IndexDashboardsConfig.enabled</code> to <code>true</code> on the application configuration to display the UI for the aforementioned features.</p>"},{"location":"frontend/docs/application_config/#index-features","title":"Index Features","text":"<p>When this configuration is enabled, ML features will be avaialable as searchable resources. This requires feature objects to be ingested via Databuilder and made available in the metadata and serch services.</p>"},{"location":"frontend/docs/application_config/#examples_13","title":"Examples","text":"<p>This enables this feature in the frontend only: <pre><code>//...\nindexFeatures: {\n  enabled: true,\n},\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#issue-tracking","title":"Issue Tracking","text":"<p>In order to enable Issue Tracking, set <code>IssueTrackingConfig.enabled</code> to <code>true</code> to see UI features. Further configuration is required to fully enable the feature, please see this entry.</p> <p>To prepopulate the issue description text field with a template to suggest more detailed information to be provided by the user when an issue is reported, set <code>IssueTrackingConfig.issueDescriptionTemplate</code> with the desired string.</p> <p>A default project ID to specify where issues will be created is set in the flask configuration, but to allow users to override this value and choose which project their issue is created in, set <code>IssueTrackingConfig.projectSelection.enabled</code> to <code>true</code>. This will add an extra input field in the <code>Report an issue</code> modal that will accept a Jira project key, but if no input is entered, it will use the value that is set in the flask configuration. This feature is currently only implemented for use with Jira issue tracking.</p> <ul> <li>Set <code>IssueTrackingConfig.projectSelection.title</code> to add a title to the input field, for example <code>Jira project key (optional)</code>, to let users know what to enter in the text field.</li> <li>An optional config <code>IssueTrackingConfig.projectSelection.inputHint</code> can be set to show a hint in the input field, which can be helpful to show users an example that conveys the expected format of the project key.</li> </ul>"},{"location":"frontend/docs/application_config/#examples_14","title":"Examples","text":"<p>This is an example of configuration for issue tracking with JIRA: <pre><code>//...\nissueTracking: {\n  enabled: true,\n  issueDescriptionTemplate:\n    \"Affected column(s): \\nProducing DAG, if known: \\nFurther details: \\n\",\n  projectSelection: {\n    enabled: true,\n    title: \"Jira project key (optional)\",\n    inputHint: \"HELP\",\n  },\n},\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#custom-logo","title":"Custom Logo","text":"<p>We can configure the application to show a custom image on the logo instead of the default Amundsen logo. For that, you would:</p> <ol> <li>Add your logo to the folder in <code>amundsen_application/static/images/</code>.</li> <li>Set the the <code>logoPath</code> key on the to the location of your image.</li> </ol>"},{"location":"frontend/docs/application_config/#examples_15","title":"Examples","text":"<p>To add a custom logo, set this up: <pre><code>logoPath: \"/static/images/custom-logo.svg\",\n</code></pre> So you would see something like this: </p>"},{"location":"frontend/docs/application_config/#custom-title","title":"Custom Title","text":"<p>We can also set a custom title for the application (the default is \u2018Amundsen\u2019). For that, we would use the \u2018logoTitle\u2019 configuration.</p>"},{"location":"frontend/docs/application_config/#examples_16","title":"Examples","text":"<p>To add a custom title, set this up: <pre><code>logoTitle: \"Your Custom App Name\",\n</code></pre></p>"},{"location":"frontend/docs/application_config/#mail-client-features","title":"Mail Client Features","text":"<p>Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications.</p> <p>As these are optional features, our <code>MailClientFeaturesConfig</code> can be used to hide/display any UI related to these features:</p> <ol> <li>Set <code>MailClientFeaturesConfig.feedbackEnabled</code> to <code>true</code> in order to display the <code>Feedback</code> component in the UI.</li> <li>Set <code>MailClientFeaturesConfig.notificationsEnabled</code> to <code>true</code> in order to display the optional UI for users to request more information about resources on the <code>TableDetail</code> page.</li> </ol> <p>For information about how to configure a custom mail client, please see this entry in our flask configuration doc.</p>"},{"location":"frontend/docs/application_config/#navigation-app-suite","title":"Navigation App Suite","text":"<p>This configuration allows to show a popover menu with related application links. This is hidden by default, and only will show up if you pass an array of links to it.</p>"},{"location":"frontend/docs/application_config/#examples_17","title":"Examples","text":"<p>Here is how you would set a list of links: <pre><code>//...\nnavAppSuite: [\n  {\n    label: 'App One',\n    id: 'appOne',\n    href: 'https://www.lyft.com',\n    target: '_blank',\n    iconPath: '/static/images/app-one-logo.svg',\n  },\n  {\n    label: 'App Two',\n    id: 'appTwo',\n    href: 'https://www.amundsen.io/',\n    iconPath: '/static/images/app-two-logo.svg',\n  },\n  //...\n],\n//...\n</code></pre> Which would render as: </p>"},{"location":"frontend/docs/application_config/#navigation-links","title":"Navigation Links","text":"<p>This configuration option allows you to customize the Navigation links at the top right side of the global header:</p> <p></p>"},{"location":"frontend/docs/application_config/#examples_18","title":"Examples","text":"<p>Here is how you would set them in the configuration: <pre><code>//...\nnavLinks: [\n    {\n      href: \"/announcements\",\n      id: \"nav::announcements\",\n      label: \"Announcements\",\n      use_router: true,\n    },\n    {\n      href: \"https://external.link.com\",\n      id: \"nav::docs\",\n      label: \"Docs\",\n      target: \"_blank\",\n      use_router: false,\n    },\n  ],\n//...\n</code></pre> Note how we can add internal links (with \u2018use_router\u2019 true) or external links with \u2018target\u2019 set to _blank so they open on a new tab.</p>"},{"location":"frontend/docs/application_config/#navigation-theme","title":"Navigation Theme","text":"<p>This configuration allows users to select a navigation theme for the global header of the application. This is \u2018dark\u2019 by default:</p> <p></p>"},{"location":"frontend/docs/application_config/#examples_19","title":"Examples","text":"<p>Here is how you would set it to the \u2018light\u2019 theme: <pre><code>//...\nnavTheme: 'light',\n//...\n</code></pre></p> <p>Which would render like the following: </p>"},{"location":"frontend/docs/application_config/#nested-columns","title":"Nested Columns","text":"<p>Nested columns will be enabled in the frontend by default if complex column types are parsed and ingested using the ComplexTypeTransformer.</p> <p>To expand all nested column type rows by default if the total number of rows does not exceed a specific value, set <code>nestedColumns.maxNestedColumns</code> to the desired limit. The default value is set to 500 to avoid an unbounded expansion.</p>"},{"location":"frontend/docs/application_config/#examples_20","title":"Examples","text":"<p>Simply set the maximum nested columns allowed in the UI: <pre><code>//...\n  nestedColumns: {\n    maxNestedColumns: 1000,\n  },\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#number-format","title":"Number Format","text":"<p>This configuration allows us to format different types of numbers like currency, and percentages in the desired format. Internally, it applies the first argument for Intl.NumberFormat, so you can check the options there.</p>"},{"location":"frontend/docs/application_config/#examples_21","title":"Examples","text":"<pre><code>//...\nnumberFormat: {\n  numberSystem: 'jap-JP'\n},\n//...\n</code></pre>"},{"location":"frontend/docs/application_config/#product-tour","title":"Product Tour","text":"<p>The Product Tour for Amundsen is a UI based walkthrough configurable component that helps onboard users into Amundsen. Alternatively, it helps us promote new features added to Amundsen, and educate our users about its use.</p> <p>The Tour triggers in two different modes. The first is a page tour, like a general \u201cGetting started with Amundsen\u201d walkthough, while the second highlights different features. Both would be formed by an overlay and a modal that is attached to elements in the UI.</p> <p>This modal window has a \u201cDimiss\u201d button that would hide the Tour altogether; a \u201cBack\u201d button that would move the user to the previous tour step, a \u201cNext\u201d button that moves it forward and a \u201cClose\u201d button with the usual \u201cX\u201d shape in the top right corner.</p> <p>For Amundsen maintainers, we extend the JavaScript configuration file with a block about the tour. This object has a shape like this when creating a \u201cpage tour\u201d:</p>"},{"location":"frontend/docs/application_config/#examples_22","title":"Examples","text":"<pre><code>...\nproductTour: {\n  '/': [\n    {\n      isFeatureTour: false,\n      isShownOnFirstVisit: true,\n      isShownProgrammatically: true,\n      steps: [\n        {\n          target: '.nav-bar-left a',\n          title: 'Welcome to Amundsen',\n          content:\n            'Hi!, welcome to Amundsen, your data discovery and catalog product!',\n          disableBeacon: true,\n        },\n        {\n          target: '.search-bar-form .search-bar-input',\n          title: 'Search for resources',\n          content:\n            'Here you will search for the resources you are looking for',\n        },\n        {\n          target: '.bookmark-list-header',\n          title: 'Save your bookmarks',\n          content:\n            'Here you will see a list of the resources you have bookmarked',\n        },\n      ],\n    },\n  ],\n},\n</code></pre> <p>Where:</p> <ul> <li>The keys of the productTour object are the paths to the pages with a tour. They support simple wildcards <code>*</code>, only at the end (for example: <code>/table_detail/*</code>).</li> <li><code>isFeatureTour</code> - tells if the tour is for a whole page (false) or just for one feature within the page.</li> <li><code>isShownOnFirstVisit</code> - whether the users will see the tour on their first visit.</li> <li><code>isShownProgrammatically</code> - whether we want to add the button to trigger the tour to the global navigation</li> <li><code>steps</code> - a list of CSS selectors to point the tour highlight, a title of the step and the content (text only). <code>disableBeacon</code> controls whether if we show a purple beacon to guide the users to the initial step of the tour.</li> </ul> <p>For \u201cfeature tours\u201d, the setup would be similar, but <code>isFeatureTour</code> would be true, and <code>disableBeacon</code> should be false (the default), so that users can start the tour.</p>"},{"location":"frontend/docs/application_config/#resource-configurations","title":"Resource Configurations","text":"<p>This configuration drives resource specific aspects of the application\u2019s user interface. Each supported resource should be mapped to an object that matches or extends the <code>BaseResourceConfig</code>.</p>"},{"location":"frontend/docs/application_config/#base-configuration","title":"Base Configuration","text":"<p>All resource configurations must match or extend the <code>BaseResourceConfig</code>. This configuration supports the following options:</p> <ol> <li><code>displayName</code>: The name displayed throughout the application to refer to this resource type.</li> <li><code>filterCategories</code>: An optional <code>FilterConfig</code> object. When set for a given resource, that resource will display filter options in the search page UI.</li> <li><code>supportedSources</code>: An optional <code>SourcesConfig</code> object.</li> </ol>"},{"location":"frontend/docs/application_config/#filter-categories","title":"Filter Categories","text":"<p>The <code>FilterConfig</code> is an array of objects that match any of the supported filter options. We currently support a <code>CheckboxFilterCategory</code>, <code>InputFilterCategory</code> and a <code>ToggleFilterCategory</code>. See our config-types for more information about each option.</p>"},{"location":"frontend/docs/application_config/#supported-sources","title":"Supported Sources","text":"<p>The <code>SourcesConfig</code> can be used for the customizations detailed below. See examples in config-default.ts.</p>"},{"location":"frontend/docs/application_config/#custom-icons","title":"Custom Icons","text":"<p>You can configure custom icons to be used throughout the UI when representing entities from particular sources. On the <code>supportedSources</code> object, add an entry with the <code>id</code> used to reference that source and map to an object that specifies the <code>iconClass</code> for that database. This <code>iconClass</code> should be defined in icons.scss.</p>"},{"location":"frontend/docs/application_config/#display-names","title":"Display Names","text":"<p>You can configure a specific display name to be used throughout the UI when representing entities from particular sources. On the <code>supportedSources</code> object, add an entry with the <code>id</code> used to reference that source and map to an object that specified the <code>displayName</code> for that source.</p>"},{"location":"frontend/docs/application_config/#table-configuration","title":"Table Configuration","text":"<p>To configure Table related features we have created a new resource configuration <code>TableResourceConfig</code> which extends <code>BaseResourceConfig</code>. In addition to the configurations explained above it also supports <code>supportedDescriptionSources</code>.</p>"},{"location":"frontend/docs/application_config/#supported-description-sources","title":"Supported Description Sources","text":"<p>A table resource may have a source of table and column description attached to it. We can customize it by using <code>supportedDescriptionSources</code> object which is an optional object. This object has <code>displayName</code> and <code>iconPath</code>, which can be used throughout the UI to represent a particular description source. See example in config-default.ts. For configuring new description sources, add an entry in <code>supportedDescriptionSources</code> with the <code>id</code> used to reference that source and add desired display name and icon for it.</p>"},{"location":"frontend/docs/application_config/#table-stats","title":"Table Stats","text":"<p>If you have a stat field that is made of a JSON like set of value names and counts, you can show that as a set of \u201cunique values\u201d. You can see an example of this in the following figure:</p> <p></p> <p>To achieve this, you will need to modify your custom configuration (config-custom.ts) by adding the name of the stat_type field that holds these values. You can find the config property in the stats section for table resource:</p> <pre><code>[ResourceType.table]: {\n  //...\n  stats: {\n    uniqueValueTypeName: \"keyNameExample\",\n  },\n}\n</code></pre> <p>The unique values set needs to be an object like this:</p> <pre><code>    {\n      end_epoch: 1609522182,\n      start_epoch: 1608917382,\n      stat_type: 'keyNameExample',\n      stat_val:\n        \"{'Category': 66, 'AnotherCategory': 54, 'More': 48}\",\n    },\n</code></pre>"},{"location":"frontend/docs/application_config/#notices","title":"Notices","text":"<p>We now can add notices to tables and dashboards. These notices allows Amundsen administrators to show informational, warning and alert messages related to the different resources (tables, dashboards, eventually people) we expose in Amundsen.</p> <p>This feature help administrators show messages related to deprecation, updates (or lack of), and informational messages related to specific resources.</p> <p>A notice is a small box with an icon and a message containing HTML markup (like links and bolded text). These will come in three flavors:</p> Informational: Marked with a blue \"i\" icon on the right side Warning: Marked with an orange exclamation mark icon on the right side Alert: Marked with a red exclamation mark icon on the right side <p>To set them up, we\u2019ll use the current configuration objects for the resources. In the event that we want to add the same notice to every table that follows a particular pattern, we use a wildcard character, *, for pattern matching. In addition, we can have dynamic HTML messages to allow for notices to change their message based on what table it is.</p> <p>For example, if company X wants to deprecate the use of one table or dashboard, they can opt to add new notices in their configuration file:</p> <pre><code>  resourceConfig: {\n    [ResourceType.table]: {\n      ... //Table Resource Configuration\n      notices: {\n          \"&lt;CLUSTER&gt;.&lt;DATABASE&gt;.&lt;SCHEMA&gt;.&lt;TABLENAME&gt;\": {\n            severity: NoticeSeverity.ALERT,\n            messageHtml: `This table is deprecated, please use &lt;a href=\"&lt;LINKTONEWTABLEDETAILPAGE&gt;\"&gt;this new table&lt;/a&gt; instead.`,\n          },\n      },\n    },\n    [ResourceType.dashboard]: {\n      ... //Dashboard Resource Configuration\n      notices: {\n          \"&lt;PRODUCT&gt;.&lt;CLUSTER&gt;.&lt;GROUPNAME&gt;.&lt;DASHBOARDNAME&gt;\": {\n            severity: NoticeSeverity.WARNING,\n            messageHtml: `This dashboard is deprecated, please use &lt;a href=\"&lt;LINKTONEWDASHBOARDDETAILPAGE&gt;\"&gt;this new dashboard&lt;/a&gt; instead.`,\n          },\n      },\n    },\n\n  },\n</code></pre> <p>The above code will show a notice with a red exclamation icon whenever a final user visits the table\u2019s Table Detail page or the Dashboard Detail page.</p> <p>If you want to target several tables at once, you can use wildcards as shown below:</p> <pre><code>  resourceConfig: {\n    [ResourceType.table]: {\n      ... //Table Resource Configuration\n      notices: {\n          \"&lt;CLUSTER&gt;.&lt;DATABASE&gt;.&lt;SCHEMA&gt;.*\": {\n            severity: NoticeSeverity.ALERT,\n            messageHtml: `This table is deprecated`,\n          },\n      },\n    },\n    [ResourceType.dashboard]: {\n      ... //Dashboard Resource Configuration\n      notices: {\n          \"&lt;PRODUCT&gt;.&lt;CLUSTER&gt;.&lt;GROUPNAME&gt;.*\": {\n            severity: NoticeSeverity.WARNING,\n            messageHtml: `This dashboard is deprecated`,\n          },\n      },\n    },\n\n  },\n</code></pre> <p>The above code will show a notice with a red exclamation icon whenever a final user visits any table within the specified cluster, database, and schema or any dashboard within the specified product, cluster, and groupname.</p> <p>Wildcards can also replace individual parts of table names. If you want to add a notice to all resources whose names followed the pattern foo_*:</p> <pre><code>  resourceConfig: {\n    [ResourceType.table]: {\n      ... //Table Resource Configuration\n      notices: {\n          \"&lt;CLUSTER&gt;.&lt;DATABASE&gt;.&lt;SCHEMA&gt;.foo_*\": {\n            severity: NoticeSeverity.INFO,\n            messageHtml: `This table has information`,\n          },\n      },\n    },\n    [ResourceType.dashboard]: {\n      ... //Dashboard Resource Configuration\n      notices: {\n          \"&lt;PRODUCT&gt;.&lt;CLUSTER&gt;.&lt;GROUPNAME&gt;.foo_*\": {\n            severity: NoticeSeverity.INFO,\n            messageHtml: `This dashboard has information`,\n          },\n      },\n    },\n\n  },\n</code></pre> <p>The above code will show the message on any table with the specified cluster, database and schema whose table name starts with <code>foo_</code> or any dashboard with the specified product, cluster, and groupname whose dashboard name starts with <code>foo_</code>.</p> <p>If you want to use a dynamic HTML message that changes depending on the name of the resource, you can use string formatting as shown below:</p> <pre><code>  resourceConfig: {\n    [ResourceType.table]: {\n      ... //Table Resource Configuration\n      notices: {\n          \"&lt;CLUSTER&gt;.&lt;DATABASE&gt;.&lt;SCHEMA&gt;.*\": {\n            severity: NoticeSeverity.ALERT,\n            messageHtml: (resourceName) =&gt; {\n              const [cluster, datasource, schema, table] = resourceName.split('.');\n              return `This schema is deprecated, please use &lt;a href=\"https://amundsen.&lt;company&gt;.net/table_detail/${cluster}/${datasource}/SCHEMA/${table}\"&gt;this table instead&lt;/a&gt;`;\n            },\n          },\n      },\n    },\n    [ResourceType.dashboard]: {\n      ... //Dashboard Resource Configuration\n      notices: {\n          \"&lt;PRODUCT&gt;.&lt;CLUSTER&gt;.&lt;GROUPNAME&gt;.*\": {\n            severity: NoticeSeverity.WARNING,\n            messageHtml: (resourceName) =&gt; {\n              const [product, cluster, groupname, dashboard] = resourceName.split('.');\n              return `${groupname} is deprecated, please use &lt;a href=\"LINKTODASHBOARD\"&gt;this dashboard instead&lt;/a&gt;`;\n            },\n          },\n      },\n    },\n\n  },\n</code></pre> <p>The above code will show a notice with a dynamic message and a red exclamation icon whenever a final user visits any table within the specified cluster, database, and schema or any dashboard within the specified product, cluster, and groupname. We can also use dynamic messages for notices without the wildcard by replacing the * with the specific table or dashboard name.</p> <p>You can also add extra information on the notices, that will be rendered as a modal. Here is a configuration example: <pre><code>  resourceConfig: {\n    [ResourceType.table]: {\n      ... //Table Resource Configuration\n      notices: {\n          \"&lt;CLUSTER&gt;.&lt;DATABASE&gt;.&lt;SCHEMA&gt;.&lt;TABLENAME&gt;\": {\n            severity: NoticeSeverity.ALERT,\n            messageHtml: `This table is deprecated, please use &lt;a href=\"&lt;LINKTONEWTABLEDETAILPAGE&gt;\"&gt;this new table&lt;/a&gt; instead.`,\n            payload: {\n              testKey: \"testValue\",\n              testKey2: 'testHTMLVAlue &lt;a href=\"http://lyft.com\"&gt;Lyft&lt;/a&gt;',\n            },\n          },\n      },\n    },\n\n  },\n</code></pre></p> <p>The above code will show a notice with a \u201cSee details\u201d link that will open a modal that renders a list of the payload key/value pairs.</p> <p>This feature\u2019s ultimate goal is to allow Amundsen administrators to point their users to more trusted/higher quality resources without removing the old references.</p> <p>Learn more about the future developments for this feature in its RFC.</p>"},{"location":"frontend/docs/application_config/#dynamic-notices","title":"Dynamic Notices","text":"<p>We are now going to allow for fetching dynamically the notices related to different resources like tables, dashboards, users, and features.</p> <p>For that, you will first enabled the <code>hasDynamicNoticesEnabled</code> flag inside the <code>resourceConfig</code> object of the goal resource. This flag is optional and will default to <code>false</code> if not set.</p>"},{"location":"frontend/docs/application_config/#examples_23","title":"Examples","text":"<p>Example of this option enabled on tables and dashboards: <pre><code>  resourceConfig: {\n    [ResourceType.table]: {\n      ... //Table Resource Configuration\n      hasDynamicNoticesEnabled: true,\n    },\n    [ResourceType.dashboard]: {\n      ... //Dashboard Resource Configuration\n      hasDynamicNoticesEnabled: true,\n    },\n  },\n</code></pre></p>"},{"location":"frontend/docs/application_config/#search-pagination","title":"Search Pagination","text":"<p>With this configuration option you can choose how many results you want to show on your search page for any given search query. The default is 10 results.</p>"},{"location":"frontend/docs/application_config/#examples_24","title":"Examples","text":"<p>Simply pass down a number: <pre><code>//...\nsearchPagination: {\n    resultsPerPage: 20,\n},\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#table-lineage","title":"Table Lineage","text":"<p>This option allows you to customize the \u201cTable Lineage\u201d links of the \u201cTable Details\u201d page. Note that this feature is intended to link to an external lineage provider.</p> <p>It includes these options  * iconPath - Path to an icon image to display next to the lineage URL.  * isBeta - Adds a \u201cbeta\u201d tag to the section header.  * isEnabled - Whether to show or hide this section  * urlGenerator - Generate a URL to the third party lineage website  * inAppListEnabled - Enable the in app Upstream/Downstream tabs for table lineage. Requires backend support.  * inAppListMessages - when an in app list is enabled this will add a custom message at the end of the lineage tabs content.  * disableAppListLinks - Set up table field based regular expression rules to disable lineage list view links.</p>"},{"location":"frontend/docs/application_config/#examples_25","title":"Examples","text":"<p>Here is an example configuration for an external provider: <pre><code>//...\n  tableLineage: {\n    externalEnabled: true,\n    iconPath: '/static/images/ICON.png',\n    isBeta: true,\n    defaultLineageDepth: 5,\n    inAppListEnabled: true,\n    inAppPageEnabled: false,\n    urlGenerator: (\n      database: string,\n      cluster: string,\n      schema: string,\n      table: string\n    ) =&gt; {\n      // Some logic here to calculate the URL\n\n      return encodeURI(`https://DEFAULT_LINEAGE_URL?schema=${schema}&amp;cluster=${cluster}&amp;db=${database}&amp;table=${table}`);\n    }\n  },\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#table-profile","title":"Table Profile","text":"<p>This configuration allows you to customize the \u201cTable Profile\u201d section of the \u201cTable Details\u201d page.</p> <p>The options are: * isBeta - Adds a \u201cbeta\u201d tag to the \u201cTable Profile\u201d section header. * isExploreEnabled - Enables the third party SQL exploration application. * exploreUrlGenerator - Generates a URL to a third party SQL explorable website.</p>"},{"location":"frontend/docs/application_config/#examples_26","title":"Examples","text":"<p>Here is an example for an SQL exploration tool: <pre><code>//...\n  tableProfile: {\n    isBeta: true,\n    isExploreEnabled: true,\n    exploreUrlGenerator: (\n      database: string,\n      cluster: string,\n      schema: string,\n      table: string,\n      partitionKey?: string,\n      partitionValue?: string\n    ) =&gt; {\n      // Some logic here\n\n      return `https://DEFAULT_EXPLORE_URL?schema=${schema}&amp;cluster=${cluster}&amp;db=${database}&amp;table=${table}`;\n    }\n  },\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#table-quality-checks","title":"Table Quality Checks","text":"<p>This configuration allows you to query and display data quality check status from an external provider. The API must be configured. Default is false.</p>"},{"location":"frontend/docs/application_config/#examples_27","title":"Examples","text":"<p>Simply enable it: <pre><code>//...\n  tableQualityChecks: {\n    isEnabled: true,\n  },\n//...\n</code></pre></p>"},{"location":"frontend/docs/application_config/#user-id-label","title":"User Id label","text":"<p>This is a temporary configuration due to lacking string customization/translation support. It will show as <code>Please enter &lt;userIdLabel&gt;</code>. Default is \u2018email address\u2019.</p>"},{"location":"frontend/docs/application_config/#examples_28","title":"Examples","text":"<p>Simply add your string: <pre><code>//...\n  userIdLabel: 'email',\n//...\n</code></pre></p>"},{"location":"frontend/docs/configuration/","title":"Configuration","text":""},{"location":"frontend/docs/configuration/#flask","title":"Flask","text":"<p>The default Flask application uses a LocalConfig that looks for the metadata and search services running on localhost. In order to use different end point, you need to create a custom config class suitable for your use case. Once the config class has been created, it can be referenced via the environment variable: <code>FRONTEND_SVC_CONFIG_MODULE_CLASS</code></p> <p>For more examples of how to leverage the Flask configuration for specific features, please see this extended doc.</p> <p>For more information on Flask configurations, please reference the official Flask documentation.</p>"},{"location":"frontend/docs/configuration/#react-application","title":"React Application","text":""},{"location":"frontend/docs/configuration/#application-config","title":"Application Config","text":"<p>Certain features of the React application import variables from an AppConfig object. The configuration can be customized by modifying config-custom.ts.</p> <p>For examples of how to leverage the application configuration for specific features, please see this extended doc.</p>"},{"location":"frontend/docs/configuration/#custom-fonts-styles","title":"Custom Fonts &amp; Styles","text":"<p>Fonts and css variables can be customized by modifying fonts-custom.scss and variables-custom.scss.</p>"},{"location":"frontend/docs/configuration/#python-entry-points","title":"Python Entry Points","text":"<p>The application also leverages python entry points for custom features. In your local <code>setup.py</code>, point the entry points detailed below to custom classes or methods that have to be implemented for a given feature. Run <code>python3 setup.py install</code> in your virtual environment and restart the application for the entry point changes to take effect.</p> <pre><code>entry_points=\"\"\"\n    [action_log.post_exec.plugin]\n    analytic_clients_action_log = path.to.file:custom_action_log_method\n\n    [preview_client]\n    table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient\n\n    [announcement_client]\n    announcement_client_class = amundsen_application.base.examples.example_announcement_client:SQLAlchemyAnnouncementClient\n\"\"\"\n</code></pre>"},{"location":"frontend/docs/configuration/#action-logging","title":"Action Logging","text":"<p>Create a custom method to handle action logging. Under the <code>[ action_log.post_exec.plugin]</code> group, point the <code>analytic_clients_action_log</code> entry point in your local <code>setup.py</code> to that method.</p>"},{"location":"frontend/docs/configuration/#preview-client","title":"Preview Client","text":"<p>Create a custom implementation of base_preview_client. Under the <code>[preview_client]</code> group, point the <code>table_preview_client_class</code> entry point in your local <code>setup.py</code> to that class.</p> <p>For those who use Apache Superset for data exploration, see this doc for how to implement a preview client for Superset.</p>"},{"location":"frontend/docs/configuration/#announcement-client","title":"Announcement Client","text":"<p>Create a custom implementation of base_announcement_client. Under the <code>[announcement_client]</code> group, point the <code>announcement_client_class</code> entry point in your local <code>setup.py</code> to that class.</p> <p>Currently Amundsen does not own the input and storage of announcements. Consider having the client fetch announcement information from an external web feed.</p>"},{"location":"frontend/docs/configuration/#authentication","title":"Authentication","text":"<p>Authentication can be hooked within Amundsen using either wrapper class or using proxy to secure the microservices on the nginx/server level. Following are the ways to setup the end-to-end authentication. - OIDC / Keycloak</p>"},{"location":"frontend/docs/developer_guide/","title":"Developer Guide","text":""},{"location":"frontend/docs/developer_guide/#environment","title":"Environment","text":"<p>Follow the installation instructions in the section Install standalone application directly from the source.</p> <p>Install the javascript development requirements:</p> <pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsenfrontendlibrary/amundsen_application\n$ cd static\n$ npm install --only=dev\n</code></pre> <p>To test local changes to the javascript static files:</p> <pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsenfrontendlibrary/amundsen_application\n$ cd static\n$ npm run dev-build # builds the development bundle\n</code></pre> <p>To test local changes to the python files, re-run the wsgi:</p> <pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsenfrontendlibrary/amundsen_application\n$ python3 wsgi.py\n</code></pre>"},{"location":"frontend/docs/developer_guide/#contributing","title":"Contributing","text":"<p>We describe our general contributing process in the main repository of Amundsen, so here we\u2019ll cover the items specific to the Frontend library. Read our Frontend Strategy doc to see where can you help.</p>"},{"location":"frontend/docs/developer_guide/#testing-python-code","title":"Testing Python Code","text":"<p>If changes were made to any python files, run the python unit tests, linter, and type checker. Unit tests are run with <code>py.test</code>. They are located in <code>tests/unit</code>. Type checks are run with <code>mypy</code>. Linting is <code>flake8</code>. There are friendly <code>make</code> targets for each of these tests:</p> <pre><code># after setting up the environment\nmake test  # unit tests in Python 3\nmake lint  # flake8\nmake mypy  # type checks\n</code></pre> <p>Fix all errors before submitting a PR.</p>"},{"location":"frontend/docs/developer_guide/#testing-frontend-code","title":"Testing Frontend Code","text":"<p><code>npm run test</code> runs our Frontend unit tests. Please add unit tests to cover new code additions and fix any test failures before submitting a PR. You can also have a dedicated terminal running <code>npm run test:watch</code> while developing, which would continuously run tests over your modified files.</p> <p>To run specific tests, run <code>npm run test-nocov -t &lt;regex&gt;</code>, where <code>&lt;regex&gt;</code> is any pattern that matches the names of the test blocks that you want to run. See our recommendations for writing unit tests.</p>"},{"location":"frontend/docs/developer_guide/#developing-react-components","title":"Developing React Components","text":"<p>To preview React components in isolation, use Storybook. Just add a <code>&lt;componentName&gt;.story.tsx</code> file in the same folder as your component. In that file, show your component in different states. Then run <code>npm run storybook</code>, which will open your browser to the storybook browse page.</p> <p>Using Storybook makes it much easier to quickly iterate on components when getting to certain states requires multiple steps of UI manipulation. The gallery also serves as a convenient place to see what reusable components are available so you can avoid reinventing the wheel.</p>"},{"location":"frontend/docs/developer_guide/#frontend-type-checking","title":"Frontend Type Checking","text":"<p>We use TypeScript in our codebase, so <code>npm run tsc</code> performs a type checking, however your IDE should point issues to you right in the code. The build commands <code>npm run build</code> and <code>npm run dev-build</code> also conduct type checking, but are slower because they also build the source code. Run any of these commands and fix all failed checks before submitting a PR.</p>"},{"location":"frontend/docs/developer_guide/#frontend-linting-and-formatting","title":"Frontend Linting and Formatting","text":"<p>We have in place two linters \u2013 ESLint for our JavaScript and TypeScript files and Stylelint for our Sass files. If you have both ESLint and Stylelint extensions installed on your IDE, you should get warnings on your editor by default.</p> <p>We also use Prettier to help us keep consistent formatting on our TypeScript and Sass code.</p> <p>Whenever you want to run these tasks manually, you can execute:</p> <ul> <li><code>npm run lint</code> to run ESLint and <code>npm run lint:fix</code> to auto-fix most of them.</li> <li><code>npm run stylelint</code> to run Stylelint and <code>npm run stylelint:fix</code> to trigger the auto-fix.</li> <li><code>npm run format</code> to run Prettier on both the TypeScript and Sass files</li> </ul> <p>We also check your changed files and format them when you create a new commit, making it easy for you and for the project to keep a consistent code style. We do this leveraging Husky and Lint-staged.</p>"},{"location":"frontend/docs/developer_guide/#accessibility-and-semantic-markup","title":"Accessibility and Semantic Markup","text":"<p>We strive to keep our application accessible. For that, we use the \u2018airbnb-typescript\u2019 preset for ESLint, which includes a bunch of accessibility rules. We also have a set of \u201cjsx-a11y/\u201d prefixed rules, which are currently on a \u201cwarn\u201d level, so they don\u2019t throw errors. Our goal is to remove that \u201cwarn\u201d level and comply with all the accessibility rules we list on our ESLint configuration.</p> <p>We also try to model our application\u2019s markup on best practices regarding semantic markup. If you are making large markup changes on one of your PRs, make sure your changes comply with this HTML semantics checklist.</p>"},{"location":"frontend/docs/developer_guide/#typography","title":"Typography","text":"<p>In the past, we have used several classes to set the styling of our heading and body text. Nowadays, we recommend to use classes in our stylesheets for each component, and extend those classes with the proper text styling by using an <code>@extend</code> to a placehoder selector:</p> <pre><code>@import \"variables\";\n@import \"typography\";\n\n.header-title-text {\n  @extend %text-headline-w2;\n}\n\n.header-subtitle-text {\n  @extend %text-subtitle-w3;\n}\n</code></pre> <p>You can find the complete list of placeholder selectors for text in this file, and its implementation in the storybook when running <code>npm run storybook</code>.</p>"},{"location":"frontend/docs/developer_guide/#iconography","title":"Iconography","text":"<p>Until recently, we haven\u2019t been using a specific icon library, using ad-hoc generated SVGs to render our icons. The initial loading strategy was based on image tags with masks, which wasn\u2019t compatible with many browsers.</p> <p>We pivoted into creating our own icons within the <code>/components/SVGIcons</code> folder. However, and given that we need to ask for specific design resources in order to get new icons, we will be moving into using a ready-made icon library, BoxIcons to create the icons in the <code>SVGIcons</code> folder. To create them, we will search for the specific BoxIcon in this site, copy the code and adapt it following the patterns in the current icons. We\u2019ll also need to export the new icon within the <code>SVGIcons/index.ts</code> file.</p> <p>Then, we will use them like this:</p> <pre><code>import { AlertIcon } from 'components/SVGIcons';\n\n...\n  &lt;AlertIcon /&gt;\n</code></pre>"},{"location":"frontend/docs/flask_config/","title":"Flask configuration","text":"<p>After modifying any variable in config.py described in this document, be sure to rebuild your application with these changes.</p> <p>NOTE: This document is a work in progress and does not include 100% of features. We welcome PRs to complete this document</p>"},{"location":"frontend/docs/flask_config/#custom-routes","title":"Custom Routes","text":"<p>In order to add any custom Flask endpoints to Amundsen\u2019s frontend application, configure a function on the <code>INIT_CUSTOM_ROUTES</code> variable. This function takes the created Flask application and can leverage Flask\u2019s add_url_rule method to add custom routes.</p> <p>Example: Setting <code>INIT_CUSTOM_ROUTES</code> to the <code>init_custom_routes</code> method below will expose a <code>/custom_route</code> endpoint on the frontend application. <pre><code>def init_custom_routes(app: Flask) -&gt; None:\n  app.add_url_rule('/custom_route', 'custom_route', custom_route)\n\ndef custom_route():\n  pass\n</code></pre></p>"},{"location":"frontend/docs/flask_config/#dashboard-preview","title":"Dashboard Preview","text":"<p>This service provides an API to download preview image of dashboard resources, which currently only supports Mode.</p> <p>The dashboard preview image is cached in user\u2019s browser up to a day. In order to adjust this you can change the value of <code>DASHBOARD_PREVIEW_IMAGE_CACHE_MAX_AGE_SECONDS</code></p>"},{"location":"frontend/docs/flask_config/#how-to-configure-mode-dashboard-preview","title":"How to configure Mode dashboard preview","text":"<p>Add the following environment variables: <pre><code>    CREDENTIALS_MODE_ADMIN_TOKEN\n    CREDENTIALS_MODE_ADMIN_PASSWORD\n    MODE_ORGANIZATION\n</code></pre></p>"},{"location":"frontend/docs/flask_config/#how-to-enable-authorization-on-mode-dashboard","title":"How to enable authorization on Mode dashboard","text":"<p>By default, Amundsen does not do any authorization on showing preview. By registering name of the Mode preview class in configuration, you can enable authorization. <pre><code>    ACL_ENABLED_DASHBOARD_PREVIEW = {'ModePreview'}\n</code></pre> Amundsen ingests Mode dashboards only from the shared space, which all registered Mode users are able to view. Therefore our authorization first validates if the current user is registered in Mode. This feature is dependent on Amundsen also ingesting Mode user information via the ModeDashboardUserExtractor and the metadata service version must be at least v2.5.2.</p>"},{"location":"frontend/docs/flask_config/#how-to-support-preview-of-different-product","title":"How to support preview of different product?","text":"<p>You can add preview support for different products by adding its preview class to DefaultPreviewMethodFactory In order to develop new preview class, you need to implement the class that inherits BasePreview class and ModePreview would be a great example.</p>"},{"location":"frontend/docs/flask_config/#mail-client-features","title":"Mail Client Features","text":"<p>Amundsen has two features that leverage the custom mail client \u2013 the feedback tool and notifications. For these features a custom implementation of base_mail_client must be mapped to the <code>MAIL_CLIENT</code> configuration variable.</p> <p>To fully enable these features in the UI, the application configuration variables for these features must also be set to true. Please see this entry in our application configuration doc for further information.</p>"},{"location":"frontend/docs/flask_config/#issue-tracking-integration-features","title":"Issue Tracking Integration Features","text":"<p>Amundsen has a feature to allow display of associated tickets within the table detail view. The feature both displays open tickets and allows users to report new tickets associated with the table. These tickets must contain the <code>table_uri</code> within the ticket text in order to be displayed; the <code>table_uri</code> is automatically added to tickets created via the feature. Tickets are displayed from most recent to oldest, and currently only open tickets are displayed. Currently only  JIRA is supported. The UI must also be enabled to use this feature, please  see configuration notes here.</p> <p>There are several configuration settings in <code>config.py</code> that should be set in order to use this feature.</p> <p>Here are the settings and what they should be set to <pre><code>    ISSUE_LABELS = []  # type: List[str] (Optional labels to be set on the created tickets)\n    ISSUE_TRACKER_URL = None  # type: str (Your JIRA environment, IE 'https://jira.net')\n    ISSUE_TRACKER_USER = None  # type: str (Recommended to be a service account)\n    ISSUE_TRACKER_PASSWORD = None  # type: str\n    ISSUE_TRACKER_PROJECT_ID = None  # type: int (Project ID for the project you would like JIRA tickets to be created in)\n    ISSUE_TRACKER_CLIENT = None  # type: str (Fully qualified class name and path)\n    ISSUE_TRACKER_CLIENT_ENABLED = False  # type: bool (Enabling the feature, must be set to True)\n    ISSUE_TRACKER_MAX_RESULTS = None  # type: int (Max issues to display at a time)\n    ISSUE_TRACKER_ISSUE_TYPE_ID = None # type: int (Jira only: Override default issue tracker ID whenever needed for cloud/hosted deployments)\n</code></pre></p>"},{"location":"frontend/docs/flask_config/#programmatic-descriptions","title":"Programmatic Descriptions","text":"<p>Amundsen supports configuring other mark down supported non-editable description boxes on the table page. This can be useful if you have multiple writers which want to write different pieces of information to Amundsen that are either very company specific and thus would never be directly integrated into Amundsen or require long form text to properly convey the information.</p> <p>What are some more specific examples of what could be used for this? - You have an existing process that generates quality reports for a dataset that you want to embed in the table page. - You have a process that detects pii information (also adding the appropriate tag/badge) but also generates a simple report to provide context. - You have extended table information that is applicable to your datastore which you want to scrape and provide in the table page</p> <p>Programmatic descriptions are referred to by a \u201cdescription source\u201d which is a unique identifier. In the UI, they will appear on the table page under structured metadata.</p> <p>In config.py you can then configure the descriptions to have a custom order, as well as whether or not they should exist in the left column or right column. <pre><code>PROGRAMMATIC_DISPLAY = {\n    'RIGHT': {\n      \"test3\" : {},\n      \"test2\" : { \"display_order\": 0 }\n    },\n    'LEFT': {\n      \"test1\" : { \"display_order\": 1 },\n      \"test0\" : { \"display_order\": 0 },\n    },\n    'test4': {\"display_order\": 0},\n}\n</code></pre> Description sources not mentioned in the configuration will be alphabetically placed at the bottom of the list. If <code>PROGRAMMATIC_DISPLAY</code> is left at <code>None</code> all added fields will show up in the order in which they were returned from the backend. Here is a screenshot of what it would look like in the bottom left:</p> <p></p>"},{"location":"frontend/docs/flask_config/#uneditable-table-descriptions","title":"Uneditable Table Descriptions","text":"<p>Amundsen supports configuring table and column description to be non-editable for selective tables. You may want to make table descriptions non-editable due to various reasons such as table already has table description from source of truth. You can define matching rules in  config.py for selecting tables. This configuration is useful as table selection criteria can be company specific which will not directly integrated with Amundsen. You can use different combinations of schema and table name for selecting tables.</p> <p>Here are some examples when this feature can be used: 1. You want to set ALL tables in your application as un-editable 2. You want to set all tables with a given schema or schema pattern as un-editable. 3. You want to set all tables with a specific table name pattern in a given schema pattern as un-editable. 4. You want to set all tables with a given table name pattern as un-editable.</p> <p>Amundsen has two variables in <code>config.py</code> file which can be used to define match rules: 1. <code>ALL_UNEDITABLE_SCHEMAS</code> : boolean on/off switch for ability to edit tables. This can also be set using the environment variable \u2018ALL_UNEDITABLE_SCHEMAS\u2019 2. <code>UNEDITABLE_SCHEMAS</code> : Set of schemas where all tables should be un-editable. It takes exact schema name. 3. <code>UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES</code> : List of MatchRuleObject, where each MatchRuleObject consists of regex for schema name or regex for table name or both.</p> <p>Purpose of <code>ALL_UNEDITABLE_SCHEMAS</code> is to provide a blanket on/off switch for disabling editing schemas. This should be used if you plan to up all of your schemas via databuilder rather then allow users to do it via the UI.</p> <p>Purpose of <code>UNEDITABLE_SCHEMAS</code> can be fulfilled by <code>UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES</code> but we are keeping both variables for backward compatibility. If you want to restrict tables from a given schemas then you can use <code>UNEDITABLE_SCHEMAS</code> as follows: <pre><code>UNEDITABLE_SCHEMAS = set(['schema1', 'schema2'])\n</code></pre> After above configuration, all tables in \u2018schema1\u2019 and \u2018schema2\u2019 will have non-editable table and column descriptions.</p> <p>If you have more complex matching rules you can use <code>UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES</code>. It provides you more flexibility and control as you can create multiple match rules and use regex for matching schema and table names.</p> <p>You can configure your match rules in <code>config.py</code> as follow: <pre><code>UNEDITABLE_TABLE_DESCRIPTION_MATCH_RULES = [\n        # match rule for all table in schema1\n        MatchRuleObject(schema_regex=r\"^(schema1)\"),\n        # macth rule for all tables in schema2 and schema3\n        MatchRuleObject(schema_regex=r\"^(schema2|schema3)\"),\n        # match rule for tables in schema4 with table name pattern 'noedit_*'\n        MatchRuleObject(schema_regex=r\"^(schema4)\", table_name_regex=r\"^noedit_([a-zA-Z_0-9]+)\"),\n        # match rule for tables in schema5, schema6 and schema7 with table name pattern 'noedit_*'\n        MatchRuleObject(schema_regex=r\"^(schema5|schema6|schema7)\", table_name_regex=r\"^noedit_([a-zA-Z_0-9]+)\"),\n        # match rule for all tables with table name pattern 'others_*'\n        MatchRuleObject(table_name_regex=r\"^others_([a-zA-Z_0-9]+)\")\n    ]\n</code></pre></p> <p>After configuring this, users will not be able to edit table and column descriptions of any table matching above match rules from UI.</p>"},{"location":"frontend/docs/installation/","title":"Installation","text":""},{"location":"frontend/docs/installation/#install-standalone-application-directly-from-the-source","title":"Install standalone application directly from the source","text":"<p>The following instructions are for setting up a standalone version of the Amundsen application. This approach is ideal for local development. <pre><code># Clone repo\n$ git clone https://github.com/amundsen-io/amundsen.git\n\n# Build static content\n$ cd amundsen/frontend/amundsen_application/static\n$ npm install\n$ npm run build # or npm run dev-build for un-minified source\n$ cd ../../\n\n# Install python resources\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip3 install -e \".[all]\" .\n\n# Start server\n$ python3 amundsen_application/wsgi.py\n# visit http://localhost:5000 to confirm the application is running\n</code></pre></p> <p>You should now have the application running at http://localhost:5000, but will notice that there is no data and interactions will throw errors. The next step is to connect the standalone application to make calls to the search and metadata services. 1. Setup a local copy of the metadata service using the instructions found here. 2. Setup a local copy of the search service using the instructions found here. 3. Modify the <code>LOCAL_HOST</code>, <code>METADATA_PORT</code>, and <code>SEARCH_PORT</code> variables in the LocalConfig to point to where your local metadata and search services are running, and restart the application with <pre><code>$ python3 amundsen_application/wsgi.py\n</code></pre></p>"},{"location":"frontend/docs/recommended_practices/","title":"Recommended Practices","text":"<p>This document serves as reference for current practices and patterns that we want to standardize across Amundsen\u2019s frontend application code. Below, we provide some high-level guidelines targeted towards new contributors or any contributor who does not yet have domain knowledge in a particular framework or core library. This document is not intended to provide an exhaustive checklist for completing certain tasks.</p> <p>We aim to maintain a reasonably consistent code base through these practices and welcome PRs to update and improve these recommendations.</p>"},{"location":"frontend/docs/recommended_practices/#code-styleguide","title":"Code Styleguide","text":"<p>We have our coding styleguide baked into our ESLint and Prettier rules for TypeScript code an Stylelint rules for Sass code.</p> <p>Looking forward, we aim at setting more strict best practices using ESLint and Stylelint. For that, we are leveraging a project called betterer, which keeps track of our errors when a given test is passed. You can run it using <code>npm run betterer</code> and it will break if you introduce any new eslint errors. If you want to ignore the new errors you can run <code>npm run betterer:update</code> to update the betterer.results file. We do not recommend adding or introducing new eslint errors.</p>"},{"location":"frontend/docs/recommended_practices/#naming-conventions","title":"Naming Conventions","text":"<p>React is not opinionated about the naming of our components. Having freedom is great, but this sometimes comes at the cost of lack of consistency on the API.</p> <p>To make sure Amundsen\u2019s React application is consistent and intuitive to use, we created this document that will describe the naming conventions for the React components.</p>"},{"location":"frontend/docs/recommended_practices/#component-names","title":"Component Names","text":""},{"location":"frontend/docs/recommended_practices/#view-components","title":"View Components","text":"<p>We will follow this convention for naming our regular \u201cview\u201d components:</p> <p>[Context]ComponentName[Type]</p> <p>Where: *   Context - the parent component or high-level page *   ComponentName - what this component does. The responsibility of the component *   Type - the type of component. Usually they are views, but they could be a Form, a List, a Figure, an Illustration, a Container</p> <p>Examples: *   SideBar (root component) *   FooterSideBar (with context) *   SideBarForm (with component type) *   FooterSideBarForm (with all)</p>"},{"location":"frontend/docs/recommended_practices/#custom-hook-components","title":"Custom Hook Components","text":"<p>We will name custom hook components with a name starting with \u201cuse\u201d, as mentioned on the official docs.</p>"},{"location":"frontend/docs/recommended_practices/#high-order-components","title":"High-order Components","text":"<p>We will name high order components (HOCs) using the \u201cwith\u201d prefix. For example:</p> <ul> <li>withAuthentication</li> <li>withSubscription</li> </ul>"},{"location":"frontend/docs/recommended_practices/#providerconsumer-components","title":"Provider/Consumer Components","text":"<p>Whenever we need to use the Provider/Consumer pattern and name the components, we will use the \u201cProvider\u201d and \u201cConsumer\u201d suffixes. For example:</p> <ul> <li>LoginProvider/LoginConsumer</li> <li>DataProvider/DataConsumer</li> </ul>"},{"location":"frontend/docs/recommended_practices/#prop-names","title":"Prop Names","text":""},{"location":"frontend/docs/recommended_practices/#handler-functions","title":"Handler Functions","text":"<p>We will use the \u201con\u201d prefix for handler functions, optionally adding a \u201csubject\u201d word before the event. The schema would be \u201con[Subject]Event\u201d, for example:</p> <ul> <li>onClick</li> <li>onItemClick</li> <li>onLogoHover</li> <li>onFormSubmit</li> </ul> <p>Internally, we will use the \u201chandle\u201d prefix for naming the handling functions, with an optional \u201csubject\u201d word in the middle. The schema would be then \u201chandle[Subject]Event\u201d, for example:</p> <ul> <li>handleClick</li> <li>handleButtonClick</li> <li>handleHeadingHover</li> </ul>"},{"location":"frontend/docs/recommended_practices/#props-by-type","title":"Props by Type","text":"<p>Props should be named to describe the component itself, and what it does, but not why it does it.</p> <ul> <li>Objects - Use a singular noun<ul> <li>item</li> </ul> </li> <li>Arrays - Use a plural noun<ul> <li>items</li> <li>users</li> </ul> </li> <li>Numbers - Use a name with \u201cnum\u201d prefix or with \u201ccount\u201d or \u201cindex\u201d suffix.<ul> <li>numItems</li> <li>userCount</li> <li>itemIndex</li> </ul> </li> <li>Booleans - Use a \u201cis\u201d, \u201chas\u201d, or \u201ccan\u201d prefix<ul> <li>\u201cis\u201d for visual variations<ul> <li>isVisible</li> <li>isEnabled</li> <li>isActive</li> </ul> </li> <li>\u201cis\u201d also for behavioral or conditional variations<ul> <li>isToggleable</li> <li>isExpandable</li> </ul> </li> <li>\u201chas\u201d for toggling UI elements<ul> <li>hasCancelSection</li> <li>hasHeader</li> </ul> </li> </ul> </li> <li>React Nodes - use \u201celement\u201d suffix<ul> <li>buttonElement</li> <li>itemElement</li> </ul> </li> </ul>"},{"location":"frontend/docs/recommended_practices/#unit-testing","title":"Unit Testing","text":"<p>We use Jest as our test framework. We leverage utility methods from Enzyme to test React components, and use redux-saga-test-plan to test our <code>redux-saga</code> middleware logic.</p>"},{"location":"frontend/docs/recommended_practices/#general","title":"General","text":"<ol> <li>Leverage TypeScript to prevent bugs in unit tests and ensure that code is tested with inputs that match the defined interfaces and types. Adding and updating test fixtures helps to provide re-useable pieces of typed test data or mock implementations for this purpose.</li> <li>Leverage <code>beforeAll()</code>/<code>beforeEach()</code> for test setup when applicable. Leverage <code>afterAll()</code>/<code>afterEach</code> for test teardown when applicable to remove any side effects of the test block. For example if a mock implementation of a method was created in <code>beforeAll()</code>, the original implementation should be restored in <code>afterAll()</code>. See Jest\u2019s setup-teardown documentation for further understanding.</li> <li>Use descriptive title strings. To assist with debugging we should be able to understand what a test is checking for and under what conditions.</li> <li>Become familiar with the variety of Jest matchers that are available. Understanding the nuances of different matchers and the cases they are each ideal for assists with writing more robust tests. For example, there are many different ways to verify objects and the best matcher to use will depend on what exactly we are testing for. Examples:</li> <li>If asserting that <code>inputObject</code> is assigned to variable <code>x</code>, asserting the equivalence of <code>x</code> using <code>.toBe()</code> creates a more robust test for this case because <code>.toBe()</code> will verify that the variable is actually referencing the given object. Contrast this to a matcher like <code>.toEqual()</code> which will verify whether or not the object happens to have a particular set of properties and values. In this case using <code>.toEqual()</code> would risk hiding bugs where <code>x</code> is not actually referencing <code>inputObject</code> as expected, yet happens to have the same key value pairs perhaps due to side effects in the code.</li> <li>If asserting that <code>outputObject</code> matches <code>expectedObject</code>, asserting the equivalence of each property on <code>outputObject</code> using <code>.toBe()</code> or asserting the equality of the two objects using <code>.toMatchObject()</code> is useful when we only care that certain values exist on <code>outputObject</code>. However if it matters that certain values do not exist on <code>outputObject</code> \u2013 as is the case with reducer outputs \u2013 <code>.toEqual()</code> is a more robust alternative as it compares all properties on both objects for equivalence.</li> <li>When testing logic that makes use of JavaScript\u2019s Date object, note that our Jest scripts are configured to run in the UTC timezone. Developers should either:</li> <li>Mock the Date object and its methods\u2019 return values, and run assertions based on the mock values.</li> <li>Create assertions knowing that the unit test suite will run as if we are in the UTC timezone.</li> <li>Code coverage is important to track but it only informs us of what code was actually run and executed during the test. The onus is on the developer to focus on use case coverage and make sure that right assertions are run so that all logic is adequately tested.</li> </ol>"},{"location":"frontend/docs/recommended_practices/#react","title":"React","text":"<ol> <li>Enzyme provides 3 different utilities for rendering React components for testing. We recommend using <code>mount</code> rendering so you can dive deep on the rendered output.</li> <li>Create a re-useable <code>setup()</code> function that will take any arguments needed to test conditional logic.</li> <li>Look for opportunities to organize tests a way such that one <code>setup()</code> can be used to test assertions that occur under the same conditions. For example, a test block for a method that has no conditional logic should only have one <code>setup()</code>. However, it is not recommended to share a <code>setup()</code> result across tests for different methods, or across tests for a method that has a dependency on a mutable piece of state. The reason is that we risk propagating side effects from one test block to another.</li> <li>Consider refactoring components or other files if they become burdensome to test. Potential options include (but are not limited to):</li> <li>Create subcomponents for large components. This is also especially useful for reducing the burden of updating tests when component layouts are changed.</li> <li>Break down large functions into smaller functions. Unit test the logic of the smaller functions individually, and mock their results when testing the larger function.</li> <li>Export constants from a separate file for hardcoded values and import them into the relevant source files and test files. This is especially helpful for strings.</li> </ol>"},{"location":"frontend/docs/recommended_practices/#redux","title":"Redux","text":"<ol> <li>Because the majority of Redux code consists of functions, we unit test those methods as usual and ensure the functions produce the expected output for any given input. See Redux\u2019s documentation on testing action creators, async action creators, and reducers, or check out examples in our code.</li> <li>Unless an action creator includes any logic other than returning the action, unit testing the reducer and saga middleware logic is sufficient and provides the most value.</li> <li><code>redux-saga</code> generator functions can be tested by iterating through it step-by-step and running assertions at each step, or by executing the entire saga and running assertions on the side effects. See redux-saga\u2019s documentation on testing sagas for a wider breadth of examples.</li> </ol>"},{"location":"frontend/docs/recommended_practices/#reference","title":"Reference","text":"<ul> <li>How to name props for React components \u2013 David\u2019s Blog</li> <li>Handy Naming Conventions for Event Handler Functions &amp; Props in React | by Juan Bernal | JavaScript In Plain English | Medium</li> <li>https://medium.com/@wittydeveloper/react-components-naming-convention-%EF%B8%8F-b50303551505</li> <li>https://reactjs.org/docs/hooks-custom.html#extracting-a-custom-hook</li> <li>https://reactjs.org/docs/higher-order-components.html</li> </ul>"},{"location":"frontend/docs/search_from_browser/","title":"Search From Browser","text":"<p>We can expose Amundsen search to web browsers to allow searching Amundsen directly from the address bar. We do this by following the OpenSearch standard and including an <code>opensearch.xml</code> file to describe how to template a search URL.</p> <p>We link to this file from <code>index.html</code> as long as the FRONTEND_BASE config variable is set. Browsers will automatically follow this link and add Amundsen as a custom search engine. In Chrome these are viewable in <code>chrome://settings/searchEngines</code>.</p>"},{"location":"frontend/docs/strategy/","title":"Amundsen Frontend Strategy","text":"<p>A vision and strategy for Amundsen\u2019s Frontend</p>"},{"location":"frontend/docs/strategy/#1-overview","title":"1. Overview","text":"<p>Amundsen\u2019s UI hasn\u2019t had a lot of updates on our front-end application in the last year. As a side effect, the code is suffering from stagnation. The team had to hold off on solving some of the user experience and quality issues due to limited resourcing. This is a problem because this can slow velocity and critical updates become more urgent if we don\u2019t focus our limited resources in the right direction.</p> <p>One thing we can do is to create a technical strategy that sets a direction for contributors and maintainers on the front-end that allows them to align, make quick, confident decisions and improve the effectiveness of the resources we have.</p> <p>Table of Contents</p> <ul> <li> <ol> <li>Overview</li> </ol> </li> <li> <ol> <li>What</li> </ol> </li> <li> <ol> <li>Where are we today</li> </ol> </li> <li>3.1. User Experience</li> <li>3.2. Codebase Quality</li> <li>3.3. Developer Experience and Collaboration</li> <li> <ol> <li>Where we want to be</li> </ol> </li> <li>4.1. User Experience</li> <li>4.2. Codebase Quality</li> <li>4.3. Developer Experience and Collaboration</li> <li> <ol> <li>Current State and Vision Summary</li> </ol> </li> <li> <ol> <li>Next Steps</li> </ol> </li> <li>6.1. Prioritized Milestones<ul> <li>6.1.1. Milestone 1: Increase OSS contributions and remove critical team devex issues</li> <li>6.1.2. Milestone 2: Eliminate main User reported issues</li> <li>6.1.3. Milestone 3: Reduce dependencies issues and improve testing</li> </ul> </li> <li> <ol> <li>Appendix A: Prioritization Process</li> </ol> </li> <li>7.1. User Experience</li> <li>7.2. Codebase Quality</li> <li>7.3. Developer Experience and Collaboration</li> <li>7.4. Next steps (by cost/impact)</li> <li>7.5. Prioritized Improvements</li> <li> <ol> <li>Appendix B: Why we got here</li> </ol> </li> </ul>"},{"location":"frontend/docs/strategy/#2-what","title":"2. What","text":"<p>A technical strategy is a tool of proactive alignment that empowers teams to move quickly and with confidence. A great technical strategy identifies a problem with the current situation, proposes a principled approach to overcome it, and then shows you a coherent roadmap to follow.</p> <p>This Strategy will break down into three parts: * Where are we now (Current State) * Where do we want to get to (Target State) * What are we doing next (Next Steps)</p>"},{"location":"frontend/docs/strategy/#3-where-are-we-today","title":"3. Where are we today","text":"<p>To create a strategy for Amundsen\u2019s UI in the near future, we need to know where we are today. Here we\u2019ll describe it with the following considerations: * User Experience - How our final users experience the UI and how well they can perform their tasks. * Codebase Quality - The quality of the codebase, how fast we can extend it with new features without hurting stability. * Developer Experience &amp; Collaboration - How easy is for new contributors to start helping in the project.</p>"},{"location":"frontend/docs/strategy/#31-user-experience","title":"3.1. User Experience","text":"<p>We lack consistency and a polished user experience. We can summarize it on these points: * Inconsistent typography * Inconsistent buttons and links * Not a11y WAG 2.1 compliant * Inconsistent error handling * Limited \u201cbrowsing\u201d experience * Bulky components &amp; layout * Homepage page is not tailored to user role * Cluttered metadata on the left panel of the Table Detail page * Differing list components across different pages and even within the same page across different tabs</p>"},{"location":"frontend/docs/strategy/#32-codebase-quality","title":"3.2. Codebase Quality","text":"<p>There are some aspects where the current codebase is sub-par: * Many old ESLint issues still around, like:     * Not destructing props     * Usage of setState within componentDidUpdate * Human-readable strings hardcoded on JSX code * Sub-optimal state management     * Scattered and inconsistent Redux logic for extracting info     * Complex and boilerplate full Redux logic * Incomplete testing     * Outdated unit testing library (Enzyme, 107 files)     * Incomplete unit test coverage     * No end to end testing * Scattered URL logic * Sub-par API     * No API type Verification     * Non-cleaned deprecated endpoints     * Inconsistent functionality between endpoints for different resources * Many bugs in the codebase * No code health monitoring * Deprecated component library * Outdated React Version (v16)</p>"},{"location":"frontend/docs/strategy/#33-developer-experience-and-collaboration","title":"3.3. Developer Experience and Collaboration","text":"<p>Developers bump into friction when dealing with the following issues: * Scattered fixture data for our unit tests * Hard to figure out typography * Inconsistent and incomplete instrumentation * No test coverage tracking * Incomplete Documentation     * Incomplete frontend configuration documentation     * No code style guide     * No naming conventions * Outdated Docker image for getting started with Amundsen for OSS     * Missing showcase of configurable features     * Outdated versions of all packages * No PR Previews * Complex and brittle developer environment setup * Custom React application on top of Flask</p>"},{"location":"frontend/docs/strategy/#4-where-we-want-to-be","title":"4. Where we want to be","text":"<p>Now that we have established where we are today (as of Jan 2023); let\u2019s now describe where we want to be within a medium to long term. We will repeat the previous grouping:</p>"},{"location":"frontend/docs/strategy/#41-user-experience","title":"4.1. User Experience","text":"<ul> <li>Consistent typography</li> <li>Consistent buttons and links</li> <li>A11y WAG 2.1 compliant</li> <li>Consistent and efficient error handling</li> <li>Delightful \u201cbrowsing\u201d experience</li> <li>Information-dense components &amp; layout</li> <li>Tailored and configurable homepage</li> <li>Tidy Table Detail page metadata</li> <li>Consistent list components</li> </ul>"},{"location":"frontend/docs/strategy/#42-codebase-quality","title":"4.2. Codebase Quality","text":"<ul> <li>No ESLint warnings</li> <li>All human-readable strings as constants</li> <li>Optimal state management<ul> <li>Testable and extensible Redux logic for extracting info</li> <li>More maintainable Redux logic</li> </ul> </li> <li>Thorough testing<ul> <li>Using React Testing Library</li> <li>Fully tested codebase (80%+)</li> <li>With end to end testing</li> </ul> </li> <li>Centralized URL logic</li> <li>Pristine API<ul> <li>Tested API type verification</li> <li>Non-deprecated endpoints</li> <li>Consistent functionality between endpoints for different resources</li> </ul> </li> <li>Bug-free code</li> <li>With codebase health monitoring</li> <li>Up to date component library</li> <li>Up to date React (v18)</li> </ul>"},{"location":"frontend/docs/strategy/#43-developer-experience-and-collaboration","title":"4.3. Developer Experience and Collaboration","text":"<ul> <li>Extendable fixture data we can use to manually test the UI</li> <li>Limited typography options</li> <li>Thorough and extensible instrumentation</li> <li>Easily tracked test coverage that breaks builds when reduced</li> <li>Thorough Documentation<ul> <li>Thorough frontend configuration documentation with examples</li> <li>Complete code style guide</li> <li>Naming conventions</li> </ul> </li> <li>Up to date Docker image for OSS<ul> <li>Including showcase of configurable features</li> <li>Updated versions of all packages</li> </ul> </li> <li>Previews with PRs</li> <li>Easy developer environment setup</li> <li>React framework (Next.js) independent from flask API</li> </ul>"},{"location":"frontend/docs/strategy/#5-current-state-and-vision-summary","title":"5. Current State and Vision Summary","text":"Consideration Current State Target State User Experience Inconsistent typography     Consistent typography     Inconsistent buttons and links     Consistent buttons and links     Not a11y WAG 2.1 compliant     A11y WAG 2.1 compliant     Inconsistent error handling     Consistent and efficient error handling     Limited \u201cbrowsing\u201d experience     Delightful \u201cbrowsing\u201d experience     Bulky components &amp; layout     Information-dense components &amp; layout     Homepage page is not tailored to user role     Tailored and configurable homepage     Cluttered metadata on the left panel of the Table Detail page     Tidy Table Detail page metadata     Differing list components across pages or tabs     Consistent list components     Codebase Quality Many old ESLint issues still around, like: <ul> <li>Not destructing props  <li>Usage of setState within componentDidUpdate </li> No ESLint warnings     Human-readable strings hardcoded on JSX code     All human-readable strings as constants     Sub-optimal state management <ul> <li>Scattered and inconsistent Redux logic for extracting info  <li>Complex and boilerplate full Redux logic </li> Optimal state management <ul> <li>Testable and extensible Redux logic for extracting info  <li>More maintainable Redux logic </li> Incomplete testing <ul> <li>Outdated unit testing library (Enzyme)  <li>Incomplete unit test coverage  <li>No end to end testing </li> Thorough testing <ul> <li>Using React Testing Library  <li>Fully tested codebase (80%+)  <li>With end to end testing </li> Scattered URL logic     Centralized URL logic     Sub-par API <ul> <li>No API type Verification  <li>Non-cleaned deprecated endpoints  <li>Inconsistent functionality between endpoints for different resources </li> Pristine API <ul> <li>Tested API type verification  <li>Non-deprecated endpoints  <li>Consistent functionality between endpoints for different resources </li> Many bugs in the codebase     Bug-free code     No code health monitoring     With codebase health monitoring     Deprecated component library     Up to date component library     Outdated React Version (v16)     Up to date React (v18)     Developer Experience and Collaboration Scattered fixture data for our unit tests     Extendable fixture data we can use to manually test the UI     Hard to figure out typography     Limited typography options     Inconsistent and incomplete instrumentation     Thorough and extensible instrumentation     No test coverage tracking     Easily tracked test coverage that breaks builds when reduced     Incomplete Documentation <ul> <li>Incomplete frontend configuration documentation  <li>No code style guide  <li>No naming conventions </li> Thorough Documentation <ul> <li>Thorough frontend configuration documentation with examples  <li>Complete code style guide  <li>Naming conventions </li> Outdated Docker image for getting started with Amundsen for OSS <ul> <li>Missing showcase of configurable features  <li>Outdated versions of all packages </li> Up to date Docker image for OSS <ul> <li>Including showcase of configurable features  <li>Updated versions of all packages </li> No PR Previews     Previews with PRs     Complex and brittle developer environment setup     Easy developer environment setup     Custom React application on top of Flask     React framework (Next.js) independent from flask API"},{"location":"frontend/docs/strategy/#6-next-steps","title":"6. Next Steps","text":"<p>We have seen where we think we are right now, and where we want to be in the future. The changes are many, so what are our next steps? In this section, we describe the changes we\u2019ll need to introduce to take us to our vision.</p>"},{"location":"frontend/docs/strategy/#61-prioritized-milestones","title":"6.1. Prioritized Milestones","text":"<p>From the next steps task prioritization (see Appendix B), we have grouped tasks into Milestones that will target specific metrics. Here is our proposal:</p>"},{"location":"frontend/docs/strategy/#611-milestone-1-increase-oss-contributions-and-remove-critical-team-devex-issues","title":"6.1.1. Milestone 1: Increase OSS contributions and remove critical team devex issues","text":"<p>Metrics: * Increased number of contributions (in PR numbers) * Team time savings (estimation)</p> <p>Success Condition: We increase the monthly contributions by 20%</p> <p>Tasks: 1. Update Docker image for OSS - Adding a showcase of configurable features 2. Fix developer environment setup - Create a document of the current setup that works 100% of the times using Lyft\u2019s tooling and Python version 3. Improve error handling - Provide useful Metadata, and API error messages 4. Add end to end testing - running them on the development machine and covering the main user flows 5. Improve Documentation - Improve our frontend configuration documentation     1. Complete docs with all the rest of config options     2. Add examples to all the options     3. Structure docs 6. Get PR previews working and usable - with extensive test data 7. Improve test coverage - 100% test coverage on config-utils and util files 8. Improve Documentation - Create a code style guide and naming conventions</p>"},{"location":"frontend/docs/strategy/#612-milestone-2-eliminate-main-user-reported-issues","title":"6.1.2. Milestone 2: Eliminate main User reported issues","text":"<p>Metrics: Customer satisfaction (Surveyed)</p> <p>Success Condition: We increase customer satisfaction by 20% (NPS score or other metric)</p> <p>Tasks: 1. Improve error handling - Provide useful or actionable UI error messages 2. Organize the left sidebar of the Table Detail Page - Move information into expandable blocks 3. Organize the left sidebar of the Table Detail Page - Declutter features 4. Squash all outstanding bugs 5. Improve error handling - Improve user experience when token expires</p>"},{"location":"frontend/docs/strategy/#613-milestone-3-reduce-dependencies-issues-and-improve-testing","title":"6.1.3. Milestone 3: Reduce dependencies issues and improve testing","text":"<p>Metrics: Team time savings (estimation)</p> <p>Success Condition: We have no issues when bumping OSS project versions</p> <p>Tasks: 1. Update Docker image for OSS - Update versions of all packages 2. Fix our developer environment setup - Simplify setup if possible     1. Align internal and external python package versions as much as possible for easier installation and pyenv setup 3. Add end to end testing - Running on CI and covering secondary user flows 4. Improve test coverage - 90% test coverage on presentational components and 80% overall test coverage</p>"},{"location":"frontend/docs/strategy/#7-appendix-a-prioritization-process","title":"7. Appendix A: Prioritization Process","text":"<p>After we synced on where we think we are right now, and where we want to be in the future, we got many tasks. So what are our next steps? In this section, we describe the changes we\u2019ll need to introduce to take us to our vision.</p> <p>We will add an extra category to the improvements: their benefits to our users, and to the OSS team. For that, we will include these labels:</p> <ol> <li>&lt;UT&gt; - Improvements that save time to our users</li> <li>&lt;DEXP&gt; - Improvements that save time to our team<ul> <li>By reducing the maintenance burden</li> <li>By reducing the time to create features and debug issues</li> <li>By reducing friction for new OSS contributions, effectively getting more help</li> </ul> </li> </ol> <p>Let\u2019s see the improvements with the initial categories and their benefits:</p>"},{"location":"frontend/docs/strategy/#71-user-experience","title":"7.1. User Experience","text":"<ul> <li>Improve our typography - &lt;DEXP&gt;<ul> <li>Remove old classes</li> </ul> </li> <li>Improve our buttons - &lt;DEXP&gt;<ul> <li>Audit our buttons in Storybook</li> <li>Audit our links in Storybook</li> <li>Update all our buttons and links to be consistent</li> <li>Add missing styles</li> </ul> </li> <li>Improve our a11y - &lt;UT&gt;<ul> <li>Audit a11y</li> <li>Fix a11y issues</li> <li>Setup automated testing to catch regressions</li> </ul> </li> <li>Improve error handling - &lt;DEXP&gt;, &lt;UT&gt;<ul> <li>Handle errors at the right level and with useful feedback</li> <li>Improve experience when token expires</li> </ul> </li> <li>Improve our browsing experience - &lt;UT&gt;<ul> <li>Complete our transition to faceted search on the search page</li> <li>Add a basic browse experience to complement the faceted search</li> </ul> </li> <li>Bulky components &amp; layout - &lt;UT&gt;<ul> <li>Update our components to be more compact</li> <li>Update our layout to be more compact</li> <li>Update table detail page left panel to look more orderly when a table has many of the \u2018optional\u2019 fields displayed.</li> </ul> </li> <li>Homepage page is not tailored to user role - &lt;UT&gt;<ul> <li>Move our announcements section into a drawer that opens from the global header</li> <li>Improve our homepage layout to contain more customizable widgets</li> </ul> </li> <li>Organize the left sidebar of the Table Detail Page - &lt;UT&gt;<ul> <li>Move information into blocks</li> <li>Reorganize the blocks and open those more important for our users</li> </ul> </li> <li>Make the lists/tables consistent - &lt;DEXP&gt;<ul> <li>Decide for one way of listing elements</li> <li>Roll it out all over the UI</li> </ul> </li> </ul>"},{"location":"frontend/docs/strategy/#72-codebase-quality","title":"7.2. Codebase Quality","text":"<ul> <li>Fix all the ESLint warnings - &lt;DEXP&gt;<ul> <li>Destructuring</li> <li>SetState issues</li> </ul> </li> <li>Improve State Management - &lt;DEXP&gt;<ul> <li>Create Redux selectors whenever we can</li> <li>Explore using Redux toolkit to simplify our Redux logic</li> </ul> </li> <li>Improve Testing - &lt;DEXP&gt;<ul> <li>Migrate Enzyme tests into React Testing Library &lt;DEXP&gt;<ul> <li>Audit work</li> <li>Create tickets and find support</li> <li>Migrate tests</li> </ul> </li> <li>Improve test coverage - &lt;DEXP&gt;<ul> <li>100% test coverage on config-utils</li> <li>100% test coverage on util files</li> <li>90% test coverage on presentational components</li> <li>80% overall test coverage components</li> </ul> </li> <li>Add end to end testing - &lt;UT&gt;, &lt;DEXP&gt;<ul> <li>Cover main user flows</li> <li>Cover secondary user flows</li> <li>Ran on CI</li> </ul> </li> </ul> </li> <li>Centralize URL logic - &lt;DEXP&gt;<ul> <li>Create util file with static URLs</li> <li>Extend with dynamic URLs</li> </ul> </li> <li>Improve our API - &lt;DEXP&gt;<ul> <li>Runtime type checking with https://gcanti.github.io/io-ts/</li> <li>Clean up all deprecated endpoints</li> <li>Refactor resource endpoints to make them consistent</li> </ul> </li> <li>Squash all the bugs - &lt;UT&gt;<ul> <li>Critical</li> <li>Major</li> <li>Minor</li> </ul> </li> <li>Add codebase health monitoring - &lt;DEXP&gt;<ul> <li>Find a free solution for OSS (code climate?)</li> <li>Introduce it</li> <li>Break builds with degradations</li> </ul> </li> <li>Update component library - &lt;DEXP&gt;</li> <li>Update React version - &lt;DEXP&gt;<ul> <li>Depends on moving to React Testing Library</li> <li>Update codebase using codemods</li> </ul> </li> </ul>"},{"location":"frontend/docs/strategy/#73-developer-experience-and-collaboration","title":"7.3. Developer Experience and Collaboration","text":"<ul> <li>Fix our fixture data - &lt;DEXP&gt;<ul> <li>Create test data builders for all our fixtures</li> <li>Create a debug mode to use the test data for manual testing of features</li> </ul> </li> <li>Easy to use text component - &lt;DEXP&gt;</li> <li>Improved instrumentation - &lt;DEXP&gt;<ul> <li>Actions with analytics payloads</li> <li>Make sure all feature utilization is accurately tracked 4</li> </ul> </li> <li>Test coverage tracking - &lt;DEXP&gt;<ul> <li>Use codecov to track test coverage</li> <li>Use codecov to break builds when reducing by a given %</li> </ul> </li> <li>Improve Documentation - &lt;DEXP&gt;<ul> <li>Improve our frontend configuration documentation<ul> <li>Complete docs with all the rest of config options</li> <li>Add examples to all the options</li> <li>Structure docs</li> </ul> </li> <li>Create a code styleguide</li> <li>Create naming conventions</li> </ul> </li> <li>Update Docker image for OSS - &lt;DEXP&gt;<ul> <li>Add showcase of configurable features</li> <li>Update versions of all packages</li> </ul> </li> <li>Get Uffici tool merged and usable - &lt;DEXP&gt;</li> <li>Fix our developer environment setup - &lt;DEXP&gt;<ul> <li>Create a document of the current setup that works 100% of the times using Lyft\u2019s tooling and Python version</li> <li>Simplify setup if possible</li> </ul> </li> <li>Migrate into using Next.js - &lt;DEXP&gt;<ul> <li>Pick between the many approaches to incrementally adopt Next.js</li> <li>Move out the application part by part into Next.js keeping Flask as the API server</li> </ul> </li> </ul>"},{"location":"frontend/docs/strategy/#74-next-steps-by-costimpact","title":"7.4. Next steps (by cost/impact)","text":"<p>We have seen the gap we\u2019ll need to cover in each consideration group and our vision for them. Now, we\u2019ll look at the work from an impact point of view. We will prioritize things with the highest leverage and tasks we should do first to make others easier later.</p> <p>For that, we have estimated the impact (from 0 to 5) and cost (T-Shirt sizing), and prioritized the improvements in the following list:</p>"},{"location":"frontend/docs/strategy/#75-prioritized-improvements","title":"7.5. Prioritized Improvements","text":"<ol> <li> <p>Update Docker image for OSS - &lt;DEXP&gt; | Impact - 4 5 5 5 - 5 | Cost - S</p> <ol> <li>Add showcase of configurable features</li> <li>Update versions of all packages</li> </ol> </li> <li> <p>Fix our developer environment setup - &lt;DEXP&gt; | Impact - 4 5 5 4 - 4.5 **| Cost - M**</p> <ol> <li>Create a document of the current setup that works 100% of the times using Lyft\u2019s tooling and Python version</li> <li>Simplify setup if possible<ol> <li>Align internal and external python package versions as much as possible for easier installation and pyenv setup</li> </ol> </li> </ol> </li> <li> <p>Improve error handling - &lt;DEXP&gt;, &lt;UT&gt; | Impact - 4 4 4 3 4 - 4 **| Cost **- M</p> <ol> <li>Provide useful Metadata error messages</li> <li>Provide useful API error messages</li> <li>Provide useful UI messaging</li> <li>Improve experience when token expires</li> </ol> </li> <li> <p>Organize the left sidebar of the Table Detail Page - &lt;UT&gt; | Impact - 5 3 5 4 4 - 4 **| Cost - M**</p> <ol> <li>Move information into blocks</li> <li>Reorganize the blocks and open those more important for our users</li> <li>Later: declutter</li> </ol> </li> <li> <p>Add end to end testing - &lt;UT&gt;, &lt;DEXP&gt; | Impact - 4 5 4 - 4.5 **| Cost **- L</p> <ol> <li>Depends on Docker image working</li> <li>Ran in Dev machine</li> <li>Cover main user flows</li> <li>Cover secondary user flows</li> <li>Ran on CI</li> </ol> </li> <li> <p>Squash all the bugs - &lt;UT&gt; | Impact - 4 4 3 5 4 - 4 **| Cost **- S</p> <ol> <li>Critical</li> <li>Major</li> <li>Minor</li> </ol> </li> <li> <p>Improve Documentation - &lt;DEXP&gt; | Impact - 5 4 4 3 4 - 4 **| Cost - M**</p> <ol> <li>Improve our frontend configuration documentation<ol> <li>Complete docs with all the rest of config options</li> <li>Add examples to all the options</li> <li>Structure docs</li> </ol> </li> <li>Create a code styleguide</li> <li>Create naming conventions</li> </ol> </li> <li> <p>Get Uffici tool merged and usable - &lt;DEXP&gt; | Impact - 4 3 3 4 - 3.5 **| Cost - M**</p> <ol> <li>Depends on the docker task</li> </ol> </li> <li> <p>Improve test coverage - &lt;DEXP&gt; | Impact - 4 3 - 3.5 **| Cost **- M</p> <ol> <li>100% test coverage on config-utils</li> <li>100% test coverage on util files</li> <li>90% test coverage on presentational components</li> <li>80% overall test coverage components</li> </ol> </li> <li> <p>Migrate Enzyme tests into React Testing Library &lt;DEXP&gt; | Impact - 3 3 - 3 | Cost -</p> <ol> <li>Audit work</li> <li>Create tickets and find support</li> <li>Migrate tests</li> </ol> </li> <li> <p>Migrate into using Next.js - &lt;DEXP&gt; | Impact - 3 2 3 4 - 3 | Cost -</p> <ol> <li>Pick between the many approaches to incrementally adopt Next.js</li> <li>Move out the application part by part into Next.js keeping Flask as the API server</li> </ol> </li> <li> <p>Improve our browsing experience - &lt;UT&gt; | Impact - 3 2 3 2 3 - 3 | Cost -</p> <ol> <li>Complete our transition to faceted search on the search page</li> <li>Add a basic browse experience to complement the faceted search</li> </ol> </li> <li> <p>Bulky components &amp; layout - &lt;DP&gt; | Impact - 4 2 3 3 3 - 3 | Cost -</p> <ol> <li>Update our components to be more compact</li> <li>Update our layout to be more compact</li> <li>Update table detail page left panel to look more orderly when a table has many of the \u2018optional\u2019 fields displayed.</li> </ol> </li> <li> <p>Homepage page is not tailored to user role - &lt;UT&gt; | Impact - 4 3  4 2 3 - 3 | Cost -</p> <ol> <li>Move our announcements section into a drawer that opens from the global header</li> <li>Improve our homepage layout to contain more customizable widgets</li> </ol> </li> <li> <p>Improve State Management - &lt;DEXP&gt; | Impact - 1 3 3 3 - 3 | Cost -</p> <ol> <li>Create Redux selectors whenever we can</li> <li>Explore using Redux toolkit to simplify our Redux logic</li> </ol> </li> <li> <p>Improve our API - &lt;DEXP&gt; | Impact - 4 3 3 3 - 3 | Cost -</p> <ol> <li>Runtime type checking with https://gcanti.github.io/io-ts/</li> <li>Clean up all deprecated endpoints</li> <li>Refactor resource endpoints to make them consistent</li> </ol> </li> <li> <p>Add codebase health monitoring - &lt;DEXP&gt; | Impact - 2 3 4 3 - 3 | Cost -</p> <ol> <li>Find a free solution for OSS (code climate?)</li> <li>Introduce it</li> <li>Break builds with degradations</li> </ol> </li> <li> <p>Update component library - &lt;DEXP&gt; | Impact - 2 3 3 3 4 - 3 | Cost -</p> </li> <li> <p>Improved instrumentation - &lt;DEXP&gt; | Impact - 1 3 2 3 - 3 | Cost -</p> <ol> <li>Actions with analytics payloads</li> <li>Make sure all feature utilization is accurately tracked</li> </ol> </li> <li> <p>Test coverage tracking - &lt;DEXP&gt; | Impact - 2 3 3 2 - 2.5 | Cost -</p> <ol> <li>Use codecov to track test coverage</li> <li>Use codecov to break builds when reducing by a given %</li> </ol> </li> <li> <p>Improve our typography - &lt;DEXP&gt; | Impact - 1 2 2 1 2 - **2 **| Cost -</p> <ol> <li>Remove old classes</li> </ol> </li> <li> <p>Improve our buttons - &lt;DEXP&gt; | Impact - 2 2 2 1 2 - **2 **| Cost -</p> <ol> <li>Audit our buttons in Storybook</li> <li>Audit our links in Storybook</li> <li>Update all our buttons and links to be consistent</li> <li>Add missing styles</li> </ol> </li> <li> <p>Improve our a11y - &lt;UT&gt; | Impact - 2 3 1 1 1 - **2 **| Cost -</p> <ol> <li>Audit a11y</li> <li>Fix a11y issues</li> <li>Setup automated testing to catch regressions</li> </ol> </li> <li> <p>Make the lists/tables consistent - &lt;DEXP&gt;, &lt;DP&gt; | Impact - 2 1 3 2 2 - 2 | Cost -</p> <ol> <li>Decide for one way of listing elements</li> <li>Roll it out all over the UI</li> </ol> </li> <li> <p>Fix all the ESLint warnings - &lt;DEXP&gt; | Impact - 2 2 2 2 -** 2** | Cost -</p> <ol> <li>Destructuring</li> <li>SetState issues</li> </ol> </li> <li> <p>Centralize URL logic - &lt;DEXP&gt; | Impact - 2 2 2 2 - 2 | Cost -</p> <ol> <li>Create util file with static URLs</li> <li>Extend with dynamic URLs</li> </ol> </li> <li> <p>Update React version - &lt;DEXP&gt; | Impact - 1 2 2 3 - 2 | Cost -</p> <ol> <li>Depends on moving to React Testing Library</li> <li>Update codebase using codemods</li> </ol> </li> <li> <p>Fix our fixture data - &lt;DEXP&gt; | Impact - 2 2 2 3 - 2 | Cost -</p> <ol> <li>Create test data builders for all our fixtures</li> <li>Create a debug mode to use the test data for manual testing of features</li> </ol> </li> <li> <p>Easy to use text component - &lt;DEXP&gt; | Impact - 1 1 3 1 3 - 2 | Cost -</p> </li> </ol>"},{"location":"frontend/docs/strategy/#8-appendix-b-why-we-got-here","title":"8. Appendix B: Why we got here","text":"<p>There is no single reason for the current state of our frontend codebase, however, we can relate to the following reasons (among others): * Lack of resources * Lack of OSS contributions to the UI * General Stagnation of the community</p>"},{"location":"frontend/docs/authentication/oidc/","title":"Oidc","text":"<p>See this doc in our main repository for information on how to set up end-to-end authentication using OIDC.</p>"},{"location":"frontend/docs/examples/announcement_client/","title":"Overview","text":"<p>Amundsen\u2019s announcement feature requires that developers create a custom implementation of <code>announcement_client</code> for collecting announcements. This feature provide ability to deliver to Users announcements of different sort regarding data discovery service.</p>"},{"location":"frontend/docs/examples/announcement_client/#implementation","title":"Implementation","text":"<p>Implement the <code>announcement_client</code> to make a request to system storing announcements.</p>"},{"location":"frontend/docs/examples/announcement_client/#shared-logic","title":"Shared Logic","text":"<p><code>announcement_client</code> implements <code>_get_posts()</code> of <code>base_announcement_client</code> with the minimal logic for this use case.</p> <p>It collects the posts from <code>get_posts()</code> method.</p> <pre><code>try:\n    announcements = self.get_posts()\nexcept Exception as e:\n    message = 'Encountered exception getting posts: ' + str(e)\n    return _create_error_response(message)\n</code></pre> <p>It verifies the shape of the data before returning it to the application. If the data does not match the <code>AnnouncementsSchema</code>, the request will fail.</p> <pre><code># validate the returned object\ndata, errors = AnnouncementsSchema().dump(announcements)\nif not errors:\n    payload = jsonify({'posts': data.get('posts'), 'msg': 'Success'})\n    return make_response(payload, HTTPStatus.OK)\nelse:\n    message = 'Announcement data dump returned errors: ' + str(errors)\n    return _create_error_response(message)\n</code></pre>"},{"location":"frontend/docs/examples/announcement_client/#custom-logic","title":"Custom Logic","text":"<p><code>announcement_client</code> has an abstract method <code>get_posts()</code>. </p> <p>This method will contain whatever custom logic is needed to collect announcements. The system within which they are stored can be anything that has programmatic access.</p> <p>Announcements could be collected from database (as in exemplary SQLAlchemyAnnouncementClient), kafka persistent topic, web rss feed, etc.</p> <p>See the following <code>example_announcement_client</code> for an example implementation of <code>base_announcement_client</code> and <code>get_posts()</code>. This example assumes a temporary sqlite database with no security, authentication, persistence or authorization configured.</p>"},{"location":"frontend/docs/examples/announcement_client/#usage","title":"Usage","text":"<p>Under the <code>[announcement_client]</code> group, point the <code>announcement_client</code> entry point in your local <code>setup.py</code> to your custom class.</p> <pre><code>entry_points=\"\"\"\n    ...\n\n    [announcement_client]\n    announcement_client_class = amundsen_application.base.examples.example_announcement_client:SQLAlchemyAnnouncementClient\n\"\"\"\n</code></pre>"},{"location":"frontend/docs/examples/announcement_client/#what-do-i-need-to-run-exemplary-announcement_client","title":"What do I need to run exemplary announcement_client ?","text":"<p>Exemplary client requires installation of SQLAlchemy to run properly:</p> <pre><code>pip install SQLAlchemy==1.3.17\n</code></pre> <p>Run <code>python3 setup.py install</code> in your virtual environment and restart the application for the entry point changes to take effect</p>"},{"location":"frontend/docs/examples/redash_preview_client/","title":"Overview","text":"<p>Amundsen\u2019s data preview feature requires that developers create a custom implementation of <code>base_preview_client</code> for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search.</p> <p>Redash is an open-source business intelligence tool that can be used for data exploration. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Redash for data previews.</p>"},{"location":"frontend/docs/examples/redash_preview_client/#implementation","title":"Implementation","text":"<p>You will need to complete the following steps to use Redash to serve your Amundsen data preview:</p> <ol> <li>Create a query template in Redash</li> <li>Generate an API key in Redash for Authentication</li> <li>Create a custom <code>RedashPreviewClient</code> class</li> <li>Update your <code>setup.py</code> and re-install the Amundsen frontend</li> </ol>"},{"location":"frontend/docs/examples/redash_preview_client/#1-creating-a-query-template-in-redash","title":"1. Creating a query template in Redash","text":"<p>In order for Amundsen to execute a query through Redash the query must already exist and be saved in Redash. Redash allows queries to be built using parameters and Amundsen uses these parameters to inject the <code>{schema}.{table}</code> into the query. When defining a new query the following SQL should be used:</p> <pre><code>select {{ SELECT_FIELDS }}\nfrom {{ SCHEMA_NAME }}.{{ TABLE_NAME }}\n{{ WHERE_CLAUSE }}\nlimit {{ RCD_LIMIT }}\n</code></pre> <p>Visit the Redash documentation on query parameters for more information on how query templating works.</p>"},{"location":"frontend/docs/examples/redash_preview_client/#2-generate-an-api-key-in-redash-for-authentication","title":"2. Generate an API key in Redash for Authentication","text":"<p>Redash uses API keys for authentication. While there are two types of API keys that Redash supports (User API keys and Query specific API keys), the Amundsen integration requires a User API key. The Query API keys do not support the dynamic templating that is required to build the queries.</p> <p>The API key used by Amundsen must have access to query the underlying table in Redash. By default, only a single API key is required for your Redash Preview Client in Amundsen. However, you may have different databases in Redash that require different access. The Redash Preview Client allows you to dynamically select which API key to use for authentication in the event you want more fine-grained control over your data preview access.</p> <p>To select an API key on the fly based off of the database, cluster, schema and table that is being queried, override the <code>_get_query_api_key</code> function (see using parameters] below for an example).</p>"},{"location":"frontend/docs/examples/redash_preview_client/#3-create-a-custom-redashpreviewclient-class","title":"3. Create a custom <code>RedashPreviewClient</code> class","text":"<p>The file <code>base_redash_preview_client</code> provides two examples for implementing the Redash preview client.</p> <ul> <li>One simple implementation that only implements the minimal requirements: a User API key and a way to look up which Redash query ID to execute</li> <li>One complex implementation which uses a different API key per query, increases the # of records returned to 100, reduces the max cache time in Redash to 1 hour, provides custom field masking in the query and creates custom where clauses for specific tables to filter the correct data returned to the preview client.</li> </ul>"},{"location":"frontend/docs/examples/redash_preview_client/#using-params-to-lookup-resources","title":"Using params to lookup resources","text":"<p>Several of the functions in <code>BaseRedashPreviewClient</code> have a single input: params and expect a single output (e.g. a query ID, query API key, custom where clause, etc.). These params are a dictionary that contain the following fields:</p> <pre><code>{\n    \"database\": \"snowflake\",\n    \"cluster\": \"ca_covid\",\n    \"schema\": \"open_data\",\n    \"tableName\": \"statewide_cases\"\n}\n</code></pre> <p>It is expected that these values can generally be used to uniquely reference your resources. For example, if you have two databases <code>snowflake.ca_covid</code> and <code>snowflake.marketing</code> the implementation to find the correct query ID may look like:</p> <pre><code>    ...\n\n    def get_redash_query_id(self, params: Dict) -&gt; Optional[int]:\n        SOURCE_DB_QUERY_MAP = {\n            'snowflake.open_data': 1,\n            'snowflake.marketing': 27,\n        }\n        database = params['database']\n        cluster = params['cluster']\n        db_cluster_key = f'{database}.{cluster}'\n        return SOURCE_DB_QUERY_MAP.get(db_cluster_key)\n</code></pre> <p>This same paradigm applies to the all funcitons with the same input signature: <code>get_redash_query_id</code>, <code>_get_query_api_key</code>, <code>get_select_fields</code>, and <code>get_where_clause</code>.</p>"},{"location":"frontend/docs/examples/redash_preview_client/#4-update-setuppy-and-reinstall-amundsen","title":"4. Update setup.py and reinstall Amundsen","text":"<p>In the <code>setup.py</code> at the root of this repo, add a <code>[preview_client]</code> group and point the <code>table_preview_client_class</code> to your custom class.</p> <pre><code>entry_points=\"\"\"\n    ...\n\n    [preview_client]\n    table_preview_client_class = amundsen_application.base.examples.example_redash_preview_client:RedashSimplePreviewClient\n\"\"\"\n</code></pre> <p>Run <code>python3 setup.py install</code> in your virtual environment and restart the application for the entry point changes to take effect.</p>"},{"location":"frontend/docs/examples/s3_preview_client/","title":"Overview","text":"<p>Amundsen\u2019s data preview feature requires that developers create a custom implementation of <code>base_preview_client</code> for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search.</p> <p>S3 is AWS block storage and is used in this scenario for storing precomputed preview data.</p>"},{"location":"frontend/docs/examples/s3_preview_client/#implementation","title":"Implementation","text":"<p>Implement the <code>base_s3_preview_client</code> to make a request to AWS using boto3 to fetch your preview data.  Your preview data needs to already be stored in S3 for this to work.  You can take a look at <code>example_s3_json_preview_client</code> to see a working implementation that fetches JSON files.</p>"},{"location":"frontend/docs/examples/s3_preview_client/#usage","title":"Usage","text":"<p>To use a preview client set these environment variables in your deployment.</p> <ul> <li><code>PREVIEW_CLIENT_ENABLED</code>: <code>true</code></li> <li><code>PREVIEW_CLIENT</code>: <code>{python path to preview client class}</code> (ex: <code>amundsen_application.base.examples.example_s3_json_preview_client.S3JSONPreviewClient</code> if you are using the JSON example client)</li> <li><code>PREVIEW_CLIENT_S3_BUCKET</code>: <code>{S3 bucket where the preview data is stored}</code></li> </ul>"},{"location":"frontend/docs/examples/superset_preview_client/","title":"Overview","text":"<p>Amundsen\u2019s data preview feature requires that developers create a custom implementation of <code>base_preview_client</code> for requesting that data. This feature assists with data discovery by providing the end user the option to view a sample of the actual resource data so that they can verify whether or not they want to transition into exploring that data, or continue their search.</p> <p>Apache Superset is an open-source business intelligence tool that can be used for data exploration. Amundsen\u2019s data preview feature was created with Superset in mind, and it is what we leverage internally at Lyft to support the feature. This document provides some insight into how to configure Amundsen\u2019s frontend application to leverage Superset for data previews.</p>"},{"location":"frontend/docs/examples/superset_preview_client/#implementation","title":"Implementation","text":"<p>Implement the <code>base_superset_preview_client</code> to make a request to an instance of Superset.</p>"},{"location":"frontend/docs/examples/superset_preview_client/#shared-logic","title":"Shared Logic","text":"<p><code>base_superset_preview_client</code> implements <code>get_preview_data()</code> of <code>base_preview_client</code> with the minimal logic for this use case.</p> <p>It updates the headers for the request if <code>optionalHeaders</code> are passed in <code>get_preview_data()</code> <pre><code># Clone headers so that it does not mutate instance's state\nheaders = dict(self.headers)\n\n# Merge optionalHeaders into headers\nif optionalHeaders is not None:\n    headers.update(optionalHeaders)\n</code></pre></p> <p>It verifies the shape of the data before returning it to the application. If the data does not match the <code>PreviewDataSchema</code>, the request will fail. <pre><code># Verify and return the results\nresponse_dict = response.json()\ncolumns = [ColumnItem(c['name'], c['type']) for c in response_dict['columns']]\npreview_data = PreviewData(columns, response_dict['data'])\ndata, errors = PreviewDataSchema().dump(preview_data)\nif not errors:\n    payload = jsonify({'preview_data': data})\n    return make_response(payload, response.status_code)\nelse:\n    return make_response(jsonify({'preview_data': {}}), HTTPStatus.INTERNAL_SERVER_ERROR)\n</code></pre></p>"},{"location":"frontend/docs/examples/superset_preview_client/#custom-logic","title":"Custom Logic","text":"<p><code>base_superset_preview_client</code> has an abstract method <code>post_to_sql_json()</code>. This method will contain whatever custom logic is needed to make a successful request to the <code>sql_json</code> enpoint based on the protections you have configured on this endpoint on your instance of Superset. For example, this may be where you have to append other values to the headers, or generate SQL queries based on your use case.</p> <p>See the following <code>example_superset_preview_client</code> for an example implementation of <code>base_superset_preview_client</code> and <code>post_to_sql_json()</code>. This example assumes a local instance of Superset running on port 8088 with no security, authentication, or authorization configured on the endpoint.</p>"},{"location":"frontend/docs/examples/superset_preview_client/#usage","title":"Usage","text":"<p>Under the <code>[preview_client]</code> group, point the <code>table_preview_client_class</code> entry point in your local <code>setup.py</code> to your custom class.</p> <pre><code>entry_points=\"\"\"\n    ...\n\n    [preview_client]\n    table_preview_client_class = amundsen_application.base.examples.example_superset_preview_client:SupersetPreviewClient\n\"\"\"\n</code></pre> <p>Run <code>python3 setup.py install</code> in your virtual environment and restart the application for the entry point changes to take effect</p>"},{"location":"installation-aws-ecs/aws-ecs-deployment/","title":"Deployment of non-production Amundsen on AWS ECS using aws-cli","text":"<p>The following is a set of instructions to run Amundsen on AWS Elastic Container Service. The current configuration is very basic but it is working. It is a migration of the docker-amundsen.yml to run on AWS ECS.</p>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#install-ecs-cli","title":"Install ECS CLI","text":"<p>The first step is to install ECS CLI, please follow the instructions from AWS documentation</p>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#get-your-access-and-secret-keys-from-iam","title":"Get your access and secret keys from IAM","text":"<pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsenfrontendlibrary/docs/instalation-aws-ecs\n$ export AWS_ACCESS_KEY_ID=xxxxxxxx\n$ export AWS_SECRET_ACCESS_KEY=xxxxxx\n$ export AWS_PROFILE=profilename\n</code></pre> <p>For the purpose of this instruction we used the tutorial on AWS documentation</p> <p>Enter the cloned directory:</p> <pre><code>cd amundsen/docs/installation-aws-ecs\n</code></pre>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-1-create-a-cluster-configuration","title":"STEP 1: Create a cluster configuration:","text":"<pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsenfrontendlibrary/docs/instalation-aws-ecs\n$ ecs-cli configure --cluster amundsen --region us-west-2 --default-launch-type EC2 --config-name amundsen\n</code></pre>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-2-create-a-profile-using-your-access-key-and-secret-key","title":"STEP 2: Create a profile using your access key and secret key:","text":"<pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsen/docs/installation-aws-ecs\n$ ecs-cli configure profile --access-key $AWS_ACCESS_KEY_ID --secret-key $AWS_SECRET_ACCESS_KEY --profile-name amundsen\n</code></pre>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-3-create-the-cluster-use-profile-name-from-awscredentials","title":"STEP 3: Create the Cluster Use profile name from \\~/.aws/credentials","text":"<pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsen/docs/installation-aws-ecs\n$ ecs-cli up --keypair JoaoCorreia --extra-user-data userData.sh --capability-iam --size 1 --instance-type t2.large --cluster-config amundsen --verbose --force --aws-profile $AWS_PROFILE\n</code></pre>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-4-deploy-the-compose-file-to-a-cluster","title":"STEP 4: Deploy the Compose File to a Cluster","text":"<pre><code># in ~/&lt;your-path-to-cloned-repo&gt;/amundsen/docs/installation-aws-ecs\n$ ecs-cli compose --cluster-config amundsen --file docker-ecs-amundsen.yml up --create-log-groups\n</code></pre> <p>You can use the ECS CLI to see what tasks are running.</p> <pre><code>$ ecs-cli ps\n</code></pre>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#step-5-open-the-ec2-instance","title":"STEP 5 Open the EC2 Instance","text":"<p>Edit the Security Group to allow traffic to your IP, you should be able to see the frontend, elasticsearch and neo4j by visiting the URLs:</p> <ul> <li>http://xxxxxxx:5000/</li> <li>http://xxxxxxx:9200/</li> <li>http://xxxxxxx:7474/browser/</li> </ul>"},{"location":"installation-aws-ecs/aws-ecs-deployment/#todo","title":"TODO","text":"<ul> <li>Configuration sent to services not working properly (amunsen.db vs graph.db)</li> <li>Create a persistent volume for graph/metadata storage. See this</li> <li>Refactor the VPC and default security group permissions</li> </ul>"},{"location":"metadata/","title":"Amundsen Metadata Service","text":"<p>Amundsen Metadata service serves Restful API and is responsible for providing and also updating metadata, such as table &amp; column description, and tags. Metadata service can use Neo4j, Apache Atlas, AWS Neptune Or Mysql RDS as a persistent layer.</p> <p>For information about Amundsen and our other services, refer to this README.md. Please also see our instructions for a quick start setup  of Amundsen with dummy data, and an overview of the architecture.</p>"},{"location":"metadata/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> </ul>"},{"location":"metadata/#doc","title":"Doc","text":"<ul> <li>https://www.amundsen.io/amundsen/</li> </ul>"},{"location":"metadata/#instructions-to-start-the-metadata-service-from-distribution","title":"Instructions to start the Metadata service from distribution","text":"<pre><code>$ venv_path=[path_for_virtual_environment]\n$ python3 -m venv $venv_path\n$ source $venv_path/bin/activate\n$ pip3 install amundsen-metadata\n$ python3 metadata_service/metadata_wsgi.py\n\n-- In a different terminal, verify getting HTTP/1.0 200 OK\n$ curl -v http://localhost:5002/healthcheck\n</code></pre>"},{"location":"metadata/#instructions-to-start-the-metadata-service-from-the-source","title":"Instructions to start the Metadata service from the source","text":"<pre><code>$ git clone https://github.com/amundsen-io/amundsenmetadatalibrary.git\n$ cd amundsenmetadatalibrary\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip3 install -e \".[all]\" .\n$ python3 metadata_service/metadata_wsgi.py\n\n-- In a different terminal, verify getting HTTP/1.0 200 OK\n$ curl -v http://localhost:5002/healthcheck\n</code></pre>"},{"location":"metadata/#instructions-to-start-the-service-from-docker","title":"Instructions to start the service from Docker","text":"<pre><code>$ docker pull amundsendev/amundsen-metadata:latest\n$ docker run -p 5002:5002 amundsendev/amundsen-metadata\n# - alternative, for production environment with Gunicorn (see its homepage link below)\n$ ## docker run -p 5002:5002 amundsendev/amundsen-metadata gunicorn --bind 0.0.0.0:5002 metadata_service.metadata_wsgi\n\n-- In a different terminal, verify getting HTTP/1.0 200 OK\n$ curl -v http://localhost:5002/healthcheck\n</code></pre>"},{"location":"metadata/#production-environment","title":"Production environment","text":"<p>By default, Flask comes with Werkzeug webserver, which is for development. For production environment use production grade web server such as Gunicorn.</p> <p><pre><code>$ pip install gunicorn\n$ gunicorn metadata_service.metadata_wsgi\n</code></pre> Here is documentation of gunicorn configuration.</p>"},{"location":"metadata/#configuration-outside-local-environment","title":"Configuration outside local environment","text":"<p>By default, Metadata service uses LocalConfig that looks for Neo4j running in localhost. In order to use different end point, you need to create Config suitable for your use case. Once config class has been created, it can be referenced by environment variable: <code>METADATA_SVC_CONFIG_MODULE_CLASS</code></p> <p>For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in metadata_service.config module. then you can set as below:</p> <p><code>METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.ProdConfig</code></p> <p>This way Metadata service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc.</p>"},{"location":"metadata/#apache-atlas","title":"Apache Atlas","text":"<p>Amundsen Metadata service can use Apache Atlas as a backend. Some of the benefits of using Apache Atlas instead of Neo4j is that Apache Atlas offers plugins to several services (e.g. Apache Hive, Apache Spark) that allow for push based updates. It also allows to set policies on what metadata is accessible and editable by means of Apache Ranger.</p> <p>Apache Atlas is a data governance service meaning you also get Apache Atlas UI for easy access to your metadata. It is, however, aimed for administrators rather then end-users, which we suggest to direct towards Amundsen UI.</p> <p>Apache Atlas is so far the only proxy in Amundsen supporting both push and pull for collecting metadata: - <code>Push</code> method by leveraging Apache Atlas Hive Hook. It\u2019s an event listener running alongside Hive Metastore, translating Hive Metastore events into Apache Atlas entities and <code>pushing</code> them to Kafka topic, from which Apache Atlas ingests the data by internal processes. - <code>Pull</code> method by leveraging Amundsen Databuilder integration with Apache Atlas. It means that extractors available in Databuilder can be used to collect metadata about external systems (like PostgresMetadataExtractor) and sending them to Apache Atlas in a shape consumable by Amundsen.</p> <p>If you would like to use Apache Atlas as a backend for Metadata service you will need to create a Config as mentioned above. Make sure to include the following:</p> <pre><code>PROXY_CLIENT = PROXY_CLIENTS['ATLAS'] # or env PROXY_CLIENT='ATLAS'\nPROXY_PORT = 21000          # or env PROXY_PORT\nPROXY_USER = 'atlasuser'    # or env CREDENTIALS_PROXY_USER\nPROXY_PASSWORD = 'password' # or env CREDENTIALS_PROXY_PASSWORD\n</code></pre> <p>To start the service with Atlas from Docker. Make sure you have <code>atlasserver</code> configured in DNS (or docker-compose)</p> <pre><code>$ docker run -p 5002:5002 --env PROXY_CLIENT=ATLAS --env PROXY_PORT=21000 --env PROXY_HOST=atlasserver --env CREDENTIALS_PROXY_USER=atlasuser --env CREDENTIALS_PROXY_PASSWORD=password amundsen-metadata:latest\n</code></pre> <p>NOTE</p> <p>While Apache Atlas supports fine grained access, Amundsen does not support this yet.</p>"},{"location":"metadata/#developer-guide","title":"Developer guide","text":""},{"location":"metadata/#code-style","title":"Code style","text":"<ul> <li>PEP 8: Amundsen Metadata service follows PEP8 - Style Guide for Python Code.</li> <li>Typing hints: Amundsen Metadata service also utilizes Typing hint for better readability.</li> </ul>"},{"location":"metadata/#api-documentation","title":"API documentation","text":"<p>We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger. When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to localhost:5002/apidocs/. Currently the documentation only works with local configuration.</p>"},{"location":"metadata/#code-structure","title":"Code structure","text":"<p>Please visit Code Structure to read how different modules are structured in Amundsen Metadata service.</p>"},{"location":"metadata/#roundtrip-tests","title":"Roundtrip tests","text":"<p>Roundtrip tests are a new feature - by implementing the abstract_proxy_tests and some test setup endpoints in the base_proxy, you can validate your proxy code against the actual data store. These tests do not run by default, but can be run by passing the <code>--roundtrip-[proxy]</code> argument. Note this requires a fully-configured backend to test against. <pre><code>$ python -m pytest --roundtrip-neptune .\n</code></pre></p>"},{"location":"metadata/CHANGELOG/","title":"CHANGELOG","text":""},{"location":"metadata/CHANGELOG/#feature","title":"Feature","text":"<ul> <li>Create users in metadata backend via API (#289) (<code>eeba485</code>)</li> <li>Add Column Badge API (#273) (<code>ee0ac63</code>)</li> <li>Column Lineage API (#280) (<code>681893f</code>)</li> <li>Table Lineage API (#262) (<code>e306034</code>)</li> <li>Column badges in Atlas Proxy (#263) (<code>a3efb4c</code>)</li> <li>Added get_lineage method to neo4j proxy (#259) (<code>b129cc7</code>)</li> <li>Neo4j backend for popular tables personalization (#233) (<code>d045efa</code>)</li> <li>Updated popular_tables endpoint to allow optional user_id (#232) (<code>5680775</code>)</li> <li>AddSwaggerEnabledAsEnvVar (#215) (<code>3c9a55e</code>)</li> <li>Add neptune proxy (#204) (<code>09845d4</code>)</li> <li>Return column level badges (#205) (<code>d4d8101</code>)</li> <li>Badges with category and badge_type fields (#201) (<code>19e1bf8</code>)</li> <li>Get last updated ts for AtlasProxy (#177) (<code>3e92586</code>)</li> <li>Data Owner Implementation of Atlas Proxy (#156) (<code>48b4c71</code>)</li> </ul>"},{"location":"metadata/CHANGELOG/#fix","title":"Fix","text":"<ul> <li>Sort reports alphabetically (#293) (<code>c8423c6</code>)</li> <li>Reconcile gremlin description writes with the Databuilder (#290) (<code>18454fe</code>)</li> <li>Swagger docs don\u2019t align with common entity (#283) (<code>db33af9</code>)</li> <li>Compatibility changes  to the gremlin integration (#260) (<code>a765424</code>)</li> <li>Proxy client creation fails after adding client_kwargs (#258) (<code>1880cec</code>)</li> <li>Reapply previous fix (#245) (<code>c7dc172</code>)</li> <li>Expire is a keyword argument for beaker cache (#239) (<code>a7b2ec5</code>)</li> <li>Get_tags no longer return tag_count 0 tag (#230) (<code>5097d2b</code>)</li> <li>Too many partitions in one Atlas query (Watermarks in Atlas Proxy) (#217) (<code>cc3768f</code>)</li> <li>Data_type as Column.col_type. (#203) (<code>7b97f62</code>)</li> <li>Add /delete bidirectional relations in case of owner (#206) (<code>40cd0dd</code>)</li> <li>Removed all badge_type fields from API (#202) (<code>6a81b97</code>)</li> <li>Improvements to the Owned By feature (#178) (<code>0558d69</code>)</li> <li>Get all tags should work for all resources (#175) (<code>cf1ab6d</code>)</li> <li>Removing OidcConfig file and making statsd configurable through envrionment variable (#157) (<code>2752492</code>)</li> </ul>"},{"location":"metadata/CHANGELOG/#documentation","title":"Documentation","text":"<ul> <li>Enumeration requires newline (#291) (<code>cf6710d</code>)</li> </ul>"},{"location":"metadata/docs/configurations/","title":"Overview","text":"<p>Most of the configurations are set through Flask Config Class.</p>"},{"location":"metadata/docs/configurations/#badges","title":"BADGES","text":"<p>In order to add a badge to a resource you should first add the combination of badge name and category to the in WHITELIST_BADGES Config Class.</p> <p>Example: <pre><code>WHITELIST_BADGES: List[Badge] = [Badge(badge_name='beta',\n                                 category='table_status')]\n</code></pre></p> <p>Once this is done users will be able to add badge the badges in the whitelist by running:</p> <p><code>curl -X PUT https://{amundsen metadata url}/table/\"{table key}\"/badge/{badge_name}?category={category}</code></p>"},{"location":"metadata/docs/configurations/#user_detail_method-optional","title":"USER_DETAIL_METHOD <code>OPTIONAL</code>","text":"<p>This is a method that can be used to get the user details from any third-party or custom system. This custom function takes user_id as a parameter, and returns a dictionary consisting user details\u2019 fields defined in UserSchema. </p> <p>Example: <pre><code>def get_user_details(user_id):\n    user_info = {\n        'email': 'test@email.com',\n        'user_id': user_id,\n        'first_name': 'Firstname',\n        'last_name': 'Lastname',\n        'full_name': 'Firstname Lastname',\n    }\n    return user_info\n\nUSER_DETAIL_METHOD = get_user_details\n</code></pre></p>"},{"location":"metadata/docs/configurations/#statistics_format_spec-optional","title":"STATISTICS_FORMAT_SPEC <code>OPTIONAL</code>","text":"<p>This is a variable enabling possibility to reformat statistics displayed in UI.</p> <p>The key is name of statistic and a value is a dictionary with optional keys: * new_name - how to rename statistic (if absent proxy should default to old name) * format - how to format numerical statistics (if absent, proxy should default to original format) * drop - should given statistic not be displayed in UI (if absent, proxy should keep it)</p> <p>Example (if you\u2019re using deeque library), you might want to: <pre><code>STATISTICS_FORMAT_SPEC = {\n        'stdDev': dict(new_name='standard deviation', format='{:,.2f}'),\n        'mean': dict(format='{:,.2f}'),\n        'maximum': dict(format='{:,.2f}'),\n        'minimum': dict(format='{:,.2f}'),\n        'completeness': dict(format='{:.2%}'),\n        'approximateNumDistinctValues': dict(new_name='distinct values', format='{:,.0f}', ),\n        'sum': dict(drop=True)\n}\n</code></pre></p>"},{"location":"metadata/docs/structure/","title":"Metadata API Structure","text":"<p>Amundsen metadata service consists of three packages, API, Entity, and Proxy.</p>"},{"location":"metadata/docs/structure/#api-package","title":"API package","text":"<p>A package that contains Flask Restful resources that serves Restful API request. The routing of API is being registered here.</p>"},{"location":"metadata/docs/structure/#proxy-package","title":"Proxy package","text":"<p>Proxy package contains proxy modules that talks dependencies of Metadata service. There are currently three modules in Proxy package,  Neo4j,  Statsd and Atlas</p> <p>Selecting the appropriate proxy (Neo4j or Atlas) is configurable using a config variable <code>PROXY_CLIENT</code>,  which takes the path to class name of proxy module available here.</p> <p>Note: Proxy\u2019s host and port are configured using config variables <code>PROXY_HOST</code> and <code>PROXY_PORT</code> respectively.  Both of these variables can be set using environment variables. </p>"},{"location":"metadata/docs/structure/#neo4j-proxy-module","title":"Neo4j proxy module","text":"<p>Neo4j proxy module serves various use case of getting metadata or updating metadata from or into Neo4j. Most of the methods have Cypher query for the use case, execute the query and transform into entity.</p>"},{"location":"metadata/docs/structure/#apache-atlas-proxy-module","title":"Apache Atlas proxy module","text":"<p>Apache Atlas proxy module serves all of the metadata from Apache Atlas, using apache_atlas.  More information on how to setup Apache Atlas to make it compatible with Amundsen can be found here </p>"},{"location":"metadata/docs/structure/#statsd-utilities-module","title":"Statsd utilities module","text":"<p>Statsd utilities module has methods / functions to support statsd to publish metrics. By default, statsd integration is disabled and you can turn in on from Metadata service configuration. For specific configuration related to statsd, you can configure it through environment variable.</p>"},{"location":"metadata/docs/structure/#entity-package","title":"Entity package","text":"<p>Entity package contains many modules where each module has many Python classes in it. These Python classes are being used as a schema and a data holder. All data exchange within Amundsen Metadata service use classes in Entity to ensure validity of itself and improve readability and mainatability.</p>"},{"location":"metadata/docs/structure/#configurations","title":"Configurations","text":"<p>There are different settings you might want to change depending on the application environment like toggling the debug mode, setting the proxy, and other such environment-specific things.</p>"},{"location":"metadata/docs/proxy/atlas_proxy/","title":"Atlas Proxy","text":"<p>In order to make the Atlas-Amundsen integration smooth, we\u2019ve developed a python package,  amundsenatlastypes that has all the required entity definitions along with helper functions needed to make Atlas compatible with Amundsen.</p> <p>Usage and Installation of <code>amundsenatlastypes</code> can be found here</p> <p>Minimum Requirements:</p> <ul> <li>amundsenatlastypes==1.2.0</li> <li>apache_atlas==0.0.11</li> </ul>"},{"location":"metadata/docs/proxy/atlas_proxy/#configurations","title":"Configurations","text":"<p>Once you are done with setting up required entity definitions using amundsenatlastypes, you are all set to use Atlas with Amundsen.</p> <p>Other things to configure:</p> <ul> <li>Popular Tables</li> </ul>"},{"location":"metadata/docs/proxy/gremlin/","title":"Gremlin Proxy","text":""},{"location":"metadata/docs/proxy/gremlin/#what-the-heck-is-gremlin-why-is-it-named-gremlin","title":"What the heck is Gremlin?  Why is it named Gremlin?","text":"<p>Gremin is the graph traversal language of Apache TinkerPop.  Why not Gremlin?</p>"},{"location":"metadata/docs/proxy/gremlin/#documentation","title":"Documentation","text":"<p>The docs linked from Gremin are a good start.   For example, the Getting Started and the PRACTICAL GREMLIN book</p>"},{"location":"metadata/docs/proxy/gremlin/#how-to-target-a-new-gremlin-backend","title":"How to target a new Gremlin backend","text":"<p>This is not an exhaustive list, but some issues we\u2019ve found along the way:   - Are there restricted property names?  For example JanusGraph does not allow a property named   <code>key</code>, so the base Gremlin proxy has a property named <code>key_property_name</code> which is set to <code>_key</code>   for JanusGraph but <code>key</code> for others.   - Is there database management required?  For example AWS Neptune does now allow explicit creation   of indexes, nor assigning data types to properties, but JanusGraph does and practically requires   the creation of indexes.   - Are there restrictions on the methods? For example, JanusGraph accepts any of the Java or Groovy   names, but Neptune accepts a strict subset. JanusGraph can install any script engine, e.g. to   allow Python lambdas but Neptune only allows Groovy lambdas.</p> <p>Other differences between Janusgraph and Neptune can be found here: https://docs.aws.amazon.com/neptune/latest/userguide/access-graph-gremlin-differences.html</p>"},{"location":"metadata/docs/proxy/mysql/","title":"MySQL Proxy","text":"<p>MySQL proxy works with SQLAlchemy ORM and Amundsen RDS containing all the ORM models used for Amundsen databuilder and metadataservice.</p>"},{"location":"metadata/docs/proxy/mysql/#requirements","title":"Requirements","text":"<p>MySQL &gt;= 5.7</p>"},{"location":"metadata/docs/proxy/mysql/#schema-migration","title":"Schema migration","text":"<p>Before running MySQL as the Amundsen metadata store, we need initialize/upgrade all the table schemas first.  The schema migration is managed by cli in metadata service and Alembic behind. </p>"},{"location":"metadata/docs/proxy/mysql/#add-config-for-mysql","title":"Add config for MySQL","text":"<p><pre><code>PROXY_CLI = PROXY_CLIS['MYSQL']\nSQLALCHEMY_DATABASE_URI = os.environ.get('SQLALCHEMY_DATABASE_URI', {mysql_connection_string})\n# other fileds used for mysql proxy\n......\n</code></pre> see metadata config</p>"},{"location":"metadata/docs/proxy/mysql/#add-environment-variables","title":"Add environment variables","text":"<p>Specify how to load the application: <code>export FLASK_APP=metadata_service/metadata_wsgi.py</code></p>"},{"location":"metadata/docs/proxy/mysql/#run-migration-commands","title":"Run migration commands","text":"<p>If we need initialize/upgrade db: <code>flask rds initdb</code></p> <p>If we need reset db:  <code>flask rds resetdb --yes</code> or <code>flask rds resetdb</code> (then enter y after a prompt message.) </p>"},{"location":"metadata/docs/proxy/neptune/","title":"Neptune","text":""},{"location":"metadata/docs/proxy/neptune/#documentation","title":"Documentation","text":"<p>In particular, see Gremlin differences, and Gremlin sessions.</p> <p>And any time you see docs from Kelvin (like the PRACTICAL GREMLIN book or lots of stackoverflow) pay attention, he works for AWS on Neptune.</p>"},{"location":"metadata/docs/proxy/neptune/#iam-authentication","title":"IAM authentication","text":"<p>The gremlin transport is usually websockets, and the requests-aws4auth library we use elsewhere is for requests, which does not support websockets at all.  So we rolled our in <code>aws4authwebsocket</code>. The saving grace of websockets and IAM is that the IAM authentication really only applies to the initialization request and the rest of the data flows over the existing TCP connection.  The usual gremlin-python transport is Tornado, which was a huge pain to try and insinuate the aws4 autentication in to, so we use the websockets-client library instead.</p>"},{"location":"metadata/docs/proxy/neptune/#how-to-get-a-gremlin-console-for-aws","title":"How to get a gremlin console for AWS","text":"<p>They have pretty decent recipe here</p>"},{"location":"metadata/docs/proxy/atlas/popular_tables/","title":"Popular Tables Configurations","text":"<p>The required entity definitions for Atlas can be applied using amundsenatlastypes. </p>"},{"location":"metadata/docs/proxy/atlas/popular_tables/#popular-tables","title":"Popular Tables","text":"<p>Amundsen has a concept of popular tables, which is a default entry point of the application for now.  Popular Tables API leverages <code>popularityScore</code> attribute of <code>Table</code> super type to enable custom sorting strategy.</p> <p>The suggested formula to generate the popularity score is provided below and should be applied by the external script or batch/stream process to update Atlas entities accordingly. <pre><code>Popularity score = number of distinct readers * log(total number of reads)\n</code></pre></p> <p><code>Table</code> entity definition with <code>popularityScore</code> attribute amundsenatlastypes==1.2.0. </p> <pre><code>    {\n      \"entityDefs\": [\n        {\n          \"name\": \"Table\",\n          \"superTypes\": [\n            \"DataSet\"\n          ],\n          \"attributeDefs\": [\n            {\n              \"name\": \"popularityScore\",\n              \"typeName\": \"float\",\n              \"isOptional\": true,\n              \"cardinality\": \"SINGLE\",\n              \"isUnique\": false,\n              \"isIndexable\": false,\n              \"defaultValue\": \"0.0\"\n            },\n            {\n              \"name\": \"readers\",\n              \"typeName\": \"array&lt;Reader&gt;\",\n              \"isOptional\": true,\n              \"cardinality\": \"LIST\",\n              \"isUnique\": false,\n              \"isIndexable\": true,\n              \"includeInNotification\": false\n            }\n          ]\n        }\n      ]\n    }\n</code></pre>"},{"location":"search/","title":"Amundsen Search Service","text":"<p>The latest enpoints for search and document updating are <code>/v2/search</code> and <code>/v2/document</code>. This API is only usable with the proxy class ElaticsearchProxyV2_1. Please refer to this search service updating doc to understand the latest search service changes.</p> <p>Amundsen Search service serves a Restful API and is responsible for searching metadata. The service leverages Elasticsearch for most of it\u2019s search capabilites.</p> <p>The search service searches through 4 indexes by default: * table_search_index_v2_1 * user_search_index_v2_1 * dashboard_search_index_v2_1 * feature_search_index_v2_1</p> <p>For information about Amundsen and our other services, refer to this README.md. Please also see our instructions for a quick start setup  of Amundsen with dummy data, and an overview of the architecture.</p>"},{"location":"search/#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.8</li> <li>Elasticsearch, supported versions:<ul> <li>7.x</li> <li>8.0.0</li> </ul> </li> </ul>"},{"location":"search/#doc","title":"Doc","text":"<ul> <li>https://www.amundsen.io/amundsen</li> </ul>"},{"location":"search/#instructions-to-start-the-search-service-from-distribution","title":"Instructions to start the Search service from distribution","text":"<pre><code>$ venv_path=[path_for_virtual_environment]\n$ python3 -m venv $venv_path\n$ source $venv_path/bin/activate\n$ python3 setup.py install\n$ python3 search_service/search_wsgi.py\n\n# In a different terminal, verify the service is up by running\n$ curl -v http://localhost:5001/healthcheck\n</code></pre>"},{"location":"search/#instructions-to-start-the-search-service-from-source","title":"Instructions to start the Search service from source","text":"<pre><code>$ git clone https://github.com/amundsen-io/amundsen.git\n$ cd search\n$ venv_path=[path_for_virtual_environment]\n$ python3 -m venv $venv_path\n$ source $venv_path/bin/activate\n$ pip3 install -e \".[all]\" .\n$ python3 search_service/search_wsgi.py\n\n# In a different terminal, verify the service is up by running\n$ curl -v http://localhost:5001/healthcheck\n</code></pre>"},{"location":"search/#instructions-to-start-the-service-from-docker","title":"Instructions to start the service from Docker","text":"<pre><code>$ docker pull amundsendev/amundsen-search:latest\n$ docker run -p 5001:5001 amundsendev/amundsen-search\n# - alternative, for production environment with Gunicorn (see its homepage link below)\n$ ## docker run -p 5001:5001 amundsendev/amundsen-search gunicorn --bind 0.0.0.0:5001 search_service.search_wsgi\n\n# In a different terminal, verify the service is up by running\n$ curl -v http://localhost:5001/healthcheck\n</code></pre>"},{"location":"search/#production-environment","title":"Production environment","text":"<p>By default, Flask comes with a Werkzeug webserver, which is used for development. For production environments a production grade web server such as Gunicorn should be used.</p> <p><pre><code>$ pip3 install gunicorn\n$ gunicorn search_service.search_wsgi\n\n# In a different terminal, verify the service is up by running\n$ curl -v http://localhost:8000/healthcheck\n</code></pre> For more imformation see the Gunicorn configuration documentation.</p>"},{"location":"search/#configuration-outside-local-environment","title":"Configuration outside local environment","text":"<p>By default, Search service uses LocalConfig that looks for Elasticsearch running in localhost. In order to use different end point, you need to create a Config suitable for your use case. Once a config class has been created, it can be referenced by an environment variable: <code>SEARCH_SVC_CONFIG_MODULE_CLASS</code></p> <p>For example, in order to have different config for production, you can inherit Config class, create Production config and passing production config class into environment variable. Let\u2019s say class name is ProdConfig and it\u2019s in search_service.config module. then you can set as below:</p> <p><code>SEARCH_SVC_CONFIG_MODULE_CLASS=search_service.config.ProdConfig</code></p> <p>This way Search service will use production config in production environment. For more information on how the configuration is being loaded and used, here\u2019s reference from Flask doc.</p>"},{"location":"search/#developer-guide","title":"Developer guide","text":""},{"location":"search/#code-style","title":"Code style","text":"<ul> <li>PEP 8: Amundsen Search service follows PEP8 - Style Guide for Python Code. </li> <li>Typing hints: Amundsen Search service also utilizes Typing hint for better readability.</li> </ul>"},{"location":"search/#api-documentation","title":"API documentation","text":"<p>We have Swagger documentation setup with OpenApi 3.0.2. This documentation is generated via Flasgger.  When adding or updating an API please make sure to update the documentation. To see the documentation run the application locally and go to <code>localhost:5001/apidocs/</code>.  Currently the documentation only works with local configuration. </p>"},{"location":"search/CHANGELOG/","title":"CHANGELOG","text":""},{"location":"search/CHANGELOG/#260","title":"2.6.0","text":""},{"location":"search/CHANGELOG/#feature","title":"Feature","text":"<ul> <li>Search service adding suport for AWS Elasticsearch (https://github.com/amundsen-io/amundsen/pull/1141)</li> <li>use chart_names in dashboard basic search (https://github.com/amundsen-io/amundsen/pull/1140)</li> <li>Atlas Dashboard Support (https://github.com/amundsen-io/amundsen/pull/1066)</li> </ul>"},{"location":"search/CHANGELOG/#fix","title":"Fix","text":"<ul> <li>remove a comment in the Search Service user model get_id function that conflicts with implementation (https://github.com/amundsen-io/amundsen/pull/1154)</li> <li>search_table API swagger file bug fix (https://github.com/amundsen-io/amundsen/pull/1120)</li> <li>aws config (https://github.com/amundsen-io/amundsen/pull/1167)</li> </ul>"},{"location":"search/CHANGELOG/#chore","title":"Chore","text":"<ul> <li>bump marshmallow to 3 (https://github.com/amundsen-io/amundsensearchlibrary/pull/192)</li> <li>refactor: shared dependencies unification (https://github.com/amundsen-io/amundsen/pull/1163)</li> </ul>"},{"location":"search/CHANGELOG/#251-and-before","title":"2.5.1 and before","text":""},{"location":"search/CHANGELOG/#feature_1","title":"Feature","text":"<ul> <li>Add dashboard search filter support (#112) (<code>17c6739</code>)</li> <li>Use query parameter for basic search [AtlasProxy] (#105) (<code>9961fde</code>)</li> </ul>"},{"location":"search/CHANGELOG/#fix_1","title":"Fix","text":"<ul> <li>Add id field to Dashboard (#174) (<code>53634fa</code>)</li> <li>Fix table post/put api bug (#172) (<code>38bccba</code>)</li> <li>Fix dashboard model errors, change deprecated pytest function (#160) (<code>1304234</code>)</li> </ul>"},{"location":"search/CHANGELOG/#documentation","title":"Documentation","text":"<ul> <li>Fix build and coverage status links in README.md (#134) (<code>1c95ff4</code>)</li> </ul>"},{"location":"tutorials/badges/","title":"How to add table level and column level badges","text":"<p>Amundsen supports use of clickable badges on tables, and non clickable badges for columns. Clickable badges trigger a search for all of the resources with the given badge name as a filter.</p> <p> Table badge</p> <p> Column badge</p>"},{"location":"tutorials/badges/#badges-configuration","title":"Badges configuration","text":"<p>In order for amundsen to accept new badges via metadata and to change the style in the UI there are two configs that need to be setup:</p> <p>On amundsen metadata library you should add your badges to the whitelist within your custom configuration file following the format of this example:</p> <pre><code># whitelist badges\nWHITELIST_BADGES: List[Badge] = [\n    Badge(badge_name='alpha',\n          category='table_status'),\n    Badge(badge_name='beta',\n          category='table_status'),\n]\n</code></pre> <p>In order to set up the color and display name on amundsen frontend library you should add the desired badges style as follows:</p> <pre><code>const configCustom: AppConfigCustom = {\n    badges: {\n        'alpha': {\n            style: BadgeStyle.DEFAULT,\n            displayName: 'Alpha',\n        },\n        'partition column': {\n            style: BadgeStyle.DEFAULT,\n            displayName: 'Partition Column',\n        },\n    }\n}\n</code></pre> <p>Note: any badges that are not defined in this configuration will show up with <code>BadgeStyle.DEFAULT</code>.</p>"},{"location":"tutorials/badges/#adding-table-badges-through-metadata-library","title":"Adding table badges through metadata library","text":"<p>To manually add a badge to a particular table the metadata API can be used. Here are the available requests:</p> <p>To add a badge on a table: <pre><code>curl -X PUT https://{your metadata url}/table/{table key}/badge/{badge name}?category={badge category}\n</code></pre></p> <p>To delete a badge on a table: <pre><code>curl -X DELETE https://{your metadata url}/table/{table key}/badge/{badge name}?category={badge category}\n</code></pre></p> <p>To add a badge on a column: <pre><code>curl -X PUT https://{your metadata url}/table/{table key}/column/{column name}/badge/{badge name}?category={badge category}\n</code></pre></p> <p>To delete a badge on a column: <pre><code>curl -X DELETE https://{your metadata url}/table/{table key}/column/{column name}/badge/{badge name}?category={badge category}\n</code></pre></p> <p><code>table key</code> is in the format of <code>datasource://database.schema/table</code></p>"},{"location":"tutorials/badges/#adding-badges-throught-databuilder-and-column-level-badges","title":"Adding badges throught databuilder (and column level badges)","text":"<p>To add badges using databuilder, you can use the BadgeMetadata class and pass in the entity you want to create a badge relationship for. For an example of how this is done search for badge in TableMetadata to see how we add badge nodes and relationships to neo4j. In hive_table_metadata_extractor.py you can see how the partition column is obtained and added to a column so the badge node can be created and related to the correct column.</p>"},{"location":"tutorials/data-preview-with-superset/","title":"How to setup a preview client with Apache Superset","text":"<p>In the previous tutorial, we talked about how to index the table metadata for a postgres database. In this tutorial, we will walk through how to configure data preview for this <code>films</code> table using Apache Superset.</p> <p>Amundsen provides an integration between Amundsen and BI Viz tool for data preview. It is not necessary to use Apache Superset as long as the BI Viz tool provides endpoint to do querying and get the results back from the BI tool. Apache Superset is an open-source business intelligence tool that can be used for data exploration and it is what we leverage internally at Lyft to support the feature.</p> <ol> <li>Please setup Apache Superset following its official installation guide:    <pre><code> # Install superset\n pip install apache-superset\n\n # Initialize the database\n superset db upgrade\n\n # Create an admin user (you will be prompted to set a username, first and last name before setting a password)\n $ export FLASK_APP=superset\n superset fab create-admin\n\n # Load some data to play with\n superset load_examples\n\n # Create default roles and permissions\n superset init\n\n # To start a development web server on port 8088, use -p to bind to another port\n superset run -p 8088 --with-threads --reload --debugger\n</code></pre></li> </ol> <p>Once setup properly, you could view the superset UI as following:    </p> <ol> <li> <p>We need to add the postgres database to superset as the following: </p> </li> <li> <p>We could verify the content of the <code>films</code> table using superset\u2019s sqlab feature: </p> </li> <li> <p>Next, We need to build a preview client following this guide and the example client code. There are a couple of things to keep in mind:</p> <ul> <li>We could start with an unauthenticated Superset(example superset config), but in production, we will need to send the impersonate info to Superset to properly verify whether the given user could view the data.</li> <li>When we build the client, we could need to configure the database id instead of the database name when send the request to superset.</li> </ul> </li> <li> <p>Once we configure the preview client, put it in the frontend service entry point (example) and restart the frontend.</p> </li> <li> <p>We could now view the preview data for the <code>films</code> table in Amundsen.  From the above figure, the preview button on the table page is clickable. Once it clicked, you could see the actual data queried from Apache Superset: </p> </li> </ol>"},{"location":"tutorials/how-to-track-user-metric/","title":"How to track Amundsen user metric","text":"<p>After you have deployed Amundsen into production, you want to track how user interacts with Amundsen for various reasons.</p> <p>The easier way is to leverage Google Analytics for basic user tracking. You could first get the analytics token for your domain and put it as the frontend config</p> <p>Besides implementing Google Analytics, we provide a way called <code>action_logging</code> to do fine grained user action tracking. The <code>action_logging</code> is a decorator to allow you to integrate user info and pipe it to your inhouse event tracking system(e.g Kafka).</p> <p>You need to put the custom method into entry_points following this example.</p> <p>And here is the IDL proto we used at Lyft to send the event message: <pre><code>message UserAction {\n    // Sending host name\n    google.protobuf.StringValue host_name = 1;\n    // start time in epoch ms\n    google.protobuf.Int64Value start_epoch_ms = 2;\n    // end time in epoch ms\n    google.protobuf.Int64Value end_epoch_ms = 3;\n    // json array contains positional arguments\n    common.LongString pos_args_json = 4;\n    // json object contains key word arguments\n    common.LongString keyword_args_json = 5;\n    // json object contains output of command\n    common.LongString output = 6;\n    // an error message or exception stacktrace\n    common.LongString error = 7;\n    // `user`\n    google.protobuf.StringValue user = 8;\n}\n</code></pre></p> <p>It matches the action log model defined in here.</p> <p>Once you have the event in your data warehouse, you could start building different KPI user metric:</p> <ol> <li>WAU</li> </ol> <p>Sample query if the event table named as <code>default.event_amundsenfrontend_user_action</code> <pre><code>SELECT date_trunc('week', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\",\n   COUNT(DISTINCT user_value) AS \"count_distinct_active_users\"\nFROM\n(SELECT *\nFROM default.event_amundsenfrontend_user_action\nWHERE ds &gt; '2019-09-01') AS \"expr_qry\"\nWHERE \"ds\" &gt;= '2020-04-21 00:00:00.000000'\nAND \"ds\" &lt;= '2020-10-21 05:31:14.000000'\nGROUP BY date_trunc('week', CAST(\"ds\" AS TIMESTAMP))\nORDER BY \"count_distinct_active_users\" DESC\nLIMIT 10000\n</code></pre></p> <ol> <li>DAU</li> </ol> <p>Sample query if the event table named as <code>default.event_amundsenfrontend_user_action</code> <pre><code>SELECT date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\",\n   COUNT(DISTINCT user_value) AS \"count_distinct_active_users\"\nFROM\n(SELECT *\nFROM default.event_amundsenfrontend_user_action\nWHERE ds &gt; '2019-09-01') AS \"expr_qry\"\nWHERE \"ds\" &gt;= '2020-07-21 00:00:00.000000'\nAND \"ds\" &lt;= '2020-10-21 00:00:00.000000'\nGROUP BY date_trunc('day', CAST(\"ds\" AS TIMESTAMP))\nORDER BY \"count_distinct_active_users\" DESC\nLIMIT 50000\n</code></pre></p> <p>You could also exclude weekends: <pre><code>SELECT date_trunc('day', CAST(\"ds\" AS TIMESTAMP)) AS \"__timestamp\",\n   COUNT(DISTINCT user_value) AS \"count_distinct_active_users\"\nFROM\n(SELECT *\nFROM default.event_amundsenfrontend_user_action\nWHERE ds &gt; '2019-09-01') AS \"expr_qry\"\nWHERE \"ds\" &gt;= '2020-04-21 00:00:00.000000'\nAND \"ds\" &lt;= '2020-10-21 05:33:11.000000'\nAND day_of_week(logged_at) NOT IN (6,\n                                 7)\nGROUP BY date_trunc('day', CAST(\"ds\" AS TIMESTAMP))\nORDER BY \"count_distinct_active_users\" DESC\nLIMIT 50000\n</code></pre></p> <ol> <li>User Penetration per role</li> </ol> <p>Sample query if the event table named as <code>default.event_amundsenfrontend_user_action</code> and a table for user: <pre><code>SELECT \"title\" AS \"title\",\n   COUNT(DISTINCT email) * 100 / MAX(role_count) AS \"penetration_percent\"\nFROM\n(SELECT e.occurred_at,\n      u.email,\n      u.title,\n      tmp.role_count\nFROM default.family_user u\nJOIN default.event_amundsenfrontend_user_action e ON u.email = e.user_value\nJOIN\n (SELECT title,\n         count(*) role_count\n  FROM default.family_user\n  GROUP BY 1) as tmp ON u.title = tmp.title\nwhere ds is not NULL) AS \"expr_qry\"\nWHERE \"occurred_at\" &gt;= from_iso8601_timestamp('2020-10-14T00:00:00.000000')\nAND \"occurred_at\" &lt;= from_iso8601_timestamp('2020-10-21T00:00:00.000000')\nAND \"role_count\" &gt; 20\nGROUP BY \"title\"\nORDER BY \"penetration_percent\" DESC\nLIMIT 100\n</code></pre></p> <ol> <li>Usage breakdown per role_count</li> </ol> <p>sample query: <pre><code>SELECT \"title\" AS \"title\",\n   count(\"email\") AS \"COUNT(email)\"\nFROM\n(SELECT e.occurred_at,\n      u.email,\n      u.title,\n      tmp.role_count\nFROM default.family_user u\nJOIN default.event_amundsenfrontend_user_action e ON u.email = e.user_value\nJOIN\n (SELECT title,\n         count(*) role_count\n  FROM default.family_user\n  GROUP BY 1) as tmp ON u.title = tmp.title\nwhere ds is not NULL) AS \"expr_qry\"\nWHERE \"occurred_at\" &gt;= from_iso8601_timestamp('2020-10-14T00:00:00.000000')\nAND \"occurred_at\" &lt;= from_iso8601_timestamp('2020-10-21T00:00:00.000000')\nGROUP BY \"title\"\nORDER BY \"COUNT(email)\" DESC\nLIMIT 15\n</code></pre></p> <ol> <li>search click through rate</li> </ol> <p>sample query: <pre><code>SELECT date_trunc('day', CAST(\"occurred_at\" AS TIMESTAMP)) AS \"__timestamp\",\n   SUM(CASE\n           WHEN CAST(json_extract_scalar(keyword_args_json, '$.index') AS BIGINT) &lt;= 3 THEN 1\n           ELSE 0\n       END) * 100 / COUNT(*) AS \"click_through_rate\"\nFROM\n(SELECT *\nFROM default.event_amundsenfrontend_user_action\nWHERE ds &gt; '2019-09-01') AS \"expr_qry\"\nWHERE \"occurred_at\" &gt;= from_iso8601_timestamp('2020-09-21T00:00:00.000000')\nAND \"occurred_at\" &lt;= from_iso8601_timestamp('2020-10-21T00:00:00.000000')\nAND \"command\" IN ('_get_table_metadata',\n                '_get_dashboard_metadata',\n                '_log_get_user')\nAND json_extract_scalar(keyword_args_json, '$.source') IN ('search_results',\n                                                         'inline_search')\nGROUP BY date_trunc('day', CAST(\"occurred_at\" AS TIMESTAMP))\nORDER BY \"click_through_rate\" DESC\nLIMIT 10000\n</code></pre></p> <ol> <li> <p>Top 50 active user</p> </li> <li> <p>Top search term</p> </li> <li> <p>Top popular tables</p> </li> <li> <p>Search click index</p> </li> <li> <p>Metadata edits</p> </li> <li> <p>Metadata edit leaders</p> </li> <li> <p>Amundsen user per role (by joining with employee data)</p> </li> <li> <p>\u2026</p> </li> </ol>"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/","title":"How to use Amundsen with Amazon Neptune","text":"<p>An alternative to Neo4j as Amundsen\u2019s database is Amazon Neptune.</p> <p>This tutorial will go into setting up Amundsen to integrate with Neptune. If you want to find out how to set up a Neptune instance you can find that information at Neptune Setup.</p>"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#configuring-your-databuilder-jobs-to-use-neptune","title":"Configuring your Databuilder jobs to use Neptune","text":"<p>The Neptune integration follows the same pattern as the rest of Amundsen\u2019s databuilder library. </p> <p>Each job contains a task and a publisher and each task comprises of a extractor, transformer, and loader.</p> <p>The Neptune databuilder integration was built so that it was compatible with all extractors  (and the models produced by those extractors) so that only the loader  and publisher diverge from the Neo4j integration.</p> <p>Note: Even though the Databuilder may support the model the Metadata Service might not.  </p>"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#loading-data-into-neptune","title":"Loading data into Neptune","text":"<p>The sample_data_loader_neptune.py script contains an example on how to ingest data into Neptune. However, the main components are the  FSNeptuneCSVLoader and the NeptuneCSVPublisher</p> <p>The <code>FSNeptuneCSVLoader</code> is responsible for converting the GraphNode and GraphRelationship  into a csv format that the Neptune bulk loader expects. The <code>FSNeptuneCSVLoader</code> has 5 configuration keys</p> <ul> <li><code>NODE_DIR_PATH</code> - Where the node csv files should go</li> <li><code>RELATION_DIR_PATH</code> - Where the relationship csv files should go</li> <li><code>FORCE_CREATE_DIR</code> - Should the loader overwrite any existing files (Default is False)</li> <li><code>SHOULD_DELETE_CREATED_DIR</code> - Should the loader delete the files once the job is over (Default is True)</li> <li><code>JOB_PUBLISHER_TAG</code> - A tag that all models published by this job share. (should be unique)</li> </ul> <p><code>NeptuneCSVPublisher</code> takes the csv files produced by the <code>FSNeptuneCSVLoader</code> and ingest them into  Neptune. It achieves this by using the Neptune\u2019s bulk loader API. The flow of the <code>NeptuneCSVPublisher</code> is:</p> <ol> <li>Upload the csv files to S3</li> <li>Initiating a bulk loading request</li> <li>Poll on that status of the request till it reports a success or failure</li> </ol> <p>The <code>NeptuneCSVPublisher</code> has the following configuration keys:</p> <ul> <li><code>NODE_FILES_DIR</code> - Where the publisher will look for node files</li> <li><code>RELATION_FILES_DIR</code> - Where the publisher will look for relationship files</li> <li><code>AWS_S3_BUCKET_NAME</code> - The name of the S3 bucket where the publisher will upload the files to.</li> <li><code>AWS_BASE_S3_DATA_PATH</code> - The location within the bucket where the publisher will upload the files</li> <li><code>NEPTUNE_HOST</code> - The Neptune host in the format of <code>&lt;HOST&gt;:&lt;PORT&gt;</code> no protocol included</li> <li><code>AWS_REGION</code> - The AWS region where the Neptune instance is located.</li> <li><code>AWS_ACCESS_KEY</code> - AWS access key (Optional)</li> <li><code>AWS_SECRET_ACCESS_KEY</code> - AWS access secret access key (Optional)</li> <li><code>AWS_SESSION_TOKEN</code> - AWS session token if you are using temporary credentials (Optional)</li> <li><code>AWS_IAM_ROLE_NAME</code> - IAM ROLE NAME used for the bulk loading</li> <li><code>AWS_STS_ENDPOINT_URL</code> - AWS STS endpoint url, if not set the global endpoint will be used (Optional)</li> <li><code>FAIL_ON_ERROR</code> - If set to True an exception will be raised on failure (default False)</li> <li><code>STATUS_POLLING_PERIOD</code> - Period in seconds checking on the status of the bulk loading request</li> </ul>"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#publishing-data-from-neptune-to-amundsen-search","title":"Publishing data from Neptune to Amundsen Search","text":"<p>In order to have your entities searchable on the front end you need to extract the data from Neptune and push it into your elasticsearch cluster, so the search service can query it. To achieve this, the data builder comes with the NeptuneSearchDataExtractor which can be integrated with the FSElasticsearchJSONLoader and the ElasticsearchPublisher. An example job can be found in the sample_data_loader_neptune.py  in the <code>create_es_publisher_sample_job</code> function.</p> <p>The <code>NeptuneSearchDataExtractor</code> supports extracting table, user, and dashboard models in a format that  <code>FSElasticsearchJSONLoader</code> accepts. It has the following configuration keys:</p> <ul> <li><code>ENTITY_TYPE_CONFIG_KEY</code> - Type of model being extracted. This supports table, user, dashboard (defaults to table)</li> <li><code>MODEL_CLASS_CONFIG_KEY</code> - Python path of class to cast the extracted data to. (Optional)</li> <li><code>JOB_PUBLISH_TAG_CONFIG_KEY</code> - Allows you to filter your extraction to a job tag. (Optional)</li> <li><code>QUERY_FUNCTION_CONFIG_KEY</code> - Allows you to pass in a extraction query of your own (Optional)</li> <li><code>QUERY_FUNCTION_KWARGS_CONFIG_KEY</code> - Keyword arguments for the custom <code>QUERY_FUNCTION</code> (Optional)</li> </ul> <p>The <code>NeptuneSearchDataExtractor</code> uses the  NeptuneSessionClient  to extract data from Neptune. The <code>NeptuneSessionClient</code> supports the following configuration keys:</p> <ul> <li><code>NEPTUNE_HOST_NAME</code> - The Neptune host in the format of <code>&lt;HOST&gt;:&lt;PORT&gt;</code> no protocol included</li> <li><code>AWS_REGION</code> - The AWS region where the Neptune instance is located.</li> <li><code>AWS_ACCESS_KEY</code> - AWS access key (Optional)</li> <li><code>AWS_SECRET_ACCESS_KEY</code> - AWS access secret access key (Optional)</li> <li><code>AWS_SESSION_TOKEN</code> - AWS session token if you are using temporary credentials (Optional)</li> </ul>"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#removing-stale-data-from-neptune","title":"Removing stale data from Neptune","text":"<p>Metadata often changes, so the neptune_staleness_removal_task  is used to remove old nodes and relationships. The databuilder contains an example script using the neptune_staleness_removal_task. </p>"},{"location":"tutorials/how-to-use-amundsen-with-aws-neptune/#configuring-the-metadata-service-to-use-neptune","title":"Configuring the Metadata Service to use Neptune","text":"<p>To set up Neptune for the Metadata Service you can copy the  NeptuneConfig and  point the environment variable <code>METADATA_SVC_CONFIG_MODULE_CLASS</code> to it. For example:</p> <pre><code>export METADATA_SVC_CONFIG_MODULE_CLASS=metadata_service.config.NeptuneConfig\n</code></pre> <p>The NeptuneConfig requires a few environment variables to be set these are:</p> <ul> <li><code>PROXY_HOST</code> - The host name of the Neptune instance. Formatted like: <code>wss://&lt;NEPTUNE_URL&gt;:&lt;NEPTUNE_PORT&gt;/gremlin</code></li> <li><code>AWS_REGION</code> - The AWS region where the Neptune instance is located.</li> <li><code>S3_BUCKET_NAME</code>- The location where the proxy can upload S3 files for bulk uploader</li> </ul> <p>In addition to the Config parameters above:</p> <ul> <li><code>IGNORE_NEPTUNE_SHARD</code> environment variable must be set to \u2018True\u2019 if you are using the default databuilder integration.</li> <li><code>STS_ENDPOINT</code> can also be set if the default endpoint does not work for your environment. For example: <code>https://sts.ap-southeast-1.amazonaws.com</code>. Refer to AWS Documentation for a full list of STS endpoints.</li> </ul>"},{"location":"tutorials/index-postgres/","title":"How to index metadata for real life databases","text":"<p>From previous doc, we have indexed tables from a csv files. In real production cases,  the table metadata is stored in data warehouses(e.g Hive, Postgres, Mysql, Snowflake, Bigquery etc.) which Amundsen has  the extractors for metadata extraction.</p> <p>In this tutorial, we will use a postgres db as an example to walk through how to index metadata for a postgres database. The doc won\u2019t cover how to setup a postgres database.</p> <ol> <li> <p>In the example, we have a postgres table in localhost postgres named <code>films</code>. </p> </li> <li> <p>We leverage the postgres metadata extractor to extract the metadata information of the postgres database. We could call the metadata extractor  in an adhoc python function as this example or from an Airflow DAG.</p> </li> <li> <p>Once we run the script, we could search the <code>films</code> table using Amundsen Search. </p> </li> <li> <p>We could also find and view the <code>films</code> table in the table detail page. </p> </li> </ol> <p>This tutorial uses postgres to serve as an example, but you could apply the same approach for your various data warehouses. If Amundsen  doesn\u2019t provide the extractor, you could build one based on the API and contribute the extractor back to us!</p>"},{"location":"tutorials/search-v2_1/","title":"New Amundsen Search","text":"<p>Amundsen Search now has a new API <code>v2</code> which supports searching all resources indexed in Amundsen and handles filtered and unfiltered search through the <code>/v2/search</code> endpoint as opposed to the old endpoints which were defined per resource. This new API also supports updating documents in Elasticsearch through <code>/v2/document</code>. The old endpoints relied on the <code>ElasticsearchProxy</code> proxy which doesn\u2019t support <code>v2</code>. The frontend service has been migrated to the <code>v2</code> API, so the old search proxy cannot be used with <code>amundsen-frontend &gt;= 4.0.0</code>. There is <code>ElasticsearchProxyV2</code> which supports <code>v2</code> and has feature parity with the old API. However this proxy doesn\u2019t include any enhancements beyond multi-valued filters with AND/OR logic because further enhancements (like search fuzziness, stemmings, highlighting, etc.)require new mappings to be created by databuilder. Finally there is <code>ElasticsearchProxyV2_1</code>. This latets proxy supports all new search enhacements that rely on new Elasticsearch mappings and it\u2019s also accessible throught <code>/v2/search</code> and <code>/v2/document</code>. This proxy class is configured by default but it will fall back to <code>ElasticsearchProxyV2</code> if it cannot find the new mappings in Elasticsearch.</p> <p>The goal of this tutorial is to explain what the new search functionality offers, how to transition into using <code>/v2/search</code> search service endpoint out of the box as well as how to customize it ahead of deprecation of the old search endpoints. The updated search service offers fuzziness, word stemming, and boosted search ranking based on usage.</p>"},{"location":"tutorials/search-v2_1/#elasticsearch-context","title":"Elasticsearch Context","text":"<p>In order for search to function, metadata for all searchable resources configured is indexed into Elasticsearch with a databuilder task and then Elasticsearch is queried via the search service. In Elasticsearch there are two main concepts that are fundamental to understanding how search works, mappings and search queries. Mappings define how fields are indexed into Elasticsearch by specifying the types of fields and how text fields are analyzed. The queries should then be written with the mapping in mind since there certain field types are required to perform certain types of queries efficiently.</p>"},{"location":"tutorials/search-v2_1/#transitioning-to-v2search","title":"Transitioning to <code>/v2/search</code>","text":"<p>In order to ensure a smooth transition from our previous search functionality to a version which supports these new features follow these steps:</p>"},{"location":"tutorials/search-v2_1/#index-metadata-to-elasticsearch-using-new-mappings","title":"Index Metadata to Elasticsearch Using New Mappings","text":"<ol> <li>Bump to <code>amundsen-databuilder &gt;= 6.8.0</code></li> <li>Configure SearchMetadatatoElasticsearchTask<ul> <li>Make sure to configure it using a different index alias. So for example if previously the configuration for writing new ES indices was (A) for this task (B) must be configured instead for each resource.     (A) <code>ELASTICSEARCH_ALIAS_CONFIG_KEY \u2192 table_search_index</code> (B) <code>ELASTICSEARCH_ALIAS_CONFIG_KEY \u2192 table_search_index_v2_1</code></li> <li>This way the previous index is preserved and the new index aliases have the new mappings that enable all of the functionality mentioned above.</li> <li>Note this task will use the mappings already provided in this file and default queries to extract metadata from neo4j. Elasticsearch mappings can be customized by extending the mapping classes and configuring the task to use the custom mapping via <code>MAPPING_CLASS</code> Queries to extract metadata from neo4j can be customized and configured through <code>CYPHER_QUERY_CONFIG_KEY</code>.</li> </ul> </li> <li>Run the task.</li> <li>Verify that your ES mappings match the new mapping definitions by running this directly on Elasticsearch.<ul> <li><code>GET new_table_search_index</code></li> </ul> </li> </ol>"},{"location":"tutorials/search-v2_1/#configuring-the-search-service","title":"Configuring the Search Service","text":"<ol> <li>Bump to <code>amundsen-search &gt;= 4.0.0</code></li> <li>Configure the Elasticsearch ES_PROXY_CLIENT to use ELASTICSEARCH_V2_1, which is enabled by default in the latest version of search unless configured differently.<ul> <li>You can customize the search query by providing a custom client. You can create your own client by extending the class from <code>es_proxy_v2_1.py</code> (ex: <code>class MyESClient(ElasticsearchProxyV2_1):</code>) and overwritting any of the functions provided to change the query.</li> </ul> </li> <li>(OPTIONAL) If the alias your new mappings are indexed under differs from <code>{resource}_search_index_v2_1</code> make sure to configure the correct string template by adding <code>ES_ALIAS_TEMPLATE = 'my_{resource}_search_index_alias'</code> to the config with your custom alias name.</li> </ol>"},{"location":"tutorials/search-v2_1/#use-the-latest-version-of-frontend","title":"Use the latest version of Frontend","text":"<ol> <li>Make sure you are using <code>amundsen-frontend &gt;= 4.0.0</code> which calls the search service <code>/v2/search</code> endpoint. </li> </ol>"},{"location":"tutorials/user-profiles/","title":"How to setup user profiles","text":""},{"location":"tutorials/user-profiles/#people-resources","title":"People resources","text":""},{"location":"tutorials/user-profiles/#what-can-i-do-with-user-resources","title":"What can I do with User Resources?","text":"<p>User profile pages and the ability to bookmark/favorite and search for users is also available as of now. See a demo of what they feels like from an end user viewpoint from around the 36 minute mark of this September 2019 talk - so you could actually argue that this video snippet can work as an end user guide.</p>"},{"location":"tutorials/user-profiles/#how-do-i-enable-user-pages","title":"How do I enable User pages?","text":"<p>The configuration to have <code>Users</code> available consists of:</p> <ol> <li> <p>Enable the users profile page index and display feature by performing this frontend configuration</p> </li> <li> <p>There are two different alternative ways to populate user profile data. You can either:</p> <ul> <li> <p>Configure the Metadata service to a do a live lookup in some directory service, like LDAP or a HR system.</p> </li> <li> <p>Setup ongoing ingest of user profile data as they onboard/change/offboard into Neo4j and Elasticsearch effectively caching it with the pros/cons of that (similar to what the Databuilder sample loader does from user CSV, see the \u201cpre-cooked demo data\u201d link in the Architecture overview</p> </li> </ul> <p>Note</p> <p>Currently, for both of these options Amundsen only provides these hooks/interfaces to add your own implementation. If you build something you think is generally useful, contributions are welcome!</p> </li> <li> <p>Configure login, according to the Authentication guide</p> </li> </ol>"}]}